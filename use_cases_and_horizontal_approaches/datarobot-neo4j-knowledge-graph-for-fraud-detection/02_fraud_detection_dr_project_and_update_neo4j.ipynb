{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298dad7e-a4cb-47fb-b71d-6b4bade31173",
   "metadata": {},
   "source": [
    "# Fraud detection with DataRobot and Neo4j knowledge graphs\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline for:\n",
    "\n",
    "* Flattening a Neo4j graph of (Clientâ€“Loan) pairs into train + holdout CSV files.\n",
    "* Creating a DataRobot project from the training data, with 'Fraud' as the target label.\n",
    "* Retrieving detailed model attributes from the best model on the Leaderboard using the DataRobot Python SDK.\n",
    "* Scoring (predicting on) the holdout (pending) dataset, collecting predictions & explanations.\n",
    "* (Optional) Updating Neo4j with these predictions for deeper analysis.\n",
    "\n",
    "Dependencies:\n",
    " - `pip install -r requirements.txt`\n",
    " - Your environment must have a class \"ClientLoanFeatureExtractor\" that extracts a DataFrame from Neo4j\n",
    "   and optionally a method to update predictions back into Neo4j.\n",
    "\n",
    "Remember to adapt or remove code as needed for your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4012a-cca2-4bc4-a407-102670249ffd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f743709-9ee0-40a6-a165-8ea471949003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from FraudGraphFeatureExtractor import (\n",
    "    ClientLoanFeatureExtractor,\n",
    "    update_neo4j_predictions,\n",
    ")\n",
    "import datarobot as dr\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8668d4f-cb42-434c-822e-dd4035353c9f",
   "metadata": {},
   "source": [
    "### Configure DataRobot credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79b2f3-eaee-4eee-88e2-a3b0d6b28885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# DR_TOKEN = \"YOUR_DATAROBOT_API_TOKEN\"\n",
    "# DR_ENDPOINT = \"https://app.datarobot.com/api/v2\"  # or your DR cluster\n",
    "#\n",
    "# If you haven't already called dr.Client(...), do so below:\n",
    "# dr.Client(token=DR_TOKEN, endpoint=DR_ENDPOINT)\n",
    "dr.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed7d989-8cbe-41e3-9f8e-c7654eb75f3f",
   "metadata": {},
   "source": [
    "### Configure Neo4j credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec170e-a5ed-4767-9312-174bc9b355ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "NEO4J_DATABASE = \"neo4j\"  # or None if single db\n",
    "\n",
    "# Connect to DataRobot (assuming you've already configured dr.Client(...) globally)\n",
    "dr.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2651cc-2d9c-4cb0-a92e-97ca65c6ec22",
   "metadata": {},
   "source": [
    "### Create two CSVs from Neo4j\n",
    "\n",
    "In this step you complete the following:\n",
    "1. Use 'ClientLoanFeatureExtractor' to flatten the Neo4j graph into a single DataFrame 'df'.\n",
    "2. Split 'df' into:\n",
    "   - df_train: All rows with loan_status != 'pending' (i.e., closed loans, labeled Fraud=0/1).\n",
    "   - df_holdout: All rows with loan_status == 'pending' (unlabeled).\n",
    "3. Save these to 'train.csv' (for modeling) and 'holdout.csv' (for scoring).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ef5fa-6f0e-4ffd-b285-fe4262e0078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting data from Neo4j -> dataframes...\")\n",
    "\n",
    "extractor = ClientLoanFeatureExtractor(\n",
    "    uri=NEO4J_URI, user=NEO4J_USER, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")\n",
    "df = extractor.extract_client_loan_rows()\n",
    "extractor.close()\n",
    "\n",
    "print(\"Full DataFrame shape:\", df.shape)\n",
    "print(df.head(5))\n",
    "\n",
    "# Separate training vs. holdout\n",
    "df_train = df[df[\"loan_status\"] != \"pending\"].copy()\n",
    "df_holdout = df[df[\"loan_status\"] == \"pending\"].copy()\n",
    "\n",
    "# Save to CSV\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_holdout.to_csv(\"holdout.csv\", index=False)\n",
    "\n",
    "print(\"train.csv shape:\", df_train.shape)\n",
    "print(\"holdout.csv shape:\", df_holdout.shape)\n",
    "print(\"Saved train.csv, holdout.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53962430-b1f1-4f85-9fbc-3942425c5cb9",
   "metadata": {},
   "source": [
    "### Upload the training dataset to DataRobot and start Autopilot\n",
    "\n",
    "1. Create a UseCase object (optional) in DataRobot to categorize your project. \n",
    "2. Upload 'train.csv' as a DataRobot dataset.\n",
    "3. Optional. Create a feature list if you have a known subset of features.\n",
    "4. Create a project using 'Project.create_from_dataset()'.\n",
    "5. Set 'Fraud' as the target label. DataRobot runs Autopilot to train multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360e3e9-481b-4089-af38-76a1f2f82a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_case_name = \"AI Accelerator: Fraud Detection with Knowledge Graphs\"\n",
    "\n",
    "use_case = dr.UseCase.create(\n",
    "    name=use_case_name, \n",
    "    description=\"Fraud Detection with Knowledge Graphs and DataRobot\"\n",
    ")\n",
    "\n",
    "train_ds = dr.Dataset.create_from_file(\"./train.csv\", categories=[\"TRAINING\"], use_cases=[use_case])\n",
    "\n",
    "# Load a separate CSV for selected features and create feature list\n",
    "features = pd.read_csv(\"./Selected Features.csv\", header=None)[0].to_list()\n",
    "train_ds.create_featurelist(\"Selected Features\", features)\n",
    "\n",
    "project_name = f\"Fraud_Loan_Demo_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "print(f\"\n",
    "Creating new DataRobot project: {project_name}\")\n",
    "\n",
    "project = dr.Project.create_from_dataset(\n",
    "    dataset_id=train_ds.id,\n",
    "    project_name=project_name,\n",
    "    use_case=use_case\n",
    ")\n",
    "\n",
    "fl = project.get_featurelist_by_name(\"Selected Features\")\n",
    "\n",
    "project.set_options(shap_only_mode=True)\n",
    "project.analyze_and_model(\n",
    "    target=\"Fraud\",\n",
    "    mode=dr.AUTOPILOT_MODE.FULL_AUTO,\n",
    "    worker_count=-1\n",
    "    featurelist_id=fl.id, \n",
    ")\n",
    "print(\"Autopilot started. Building models...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e89f24-4f79-48d6-a1e5-33b92ef1de36",
   "metadata": {},
   "source": [
    "### Choose the best model\n",
    "\n",
    "Wait for autopilot to finish. Then, pick the top model from the Leaderboard and display some advanced attributes: blueprint_id, metrics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df5c4c-0d9a-45e6-9f42-5cc1ba681e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nWaiting for Autopilot to finish (this may take a while).\")\n",
    "project.wait_for_autopilot()\n",
    "\n",
    "# Retrieve models sorted by rank\n",
    "models = project.get_models()\n",
    "best_model = models[0]\n",
    "print(f\"Best model = {best_model.model_type}, id={best_model.id}\")\n",
    "\n",
    "print(\"\\n--- Detailed Model Info ---\")\n",
    "# Let's print each relevant attribute from the Model class\n",
    "print(\"Model ID:\", best_model.id)\n",
    "print(\"Project ID:\", best_model.project_id)\n",
    "print(\"Processes:\", best_model.processes)\n",
    "print(\"Featurelist Name:\", best_model.featurelist_name)\n",
    "print(\"Featurelist ID:\", best_model.featurelist_id)\n",
    "print(\"Sample pct (if non-datetime partition):\", best_model.sample_pct)\n",
    "print(\"Training row count:\", best_model.training_row_count)\n",
    "print(\"Training duration (datetime partition):\", best_model.training_duration)\n",
    "print(\"Training start date:\", best_model.training_start_date)\n",
    "print(\"Training end date:\", best_model.training_end_date)\n",
    "print(\"Model Type:\", best_model.model_type)\n",
    "print(\"Model Category:\", best_model.model_category)\n",
    "print(\"Is Frozen?:\", best_model.is_frozen)\n",
    "print(\"Blueprint ID:\", best_model.blueprint_id)\n",
    "print(\"Metrics:\", best_model.metrics)\n",
    "print(\"N Clusters:\", best_model.n_clusters)\n",
    "print(\"Has Empty Clusters?:\", best_model.has_empty_clusters)\n",
    "print(\"Is starred?:\", best_model.is_starred)\n",
    "print(\"Prediction Threshold:\", best_model.prediction_threshold)\n",
    "print(\"Model Number:\", best_model.model_number)\n",
    "print(\"Parent Model ID:\", best_model.parent_model_id)\n",
    "print(\"Supports composable ml?:\", best_model.supports_composable_ml)\n",
    "if hasattr(best_model, \"use_project_settings\"):\n",
    "    print(\"Use project settings:\", best_model.use_project_settings)\n",
    "print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2634abd-c643-4ef1-8367-ba7c6ac83ed5",
   "metadata": {},
   "source": [
    "### Predict on the Holdout dataset and request explanations\n",
    "\n",
    "Upload 'holdout.csv' to DataRobot as a separate dataset, then request predictions using the best model. \n",
    "You can optionally request prediction explanations to see the top feature drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36514d-406a-49d6-8708-e68f478719f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nScoring holdout.csv with best model...\")\n",
    "holdout_ds = project.upload_dataset(\"holdout.csv\")\n",
    "\n",
    "# Request predictions on the holdout dataset\n",
    "pred_job = best_model.request_predictions(holdout_ds.id)\n",
    "pred_df = pred_job.get_result_when_complete(max_wait=600)\n",
    "\n",
    "print(\"Predictions shape:\", pred_df.shape)\n",
    "print(\"Sample predictions:\")\n",
    "print(pred_df.head())\n",
    "\n",
    "# If you want explanations:\n",
    "explanations_job = dr.PredictionExplanations.create(\n",
    "    project_id=project.id,\n",
    "    model_id=best_model.id,\n",
    "    dataset_id=holdout_ds.id,\n",
    "    max_explanations=5,\n",
    ")\n",
    "explanations_df = explanations_job.get_result_when_complete(\n",
    "    max_wait=999\n",
    ").get_all_as_dataframe()\n",
    "print(\"\\nExplanations sample:\\n\", explanations_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751d79c-1104-4e29-9570-8169f63321c5",
   "metadata": {},
   "source": [
    "### Combine predictions and the top explanation with the original Holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228f29c-f523-4ae4-b6a8-00cc9d4ff40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored = df_holdout.copy()\n",
    "\n",
    "# The output column might be 'positive_probability' or 'prediction'\n",
    "pred_col = (\n",
    "    \"positive_probability\"\n",
    "    if \"positive_probability\" in pred_df.columns\n",
    "    else \"prediction\"\n",
    ")\n",
    "df_scored[\"pred_fraud_probability\"] = pred_df[pred_col].values\n",
    "\n",
    "# Minimal approach for top explanation\n",
    "df_scored[\"top_feature\"] = explanations_df[\"explanation_0_feature\"]\n",
    "df_scored[\"top_feature_value\"] = explanations_df[\"explanation_0_feature_value\"]\n",
    "df_scored[\"top_feat_qual_strgth\"] = explanations_df[\n",
    "    \"explanation_0_qualitative_strength\"\n",
    "]\n",
    "\n",
    "df_scored.to_csv(\"holdout_scored.csv\", index=False)\n",
    "print(\"holdout_scored.csv saved with predictions + top explanation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99536803-466d-428c-a764-523f98bd8089",
   "metadata": {},
   "source": [
    "### 8) Post-Processing\n",
    "\n",
    "You can define a threshold for \"flagged_as_fraud\". Then you can re-inject these predictions back to Neo4j if you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca296eeb-2f05-4437-a4dc-7abeea21efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored[\"flagged_as_fraud\"] = (df_scored[\"pred_fraud_probability\"] > 0.45).astype(int)\n",
    "print(\"\\nHigh-level summary of flagged loans:\")\n",
    "print(df_scored[\"flagged_as_fraud\"].value_counts())\n",
    "\n",
    "\n",
    "print(\"\\nUpdating Neo4j with predictions from best model...\")\n",
    "update_neo4j_predictions(df_scored, best_model)\n",
    "print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
