{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d904e66-0473-4921-b940-11d2a7e7dc73",
   "metadata": {},
   "source": [
    "# Use Feature Discovery SQL in other Spark clusters\n",
    "\n",
    "Authors: Harry Dinh, John Edwards\n",
    "Date: 15/10/2024\n",
    "\n",
    "This notebook provides a framework for running Feature Discovery SQL in a new Spark cluster on Docker. It guides you through the process of setting up a Spark cluster in Docker, registering custom User Defined Functions (UDFs), and executing complex SQL queries for feature engineering across multiple datasets. The same approach can be applied to other Spark environments, such as GCP Dataproc, Amazon EMR and Cloudera CDP. This approach also provides flexibility for running Feature Discovery on various Spark platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ddfd7-04a0-4d97-9401-8b24f732adb4",
   "metadata": {},
   "source": [
    "## Problem framing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e02f5-e399-452a-bfaa-bded5f8f260d",
   "metadata": {},
   "source": [
    "\n",
    "Features are commonly split across multiple data assets. Bringing these data assets together can take a lot of work, as it involves joining them and then running machine learning models. It's even more difficult when the datasets are of different granularities, as it requires you to aggregate to join the data successfully.\n",
    "\n",
    "[Feature Discovery](https://docs.datarobot.com/en/docs/data/transform-data/feature-discovery/enrich-data-using-feature-discovery.html) solves this problem by automating the procedure of joining and aggregating your datasets. After you define how the datasets need to be joined, DataRobot handles feature generation and modeling.\n",
    "\n",
    "<img src=\"img/FD_graph.png\" width=\"800\">\n",
    "\n",
    "<img src=\"img/FD_SQL.png\" width=\"800\">\n",
    "\n",
    "Feature Discovery uses Spark to perform joins and aggregations, generating Spark SQL at the end of the process. In some cases, you may want to run this Spark SQL in other Spark clusters to gain more flexibility and scalability for handling larger datasets, without the need to load data directly into the DataRobot environment. This approach allows you to leverage external Spark clusters for more resource-intensive tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d8611-a574-462d-a3a3-8a97911aa576",
   "metadata": {},
   "source": [
    "## File overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cee3c6-e7ca-4856-957f-d18856c5b6d1",
   "metadata": {},
   "source": [
    "The file structure is outlined below:\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── Using Feature Discovery SQL in other Spark clusters.ipynb\n",
    "├── apps\n",
    "│    ├── DataRobotRunSSSQL.py\n",
    "│    ├── LC_FD_SQL.sql\n",
    "│    ├── LC_profile.csv\n",
    "│    ├── LC_train.csv\n",
    "│    └── LC_transactions.csv\n",
    "├── data\n",
    "├── libs\n",
    "│    ├── spark-udf-assembly-0.1.0.jar\n",
    "│    └── venv.tar.gz\n",
    "├── docker-compose.yml\n",
    "├── Dockerfile\n",
    "├── start-spark.sh\n",
    "└── utils.py\n",
    "```\n",
    "- `Using Feature Discovery SQL in other Spark clusters.ipynb`is the notebook providing a framework for running Feature Discovery SQL in a new Spark cluster on Docker.\n",
    "- `docker-compose.yml`, `Dockerfile`, and `start-spark.sh` are files used by Docker to build and start the Docker container with Spark.\n",
    "- `utils.py` includes a helper function to download datasets and the UDFs jar.\n",
    "- The `app` directory includes:\n",
    "  - Spark SQL (a file with a `.sql` extension)\n",
    "  - Datasets (files with a `.csv` extension)\n",
    "  - Helper function (files with a `.py` extension) to parse and execute the SQL\n",
    "- The `libs` directory includes:\n",
    "  - A User-Defined Functions (UDFs) jar file\n",
    "  - An environment file (only required if datasets include Japanese text, which requires a Mecab tokenizer to handle)\n",
    "- The `data` directory is empty, as it is used to store the output result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c002622-8689-43ee-932a-05447844e7dc",
   "metadata": {},
   "source": [
    "## Download datasets and .jar files\n",
    "\n",
    "Use the code below to import the datasets and .jar files used in this accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94ed031-1f67-4b91-99af-9f30b7fd7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DATASETS, download_files_from_public_s3, ENV_AND_JARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25853f79-33e6-424c-a4ac-f10c81bc09cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded LC_train.csv and saved to apps/LC_train.csv\n",
      "Downloaded LC_profile.csv and saved to apps/LC_profile.csv\n",
      "Downloaded LC_transactions.csv and saved to apps/LC_transactions.csv\n",
      "libs/spark-udf-assembly-0.1.0.jar already exists\n",
      "libs/venv.tar.gz already exists\n"
     ]
    }
   ],
   "source": [
    "# Download datasets\n",
    "download_files_from_public_s3(files_dict=DATASETS, to_folder=\"apps/\")\n",
    "\n",
    "# Download UDFs .jars and environment file\n",
    "download_files_from_public_s3(files_dict=ENV_AND_JARS, to_folder=\"libs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f9e29-c4b6-43e8-a27e-063ab92e00b1",
   "metadata": {},
   "source": [
    "## Start the Spark cluster on Docker and execute SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf16f5-57a2-4ede-bd46-793e87381e6c",
   "metadata": {},
   "source": [
    "### Run Docker\n",
    "\n",
    "Make sure that Docker is installed and running on the system where Jupyter is hosted. Run the following code to check the Docker version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0277e4f-1c65-4006-bd28-078794295e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 24.0.7, build afdd53b\n"
     ]
    }
   ],
   "source": [
    "# Check Docker installation\n",
    "!docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce250080-876d-4ef9-8492-fe6142ff3e10",
   "metadata": {},
   "source": [
    "If Docker is not installed, follow the [Docker installation guide](https://docs.docker.com/get-started/get-docker/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289ad6f-cb58-47ae-9fad-3c2e193fa327",
   "metadata": {},
   "source": [
    "### Build the Docker image\n",
    "\n",
    "Next, build the Docker image using the Dockerfile in your repository. Ensure that the Dockerfile is in the directory where the Jupyter notebook is located and run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bbdeb2-8736-47ca-a0c5-51281e12c5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                    docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.0s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.1s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.3s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.4s (3/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.4s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.4s (12/12) FINISHED                         docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.4s\n",
      "\u001b[0m\u001b[34m => [builder 1/4] FROM docker.io/library/openjdk:8-jre-slim-buster@sha256  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 83B                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [builder 2/4] RUN apt-get update && apt-get install -y curl vi  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [builder 3/4] RUN update-alternatives --install \"/usr/bin/pyth  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [builder 4/4] RUN wget --no-verbose -O apache-spark.tgz \"https  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [apache-spark 1/3] WORKDIR /opt/spark                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [apache-spark 2/3] RUN mkdir -p /opt/spark/logs && touch /opt/  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [apache-spark 3/3] COPY start-spark.sh /                        0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:a5328dffb2c685c95621ae7149a6cae1f127138872a10  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/apache-spark:3.2.1                      0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1m\n",
      "What's Next?\n",
      "\u001b[0m  1. Sign in to your Docker account → \u001b[36mdocker login\u001b[0m\n",
      "  2. View a summary of image vulnerabilities and recommendations → \u001b[36mdocker scout quickview\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker build -t apache-spark:3.2.1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7faf0-9f5b-4ffc-ad35-6ade3468689a",
   "metadata": {},
   "source": [
    "### Start the Spark cluster using Docker compose\n",
    "\n",
    "Next, use `docker-compose` to start the Spark master and worker containers. Ensure that the docker-compose.yml file is in the same directory as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5017ab-84df-4b09-8a55-d53b4ecb96db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 4/0\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0aa520-bece-452c-83bf-5a8697625056",
   "metadata": {},
   "source": [
    "### Submit Spark jobs using spark-submit\n",
    "\n",
    "To run a specific Spark job (e.g., the `DataRobotRunSSSQL.py` script), you can use the following command from the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ea2e9-4b0b-458f-9db4-18170156854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec -it spark-master /opt/spark/bin/spark-submit \\\n",
    "  --conf \"spark.sql.legacy.timeParserPolicy=LEGACY\" \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark-libs/spark-udf-assembly-0.1.0.jar \\\n",
    "  -c spark.sql.caseSensitive=true \\\n",
    "  /opt/spark-apps/DataRobotRunSSSQL.py \\\n",
    "    /opt/spark-apps/LC_FD_SQL.sql \\\n",
    "    --input=csv,primary_dataset=/opt/spark-apps/LC_train.csv \\\n",
    "    --input=csv,LC_profile=/opt/spark-apps/LC_profile.csv \\\n",
    "    --input=csv,LC_transactions=/opt/spark-apps/LC_transactions.csv \\\n",
    "    --output=csvfile,/opt/spark/spark-warehouse/result.csv  # output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e5ee4-96a1-4f27-811a-27742804f2c4",
   "metadata": {},
   "source": [
    "### Check the results\n",
    "\n",
    "Once the job has been executed, you can view the result directly from the notebook. For example, to view the first few lines of the result CSV, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597c2a5b-5b76-4fb9-b534-a6e96064c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74df0b9f-09ac-416a-b2b2-81ee98e5f0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LC_transactions (days since previous event by CustomerID) (30 days max)</th>\n",
       "      <th>LC_transactions[Date] (Day of Week) (1 week most frequent)</th>\n",
       "      <th>LC_transactions[Date] (Day of Week) (30 days latest)</th>\n",
       "      <th>LC_transactions (days since previous event by CustomerID) (30 days avg)</th>\n",
       "      <th>BadLoan</th>\n",
       "      <th>LC_transactions[Amount] (30 days std)</th>\n",
       "      <th>LC_transactions[Amount] (1 week min)</th>\n",
       "      <th>LC_transactions[Description] (30 days unique count)</th>\n",
       "      <th>LC_transactions[Date] (Day of Month) (30 days latest)</th>\n",
       "      <th>LC_transactions[Amount] (30 days min)</th>\n",
       "      <th>...</th>\n",
       "      <th>LC_transactions[Date] (Day of Week) (30 days most frequent)</th>\n",
       "      <th>date (days from LC_transactions[Date]) (1 week std)</th>\n",
       "      <th>LC_transactions[Amount] (30 days median)</th>\n",
       "      <th>LC_transactions[Amount] (30 days max)</th>\n",
       "      <th>LC_transactions[AccountID] (1 week tokens)</th>\n",
       "      <th>LC_profile[emp_length]</th>\n",
       "      <th>LC_transactions[AccountID] (word count) (30 days sum)</th>\n",
       "      <th>LC_transactions[Description] (1 week counts)</th>\n",
       "      <th>LC_transactions[Date] (Day of Month) (1 week most frequent)</th>\n",
       "      <th>LC_profile[zip_code]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>No</td>\n",
       "      <td>23.653558</td>\n",
       "      <td>14.99</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>10.25</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.118034</td>\n",
       "      <td>10.25</td>\n",
       "      <td>8.42</td>\n",
       "      <td>{\"a355056969\" : 6.0}</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>15.0</td>\n",
       "      <td>{\"internet payment\" : 2.0, \"international tran...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>782xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>No</td>\n",
       "      <td>597.190780</td>\n",
       "      <td>1.49</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.542778</td>\n",
       "      <td>32.25</td>\n",
       "      <td>95.00</td>\n",
       "      <td>{\"a366458676\" : 12.0, \"a547274624\" : 4.0, \"a38...</td>\n",
       "      <td>3 years</td>\n",
       "      <td>88.0</td>\n",
       "      <td>{\"amortisation\" : 1.0, \"internet payment\" : 2....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>010xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.241071</td>\n",
       "      <td>No</td>\n",
       "      <td>439.613390</td>\n",
       "      <td>131.50</td>\n",
       "      <td>44.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.114924</td>\n",
       "      <td>61.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"a643390547\" : 2.0, \"a346560403\" : 10.0}</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>113.0</td>\n",
       "      <td>{\"internet payment\" : 1.0, \"telco\" : 1.0, \"sup...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>890xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>No</td>\n",
       "      <td>74.973167</td>\n",
       "      <td>205.29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"a458669971\" : 1.0}</td>\n",
       "      <td>9 years</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{\"internet payment\" : 1.0}</td>\n",
       "      <td>12.0</td>\n",
       "      <td>277xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>No</td>\n",
       "      <td>316.576980</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.699673</td>\n",
       "      <td>39.00</td>\n",
       "      <td>73.30</td>\n",
       "      <td>{\"a249156803\" : 4.0, \"a448123747\" : 1.0, \"a505...</td>\n",
       "      <td>5 years</td>\n",
       "      <td>41.0</td>\n",
       "      <td>{\"internet payment\" : 1.0, \"retail furniture\" ...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>939xx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LC_transactions (days since previous event by CustomerID) (30 days max)  \\\n",
       "0                                                5.0                         \n",
       "1                                                3.0                         \n",
       "2                                                4.0                         \n",
       "3                                               14.0                         \n",
       "4                                                3.0                         \n",
       "\n",
       "   LC_transactions[Date] (Day of Week) (1 week most frequent)  \\\n",
       "0                                                5.0            \n",
       "1                                                6.0            \n",
       "2                                                3.0            \n",
       "3                                                1.0            \n",
       "4                                                0.0            \n",
       "\n",
       "   LC_transactions[Date] (Day of Week) (30 days latest)  \\\n",
       "0                                                5.0      \n",
       "1                                                6.0      \n",
       "2                                                3.0      \n",
       "3                                                1.0      \n",
       "4                                                1.0      \n",
       "\n",
       "   LC_transactions (days since previous event by CustomerID) (30 days avg)  \\\n",
       "0                                           1.714286                         \n",
       "1                                           0.310345                         \n",
       "2                                           0.241071                         \n",
       "3                                           6.250000                         \n",
       "4                                           0.675000                         \n",
       "\n",
       "  BadLoan  LC_transactions[Amount] (30 days std)  \\\n",
       "0      No                              23.653558   \n",
       "1      No                             597.190780   \n",
       "2      No                             439.613390   \n",
       "3      No                              74.973167   \n",
       "4      No                             316.576980   \n",
       "\n",
       "   LC_transactions[Amount] (1 week min)  \\\n",
       "0                                 14.99   \n",
       "1                                  1.49   \n",
       "2                                131.50   \n",
       "3                                205.29   \n",
       "4                               1000.00   \n",
       "\n",
       "   LC_transactions[Description] (30 days unique count)  \\\n",
       "0                                               12.0     \n",
       "1                                               33.0     \n",
       "2                                               44.0     \n",
       "3                                                5.0     \n",
       "4                                               25.0     \n",
       "\n",
       "   LC_transactions[Date] (Day of Month) (30 days latest)  \\\n",
       "0                                               23.0       \n",
       "1                                                3.0       \n",
       "2                                               18.0       \n",
       "3                                               12.0       \n",
       "4                                               12.0       \n",
       "\n",
       "   LC_transactions[Amount] (30 days min)  ...  \\\n",
       "0                                  10.25  ...   \n",
       "1                                   0.83  ...   \n",
       "2                                   0.28  ...   \n",
       "3                                   2.50  ...   \n",
       "4                                   1.45  ...   \n",
       "\n",
       "   LC_transactions[Date] (Day of Week) (30 days most frequent)  \\\n",
       "0                                                5.0             \n",
       "1                                                5.0             \n",
       "2                                                2.0             \n",
       "3                                                4.0             \n",
       "4                                                0.0             \n",
       "\n",
       "  date (days from LC_transactions[Date]) (1 week std)  \\\n",
       "0                                           1.118034    \n",
       "1                                           1.542778    \n",
       "2                                           1.114924    \n",
       "3                                           0.000000    \n",
       "4                                           1.699673    \n",
       "\n",
       "   LC_transactions[Amount] (30 days median)  \\\n",
       "0                                     10.25   \n",
       "1                                     32.25   \n",
       "2                                     61.50   \n",
       "3                                     55.98   \n",
       "4                                     39.00   \n",
       "\n",
       "   LC_transactions[Amount] (30 days max)  \\\n",
       "0                                   8.42   \n",
       "1                                  95.00   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                  73.30   \n",
       "\n",
       "          LC_transactions[AccountID] (1 week tokens)  LC_profile[emp_length]  \\\n",
       "0                               {\"a355056969\" : 6.0}                < 1 year   \n",
       "1  {\"a366458676\" : 12.0, \"a547274624\" : 4.0, \"a38...                 3 years   \n",
       "2          {\"a643390547\" : 2.0, \"a346560403\" : 10.0}               10+ years   \n",
       "3                               {\"a458669971\" : 1.0}                 9 years   \n",
       "4  {\"a249156803\" : 4.0, \"a448123747\" : 1.0, \"a505...                 5 years   \n",
       "\n",
       "   LC_transactions[AccountID] (word count) (30 days sum)  \\\n",
       "0                                               15.0       \n",
       "1                                               88.0       \n",
       "2                                              113.0       \n",
       "3                                                5.0       \n",
       "4                                               41.0       \n",
       "\n",
       "        LC_transactions[Description] (1 week counts)  \\\n",
       "0  {\"internet payment\" : 2.0, \"international tran...   \n",
       "1  {\"amortisation\" : 1.0, \"internet payment\" : 2....   \n",
       "2  {\"internet payment\" : 1.0, \"telco\" : 1.0, \"sup...   \n",
       "3                         {\"internet payment\" : 1.0}   \n",
       "4  {\"internet payment\" : 1.0, \"retail furniture\" ...   \n",
       "\n",
       "   LC_transactions[Date] (Day of Month) (1 week most frequent)  \\\n",
       "0                                               23.0             \n",
       "1                                                3.0             \n",
       "2                                               18.0             \n",
       "3                                               12.0             \n",
       "4                                               11.0             \n",
       "\n",
       "   LC_profile[zip_code]  \n",
       "0                 782xx  \n",
       "1                 010xx  \n",
       "2                 890xx  \n",
       "3                 277xx  \n",
       "4                 939xx  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/result.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429668b-607d-40fa-9be1-dffc2bda6ff9",
   "metadata": {},
   "source": [
    "### Stop the cluster and clean up\n",
    "\n",
    "Once you are finished with the previous tasks, you can stop and remove the Spark cluster containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd8402-befe-45b0-8a41-60ea9eb7db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the running containers\n",
    "!docker-compose down\n",
    "\n",
    "# Remove the Spark Docker image\n",
    "!docker image rm apache-spark:3.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756204d-10cd-428c-8e29-33ca1fc54418",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This accelerator provides a step-by-step guide for running Feature Discovery-generated SQL in an external Spark cluster, such as a Docker-based Spark environment. It demonstrates how to execute complex SQL queries, including custom User-Defined Functions (UDFs), in a more scalable Spark cluster. By following this framework, you can offload resource-intensive SQL tasks to larger Spark clusters, allowing for greater flexibility in handling large datasets. The same approach can be adapted to other Spark platforms, enabling seamless integration with various infrastructure setups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
