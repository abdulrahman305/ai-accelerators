{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d904e66-0473-4921-b940-11d2a7e7dc73",
   "metadata": {},
   "source": [
    "# Using Feature Discovery SQL in other Spark clusters\n",
    "\n",
    "Authors: Harry Dinh, John Edwards\n",
    "\n",
    "Date: 15/10/2024\n",
    "\n",
    "This notebook provides a framework for running Feature Discovery SQL in a new Spark cluster on Docker.  It guides you through the process of setting up a Spark cluster in Docker, registering custom User Defined Functions (UDFs), and executing complex SQL queries for feature engineering across multiple datasets. The same approach can be applied to other Spark environments, such as GCP Dataproc, Amazon EMR, Cloudera CDP, ... providing flexibility for running Feature Discovery on various Spark platforms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ddfd7-04a0-4d97-9401-8b24f732adb4",
   "metadata": {},
   "source": [
    "# Problem framing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e02f5-e399-452a-bfaa-bded5f8f260d",
   "metadata": {},
   "source": [
    "More often than not, features are split across multiple data assets. Bringing these data assets together can take a lot of work—joining them and then running machine learning models on top. It's even more difficult when the datasets are of different granularities. In this case, you have to aggregate to join the data successfully.\n",
    "\n",
    "[Feature Discovery](https://docs.datarobot.com/en/docs/data/transform-data/feature-discovery/enrich-data-using-feature-discovery.html) solves this problem by automating the procedure of joining and aggregating your datasets. After defining how the datasets need to be joined, you leave feature generation and modeling to DataRobot.\n",
    "\n",
    "<img src=\"img/FD_graph.png\" width=\"800\">\n",
    "\n",
    "<img src=\"img/FD_SQL.png\" width=\"800\">\n",
    "\n",
    "Feature Discovery uses Spark to perform joins and aggregations, generating Spark SQL at the end of the process. In some cases, you may want to run this Spark SQL in other Spark clusters to gain more flexibility and scalability for handling larger datasets, without the need to load data directly into the DataRobot environment. This approach allows you to leverage external Spark clusters for more resource-intensive tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d8611-a574-462d-a3a3-8a97911aa576",
   "metadata": {},
   "source": [
    "# Files overview\n",
    "The file structure is organized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cee3c6-e7ca-4856-957f-d18856c5b6d1",
   "metadata": {},
   "source": [
    "```bash\n",
    ".\n",
    "├── Using Feature Discovery SQL in other Spark clusters.ipynb\n",
    "├── apps\n",
    "│   ├── DataRobotRunSSSQL.py\n",
    "│   ├── LC_FD_SQL.sql\n",
    "│   ├── LC_profile.csv\n",
    "│   ├── LC_train.csv\n",
    "│   └── LC_transactions.csv\n",
    "├── data\n",
    "├── libs\n",
    "│   ├── spark-udf-assembly-0.1.0.jar\n",
    "│   └── venv.tar.gz\n",
    "├── docker-compose.yml\n",
    "├── Dockerfile\n",
    "├── start-spark.sh\n",
    "└── utils.py\n",
    "```\n",
    "- File `Using Feature Discovery SQL in other Spark clusters.ipynb`: This notebook provides a framework for running Feature Discovery SQL in a new Spark cluster on Docker.\n",
    "- File `docker-compose.yml`, `Dockerfile`, `start-spark.sh` are files will be used by Docker to build and start Docker container with Spark.\n",
    "- File `utils.py` includes helper function to download datasets & UDFs jar.\n",
    "- Directory `app` includes:\n",
    "  - Spark SQL (file with `.sql` extension)\n",
    "  - Datasets (files with `.csv` extension)\n",
    "  - Helper function (files with `.py` extension) to parse and execute the SQL\n",
    "- Directory `libs` includes:\n",
    "  - User Defined Functions (UDFs) jar file\n",
    "  - Environment file (only required if datasets include Japanese text, which requires Mecab tokenizer to handle)\n",
    "- Directory `data` is empty, will be used to store the output result\n",
    "  \n",
    "**\\*Note that the datasets, UDFs jar & environment files are initially not available, they have to be downloaded in the next section.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c002622-8689-43ee-932a-05447844e7dc",
   "metadata": {},
   "source": [
    "# Download datasets and jars files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94ed031-1f67-4b91-99af-9f30b7fd7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DATASETS, download_files_from_public_s3, ENV_AND_JARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25853f79-33e6-424c-a4ac-f10c81bc09cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded LC_train.csv and saved to apps/LC_train.csv\n",
      "Downloaded LC_profile.csv and saved to apps/LC_profile.csv\n",
      "Downloaded LC_transactions.csv and saved to apps/LC_transactions.csv\n",
      "libs/spark-udf-assembly-0.1.0.jar already exists\n",
      "libs/venv.tar.gz already exists\n"
     ]
    }
   ],
   "source": [
    "# Download datasets\n",
    "download_files_from_public_s3(files_dict=DATASETS, to_folder=\"apps/\")\n",
    "\n",
    "# Download UDFs jars & environment file\n",
    "download_files_from_public_s3(files_dict=ENV_AND_JARS, to_folder=\"libs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f9e29-c4b6-43e8-a27e-063ab92e00b1",
   "metadata": {},
   "source": [
    "# Start Spark cluster on Docker and execute SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf16f5-57a2-4ede-bd46-793e87381e6c",
   "metadata": {},
   "source": [
    "### 1. Ensure Docker is Running\n",
    "Make sure that Docker is installed and running on the system where Jupyter is hosted.\n",
    "\n",
    "Run the following to check the Docker version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0277e4f-1c65-4006-bd28-078794295e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 24.0.7, build afdd53b\n"
     ]
    }
   ],
   "source": [
    "# Check Docker installation\n",
    "!docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce250080-876d-4ef9-8492-fe6142ff3e10",
   "metadata": {},
   "source": [
    "If Docker is not installed, follow the [Docker installation guide](https://docs.docker.com/get-started/get-docker/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289ad6f-cb58-47ae-9fad-3c2e193fa327",
   "metadata": {},
   "source": [
    "### 2. Build the Docker Image\n",
    "Now, we'll build the Docker image using the Dockerfile in your repository. \n",
    "Ensure that the Dockerfile is in the directory where the Jupyter notebook is located and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bbdeb2-8736-47ca-a0c5-51281e12c5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                    docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  1.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.0s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.1s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.3s (2/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.4s (3/3)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.4s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.4s (12/12) FINISHED                         docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.49kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/openjdk:8-jre-slim-bus  2.4s\n",
      "\u001b[0m\u001b[34m => [builder 1/4] FROM docker.io/library/openjdk:8-jre-slim-buster@sha256  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 83B                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [builder 2/4] RUN apt-get update && apt-get install -y curl vi  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [builder 3/4] RUN update-alternatives --install \"/usr/bin/pyth  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [builder 4/4] RUN wget --no-verbose -O apache-spark.tgz \"https  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [apache-spark 1/3] WORKDIR /opt/spark                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [apache-spark 2/3] RUN mkdir -p /opt/spark/logs && touch /opt/  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [apache-spark 3/3] COPY start-spark.sh /                        0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:a5328dffb2c685c95621ae7149a6cae1f127138872a10  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/apache-spark:3.2.1                      0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1m\n",
      "What's Next?\n",
      "\u001b[0m  1. Sign in to your Docker account → \u001b[36mdocker login\u001b[0m\n",
      "  2. View a summary of image vulnerabilities and recommendations → \u001b[36mdocker scout quickview\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker build -t apache-spark:3.2.1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7faf0-9f5b-4ffc-ad35-6ade3468689a",
   "metadata": {},
   "source": [
    "### 3. Start the Spark Cluster using Docker Compose\n",
    "Next, use docker-compose to start the Spark master and worker containers. Ensure that the docker-compose.yml file is in the same directory as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5017ab-84df-4b09-8a55-d53b4ecb96db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 4/0\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mCreated\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mStarted\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0aa520-bece-452c-83bf-5a8697625056",
   "metadata": {},
   "source": [
    "### 4. Submit Spark Jobs Using Spark-Submit\n",
    "To run a specific Spark job (e.g., the DataRobotRunSSSQL.py script), you can use the following command from the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46ea2e9-4b0b-458f-9db4-18170156854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/04 09:30:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "24/10/04 09:30:07 INFO SparkContext: Running Spark version 3.2.1\n",
      "24/10/04 09:30:07 INFO ResourceUtils: ==============================================================\n",
      "24/10/04 09:30:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/10/04 09:30:07 INFO ResourceUtils: ==============================================================\n",
      "24/10/04 09:30:07 INFO SparkContext: Submitted application: SQLProcessor\n",
      "24/10/04 09:30:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/10/04 09:30:07 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/10/04 09:30:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/10/04 09:30:07 INFO SecurityManager: Changing view acls to: root\n",
      "24/10/04 09:30:07 INFO SecurityManager: Changing modify acls to: root\n",
      "24/10/04 09:30:07 INFO SecurityManager: Changing view acls groups to: \n",
      "24/10/04 09:30:07 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/10/04 09:30:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "24/10/04 09:30:07 INFO Utils: Successfully started service 'sparkDriver' on port 41827.\n",
      "24/10/04 09:30:07 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/10/04 09:30:07 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/10/04 09:30:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/10/04 09:30:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/10/04 09:30:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/10/04 09:30:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-68d2e851-582f-44ad-a072-9e7c32331407\n",
      "24/10/04 09:30:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "24/10/04 09:30:07 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/10/04 09:30:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/10/04 09:30:07 INFO SparkUI: Bound SparkUI to spark-master, and started at http://c37eddbb00a9:4040\n",
      "24/10/04 09:30:07 INFO SparkContext: Added JAR file:///opt/spark-libs/spark-udf-assembly-0.1.0.jar at spark://c37eddbb00a9:41827/jars/spark-udf-assembly-0.1.0.jar with timestamp 1728034207036\n",
      "24/10/04 09:30:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "24/10/04 09:30:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.22.0.2:7077 after 11 ms (0 ms spent in bootstraps)\n",
      "24/10/04 09:30:07 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241004093007-0000\n",
      "24/10/04 09:30:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40107.\n",
      "24/10/04 09:30:07 INFO NettyBlockTransferService: Server created on c37eddbb00a9:40107\n",
      "24/10/04 09:30:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/10/04 09:30:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c37eddbb00a9, 40107, None)\n",
      "24/10/04 09:30:07 INFO BlockManagerMasterEndpoint: Registering block manager c37eddbb00a9:40107 with 366.3 MiB RAM, BlockManagerId(driver, c37eddbb00a9, 40107, None)\n",
      "24/10/04 09:30:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c37eddbb00a9, 40107, None)\n",
      "24/10/04 09:30:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c37eddbb00a9, 40107, None)\n",
      "24/10/04 09:30:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241004093007-0000/0 on worker-20241004093006-172.22.0.4-7000 (172.22.0.4:7000) with 1 core(s)\n",
      "24/10/04 09:30:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20241004093007-0000/0 on hostPort 172.22.0.4:7000 with 1 core(s), 1024.0 MiB RAM\n",
      "24/10/04 09:30:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241004093007-0000/1 on worker-20241004093006-172.22.0.3-7000 (172.22.0.3:7000) with 1 core(s)\n",
      "24/10/04 09:30:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20241004093007-0000/1 on hostPort 172.22.0.3:7000 with 1 core(s), 1024.0 MiB RAM\n",
      "24/10/04 09:30:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241004093007-0000/0 is now RUNNING\n",
      "24/10/04 09:30:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241004093007-0000/1 is now RUNNING\n",
      "24/10/04 09:30:07 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "24/10/04 09:30:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/10/04 09:30:07 INFO SharedState: Warehouse path is 'file:/opt/spark/spark-warehouse'.\n",
      "24/10/04 09:30:08 WARN SimpleFunctionRegistry: The function transform_values replaced a previously registered function.\n",
      "24/10/04 09:30:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.4:36490) with ID 0,  ResourceProfileId 0\n",
      "24/10/04 09:30:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.3:60010) with ID 1,  ResourceProfileId 0\n",
      "24/10/04 09:30:08 INFO BlockManagerMasterEndpoint: Registering block manager 172.22.0.3:45645 with 366.3 MiB RAM, BlockManagerId(1, 172.22.0.3, 45645, None)\n",
      "24/10/04 09:30:08 INFO BlockManagerMasterEndpoint: Registering block manager 172.22.0.4:35591 with 366.3 MiB RAM, BlockManagerId(0, 172.22.0.4, 35591, None)\n",
      "24/10/04 09:30:09 INFO CodeGenerator: Code generated in 79.862042 ms\n",
      "\n",
      "Spark DataRobot SAFER functions:\n",
      "  dr_add_val_to_key_and_map_with_1\n",
      "  dr_agg_entropy\n",
      "  dr_agg_merge_categorical_stats_map\n",
      "  dr_agg_merge_map_and_sum_values\n",
      "  dr_agg_seq\n",
      "  dr_agg_seq_concat\n",
      "  dr_agg_to_occurr_map\n",
      "  dr_agg_to_occurr_map_hashed\n",
      "  dr_categorical_stats\n",
      "  dr_col_to_map_with_double\n",
      "  dr_col_to_map_with_str\n",
      "  dr_entropy_from_map\n",
      "  dr_get_max_value_key\n",
      "  dr_join_first_vals\n",
      "  dr_list_to_map_cnt\n",
      "  dr_map_array_with_long\n",
      "  dr_map_to_json\n",
      "  dr_mecab_tokenizer\n",
      "  dr_numeric_rounding\n",
      "  dr_regexp_match_all\n",
      "  dr_seq_to_ngram_map\n",
      "24/10/04 09:30:09 INFO InMemoryFileIndex: It took 16 ms to list leaf files for 1 paths.\n",
      "24/10/04 09:30:09 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/10/04 09:30:09 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:09 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#6, None)) > 0)\n",
      "24/10/04 09:30:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "24/10/04 09:30:09 INFO CodeGenerator: Code generated in 9.984458 ms\n",
      "24/10/04 09:30:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 339.5 KiB, free 366.0 MiB)\n",
      "24/10/04 09:30:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)\n",
      "24/10/04 09:30:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.3 MiB)\n",
      "24/10/04 09:30:09 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:09 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:09 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 365.9 MiB)\n",
      "24/10/04 09:30:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)\n",
      "24/10/04 09:30:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c37eddbb00a9:40107 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "24/10/04 09:30:09 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4864 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.22.0.3:45645 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "24/10/04 09:30:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 366.3 MiB)\n",
      "24/10/04 09:30:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 561 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:10 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.603 s\n",
      "24/10/04 09:30:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/10/04 09:30:10 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.624220 s\n",
      "24/10/04 09:30:10 INFO CodeGenerator: Code generated in 4.089958 ms\n",
      "24/10/04 09:30:10 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:10 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:10 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "24/10/04 09:30:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 339.5 KiB, free 365.6 MiB)\n",
      "24/10/04 09:30:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.6 MiB)\n",
      "24/10/04 09:30:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:10 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:10 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:10 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:10 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.5 KiB, free 365.5 MiB)\n",
      "24/10/04 09:30:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 365.5 MiB)\n",
      "24/10/04 09:30:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c37eddbb00a9:40107 (size: 8.4 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4864 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.22.0.4:35591 (size: 8.4 KiB, free: 366.3 MiB)\n",
      "24/10/04 09:30:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.3 MiB)\n",
      "24/10/04 09:30:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1047 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:11 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.063 s\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 1.066539 s\n",
      "\n",
      "Loaded CSV file /opt/spark-apps/LC_train.csv into table primary_dataset\n",
      "24/10/04 09:30:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:11 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
      "24/10/04 09:30:11 INFO CodeGenerator: Code generated in 5.63075 ms\n",
      "24/10/04 09:30:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 339.4 KiB, free 365.2 MiB)\n",
      "24/10/04 09:30:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.2 MiB)\n",
      "24/10/04 09:30:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:11 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Registering RDD 13 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 365.2 MiB)\n",
      "24/10/04 09:30:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 365.1 MiB)\n",
      "24/10/04 09:30:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on c37eddbb00a9:40107 (size: 8.0 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:11 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:11 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4853 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.22.0.4:35591 (size: 8.0 KiB, free: 366.3 MiB)\n",
      "24/10/04 09:30:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 92 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:11 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:11 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.102 s\n",
      "24/10/04 09:30:11 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:11 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:11 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:11 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:11 INFO CodeGenerator: Code generated in 5.366125 ms\n",
      "24/10/04 09:30:11 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Final stage: ResultStage 4 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.0 KiB, free 365.1 MiB)\n",
      "24/10/04 09:30:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.1 MiB)\n",
      "24/10/04 09:30:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on c37eddbb00a9:40107 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:11 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.22.0.4:35591 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.22.0.4:36490\n",
      "24/10/04 09:30:11 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 65 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:11 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:11 INFO DAGScheduler: ResultStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.071 s\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/10/04 09:30:11 INFO DAGScheduler: Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.077432 s\n",
      "\n",
      "Table primary_dataset rows: 9499\n",
      "24/10/04 09:30:11 INFO CodeGenerator: Code generated in 3.949792 ms\n",
      "\n",
      "Table primary_dataset structure:\n",
      "  CustomerID: string\n",
      "  BadLoan: string\n",
      "  date: string\n",
      "24/10/04 09:30:11 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/10/04 09:30:11 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#45, None)) > 0)\n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 339.5 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 7 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:12 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Got job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Final stage: ResultStage 5 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[20] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 11.6 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 364.7 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on c37eddbb00a9:40107 (size: 5.8 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4866 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.22.0.3:45645 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 39 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:12 INFO DAGScheduler: ResultStage 5 (csv at NativeMethodAccessorImpl.java:0) finished in 0.043 s\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Job 4 finished: csv at NativeMethodAccessorImpl.java:0, took 0.045324 s\n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 339.5 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 9 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:12 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 16.7 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on c37eddbb00a9:40107 (size: 8.6 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4866 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.22.0.4:35591 (size: 8.6 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 76 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:12 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.081 s\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.082849 s\n",
      "\n",
      "Loaded CSV file /opt/spark-apps/LC_profile.csv into table LC_profile\n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 339.4 KiB, free 364.0 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 364.0 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 11 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Registering RDD 30 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[30] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 16.2 KiB, free 364.0 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 364.0 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on c37eddbb00a9:40107 (size: 8.3 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[30] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4855 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.22.0.4:35591 (size: 8.3 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 30 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:12 INFO DAGScheduler: ShuffleMapStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.036 s\n",
      "24/10/04 09:30:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:12 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:12 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Final stage: ResultStage 9 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[33] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 11.0 KiB, free 364.0 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 364.0 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on c37eddbb00a9:40107 (size: 5.5 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[33] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.22.0.4:35591 (size: 5.5 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.22.0.4:36490\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 16 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:12 INFO DAGScheduler: ResultStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.019 s\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.021862 s\n",
      "\n",
      "Table LC_profile rows: 10000\n",
      "\n",
      "Table LC_profile structure:\n",
      "  CustomerID: string\n",
      "  loan_amnt: int\n",
      "  funded_amnt: int\n",
      "  term: string\n",
      "  int_rate: string\n",
      "  installment: double\n",
      "  grade: string\n",
      "  sub_grade: string\n",
      "  emp_title: string\n",
      "  emp_length: string\n",
      "  home_ownership: string\n",
      "  annual_inc: string\n",
      "  verification_status: string\n",
      "  purpose: string\n",
      "  zip_code: string\n",
      "  addr_state: string\n",
      "24/10/04 09:30:12 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/10/04 09:30:12 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#136, None)) > 0)\n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 339.5 KiB, free 363.6 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 363.6 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 14 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13901819 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:12 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Got job 8 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Final stage: ResultStage 10 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[37] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 11.6 KiB, free 363.6 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 363.6 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on c37eddbb00a9:40107 (size: 5.8 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[37] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4871 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.22.0.3:45645 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 33 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:12 INFO DAGScheduler: ResultStage 10 (csv at NativeMethodAccessorImpl.java:0) finished in 0.038 s\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Job 8 finished: csv at NativeMethodAccessorImpl.java:0, took 0.039889 s\n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:12 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 339.5 KiB, free 363.2 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 363.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 16 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13901819 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:12 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Got job 9 (csv at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Final stage: ResultStage 11 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[43] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.5 KiB, free 363.2 MiB)\n",
      "24/10/04 09:30:12 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 363.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on c37eddbb00a9:40107 (size: 8.4 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:12 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[43] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:12 INFO TaskSchedulerImpl: Adding task set 11.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4871 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 10) (172.22.0.3, executor 1, partition 1, PROCESS_LOCAL, 4871 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.22.0.4:35591 (size: 8.4 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c37eddbb00a9:40107 in memory (size: 5.8 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.22.0.3:45645 (size: 8.4 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.22.0.3:45645 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_14_piece0 on c37eddbb00a9:40107 in memory (size: 32.7 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.22.0.3:45645 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on c37eddbb00a9:40107 in memory (size: 8.4 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.22.0.4:35591 in memory (size: 8.4 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_8_piece0 on c37eddbb00a9:40107 in memory (size: 5.8 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.22.0.3:45645 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on c37eddbb00a9:40107 in memory (size: 8.0 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.22.0.4:35591 in memory (size: 8.0 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.22.0.3:45645 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_15_piece0 on c37eddbb00a9:40107 in memory (size: 5.8 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_4_piece0 on c37eddbb00a9:40107 in memory (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.22.0.4:35591 in memory (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_12_piece0 on c37eddbb00a9:40107 in memory (size: 8.3 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.22.0.4:35591 in memory (size: 8.3 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on c37eddbb00a9:40107 in memory (size: 5.5 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.22.0.4:35591 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_13_piece0 on c37eddbb00a9:40107 in memory (size: 5.5 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.22.0.4:35591 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on c37eddbb00a9:40107 in memory (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.22.0.4:35591 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 310 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_11_piece0 on c37eddbb00a9:40107 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.22.0.4:35591 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on c37eddbb00a9:40107 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.22.0.3:45645 in memory (size: 32.7 KiB, free: 366.3 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_10_piece0 on c37eddbb00a9:40107 in memory (size: 8.6 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:12 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.22.0.4:35591 in memory (size: 8.6 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 10) in 844 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:13 INFO DAGScheduler: ResultStage 11 (csv at NativeMethodAccessorImpl.java:0) finished in 0.860 s\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Job 9 finished: csv at NativeMethodAccessorImpl.java:0, took 0.861582 s\n",
      "\n",
      "Loaded CSV file /opt/spark-apps/LC_transactions.csv into table LC_transactions\n",
      "24/10/04 09:30:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:13 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 339.4 KiB, free 364.9 MiB)\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO SparkContext: Created broadcast 18 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13901819 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Registering RDD 47 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[47] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 15.6 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on c37eddbb00a9:40107 (size: 8.0 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[47] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 11) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 12) (172.22.0.3, executor 1, partition 1, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.22.0.4:35591 (size: 8.0 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.22.0.3:45645 (size: 8.0 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 11) in 75 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 12) in 149 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:13 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.155 s\n",
      "24/10/04 09:30:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:13 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:13 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[50] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 11.0 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on c37eddbb00a9:40107 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[50] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.22.0.4:35591 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.22.0.4:36490\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 21 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:13 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.023 s\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.025336 s\n",
      "\n",
      "Table LC_transactions rows: 412459\n",
      "\n",
      "Table LC_transactions structure:\n",
      "  CustomerID: string\n",
      "  AccountID: string\n",
      "  Date: string\n",
      "  Amount: string\n",
      "  Description: string\n",
      "24/10/04 09:30:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:13 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 339.4 KiB, free 364.5 MiB)\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:13 INFO SparkContext: Created broadcast 21 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Got map stage job 12 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.5 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on c37eddbb00a9:40107 (size: 8.0 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:13 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4853 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.22.0.3:45645 (size: 8.0 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 29 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:13 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.033 s\n",
      "24/10/04 09:30:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:13 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:13 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Got job 13 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[57] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 11.0 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:13 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on c37eddbb00a9:40107 (size: 5.5 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:13 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[57] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 15) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:13 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.22.0.3:45645 (size: 5.5 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.22.0.3:60010\n",
      "24/10/04 09:30:13 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 15) in 64 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:13 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.067 s\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "24/10/04 09:30:13 INFO DAGScheduler: Job 13 finished: count at NativeMethodAccessorImpl.java:0, took 0.069373 s\n",
      "\n",
      "Table DR_PRIMARY_TABLE rows: 9499\n",
      "\n",
      "Table DR_PRIMARY_TABLE structure:\n",
      "  CustomerID: string\n",
      "  BadLoan: string\n",
      "  date: string\n",
      "\n",
      "Running SQL Block: Create \"DR_PRIMARY_TABLE\" table with prediction point\n",
      "\n",
      "Running SQL Block: Create filtered \"LC_profile\" table\n",
      "\n",
      "Running SQL Block: Create \"LC_profile (by {\"CustomerID\"}-{\"CustomerID\"})\" table with engineered features\n",
      "\n",
      "Running SQL Block: Create \"DR_PRIMARY_TABLE\" table with engineered features from lookup tables\n",
      "\n",
      "Running SQL Block: Create filtered \"LC_transactions\" table (30 days)\n",
      "\n",
      "Running SQL Block: Create filtered \"LC_transactions\" table (1 week)\n",
      "\n",
      "Running SQL Block: Create \"LC_transactions (by {\"CustomerID\"}-{\"CustomerID\"})\" table with engineered features (30 days)\n",
      "\n",
      "Running SQL Block: Create \"LC_transactions (by {\"CustomerID\"}-{\"CustomerID\"})\" table with engineered features (1 week)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_19_piece0 on c37eddbb00a9:40107 in memory (size: 8.0 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.22.0.4:35591 in memory (size: 8.0 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.22.0.3:45645 in memory (size: 8.0 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.22.0.3:45645 in memory (size: 8.0 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_22_piece0 on c37eddbb00a9:40107 in memory (size: 8.0 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_23_piece0 on c37eddbb00a9:40107 in memory (size: 5.5 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.22.0.3:45645 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_18_piece0 on c37eddbb00a9:40107 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.22.0.3:45645 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.22.0.4:35591 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_20_piece0 on c37eddbb00a9:40107 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.22.0.4:35591 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_17_piece0 on c37eddbb00a9:40107 in memory (size: 8.4 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.22.0.4:35591 in memory (size: 8.4 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.22.0.3:45645 in memory (size: 8.4 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_21_piece0 on c37eddbb00a9:40107 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "\n",
      "Running SQL Block: Create \"LC_transactions (aggregated by {\"CustomerID\"}-{\"CustomerID\"}) (FDW 1 week)\" table with engineered features (1 week)\n",
      "24/10/04 09:30:14 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.22.0.3:45645 in memory (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "\n",
      "Running SQL Block: Create \"LC_transactions (aggregated by {\"CustomerID\"}-{\"CustomerID\"}) (FDW 30 days)\" table with engineered features (30 days)\n",
      "\n",
      "Running SQL Block: Create \"DR_PRIMARY_TABLE\" table with engineered features\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, date: string>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#1852)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(gettimestamp(from_unixtime(cast(CheckOverflow((promote_precision(cast(FLOOR(CheckOverflow((promote_precision(cast(cast(unix_timestamp(gettimestamp(date#2701, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as decimal(20,0)) as decimal(21,1))) / 60.0), DecimalType(27,6), true)) as decimal(23,1))) * 60.0), DecimalType(26,1), true) as bigint), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM-dd HH:mm:ss, TimestampType, Some(Etc/UTC), false)),isnotnull(CustomerID#2699)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, date: string>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2058)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, AccountID: string, Date: string, Amount: string, Description: string ... 3 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(gettimestamp(from_unixtime(cast(CheckOverflow((promote_precision(cast(FLOOR(CheckOverflow((promote_precision(cast(cast(unix_timestamp(gettimestamp(date#2720, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as decimal(20,0)) as decimal(21,1))) / 60.0), DecimalType(27,6), true)) as decimal(23,1))) * 60.0), DecimalType(26,1), true) as bigint), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM-dd HH:mm:ss, TimestampType, Some(Etc/UTC), false)),isnotnull(CustomerID#2718)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, date: string>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2397)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, AccountID: string, Date: string, Amount: string, Description: string ... 3 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(gettimestamp(from_unixtime(cast(CheckOverflow((promote_precision(cast(FLOOR(CheckOverflow((promote_precision(cast(cast(unix_timestamp(gettimestamp(date#2560, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as decimal(20,0)) as decimal(21,1))) / 60.0), DecimalType(27,6), true)) as decimal(23,1))) * 60.0), DecimalType(26,1), true) as bigint), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM-dd HH:mm:ss, TimestampType, Some(Etc/UTC), false)),isnotnull(CustomerID#2558)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, date: string>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2552)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, AccountID: string, Date: string, Amount: string, Description: string ... 3 more fields>\n",
      "24/10/04 09:30:15 INFO CodeGenerator: Code generated in 15.150208 ms\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 339.4 KiB, free 364.9 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 24 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Registering RDD 61 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Got map stage job 14 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[61] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:15 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 20.9 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 364.8 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on c37eddbb00a9:40107 (size: 10.1 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[61] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:15 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:15 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 16) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4853 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.22.0.3:45645 (size: 10.1 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:15 INFO CodeGenerator: Code generated in 15.031125 ms\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 339.4 KiB, free 364.5 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 364.5 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 26 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Registering RDD 65 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Got map stage job 15 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[65] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 28.0 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 364.4 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on c37eddbb00a9:40107 (size: 13.5 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[65] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:15 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:15 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 17) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4853 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.22.0.4:35591 (size: 13.5 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:15 INFO CodeGenerator: Code generated in 31.461708 ms\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 339.4 KiB, free 364.1 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 364.0 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 28 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Registering RDD 69 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Got map stage job 16 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 25.0 KiB, free 364.0 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 364.0 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on c37eddbb00a9:40107 (size: 11.2 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:15 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:15 INFO CodeGenerator: Code generated in 26.101958 ms\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 339.4 KiB, free 363.7 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 363.6 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 30 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13901819 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Registering RDD 73 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Got map stage job 17 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Final stage: ShuffleMapStage 21 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[73] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 23.3 KiB, free 363.6 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 363.6 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on c37eddbb00a9:40107 (size: 10.4 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[73] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:15 INFO TaskSchedulerImpl: Adding task set 21.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:15 INFO CodeGenerator: Code generated in 17.194 ms\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 339.4 KiB, free 363.3 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 363.3 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 32 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13901819 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Registering RDD 77 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 8\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Got map stage job 18 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[77] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 23.3 KiB, free 363.2 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 363.2 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on c37eddbb00a9:40107 (size: 10.4 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[77] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:15 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:15 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 18) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4853 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:15 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 17) in 241 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:15 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:15 INFO DAGScheduler: ShuffleMapStage 19 (count at NativeMethodAccessorImpl.java:0) finished in 0.247 s\n",
      "24/10/04 09:30:15 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:15 INFO DAGScheduler: running: Set(ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 18, ShuffleMapStage 22)\n",
      "24/10/04 09:30:15 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.22.0.4:35591 (size: 11.2 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:15 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:15 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:15 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:15 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO CodeGenerator: Code generated in 18.316333 ms\n",
      "24/10/04 09:30:15 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 31.7 KiB, free 363.2 MiB)\n",
      "24/10/04 09:30:15 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 363.2 MiB)\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on c37eddbb00a9:40107 (size: 15.3 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:15 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:15 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:15 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 19) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:15 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 16) in 471 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:15 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:15 INFO DAGScheduler: ShuffleMapStage 18 (count at NativeMethodAccessorImpl.java:0) finished in 0.477 s\n",
      "24/10/04 09:30:15 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:15 INFO DAGScheduler: running: Set(ShuffleMapStage 20, ResultStage 24, ShuffleMapStage 21, ShuffleMapStage 22)\n",
      "24/10/04 09:30:15 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:15 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.22.0.3:45645 (size: 10.4 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:15 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:15 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 20) (172.22.0.4, executor 0, partition 1, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 18) in 496 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:16 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:16 INFO DAGScheduler: ShuffleMapStage 20 (count at NativeMethodAccessorImpl.java:0) finished in 0.689 s\n",
      "24/10/04 09:30:16 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:16 INFO DAGScheduler: running: Set(ResultStage 24, ShuffleMapStage 21, ShuffleMapStage 22)\n",
      "24/10/04 09:30:16 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:16 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.22.0.4:35591 (size: 10.4 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 21) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 19) in 544 ms on 172.22.0.3 (executor 1) (1/2)\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.22.0.3:45645 (size: 10.4 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 22) (172.22.0.4, executor 0, partition 1, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 20) in 402 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:16 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:16 INFO DAGScheduler: ShuffleMapStage 21 (count at NativeMethodAccessorImpl.java:0) finished in 1.012 s\n",
      "24/10/04 09:30:16 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:16 INFO DAGScheduler: running: Set(ResultStage 24, ShuffleMapStage 22)\n",
      "24/10/04 09:30:16 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:16 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.22.0.4:35591 (size: 10.4 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 21) in 415 ms on 172.22.0.3 (executor 1) (1/2)\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 23) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 22) in 298 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:16 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:16 INFO DAGScheduler: ShuffleMapStage 22 (count at NativeMethodAccessorImpl.java:0) finished in 1.277 s\n",
      "24/10/04 09:30:16 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:16 INFO DAGScheduler: running: Set(ResultStage 24)\n",
      "24/10/04 09:30:16 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:16 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.22.0.4:35591 (size: 15.3 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.22.0.4:36490\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 23) in 52 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:16 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:16 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1.096 s\n",
      "24/10/04 09:30:16 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "24/10/04 09:30:16 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1.101321 s\n",
      "24/10/04 09:30:16 INFO CodeGenerator: Code generated in 3.36925 ms\n",
      "24/10/04 09:30:16 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 1536.0 KiB, free 361.7 MiB)\n",
      "24/10/04 09:30:16 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 141.9 KiB, free 361.5 MiB)\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on c37eddbb00a9:40107 (size: 141.9 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:16 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:16 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:16 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:16 INFO CodeGenerator: Code generated in 23.117125 ms\n",
      "24/10/04 09:30:16 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 339.4 KiB, free 361.2 MiB)\n",
      "24/10/04 09:30:16 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 361.2 MiB)\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:16 INFO SparkContext: Created broadcast 36 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:16 INFO DAGScheduler: Registering RDD 84 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9\n",
      "24/10/04 09:30:16 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:16 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:16 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:16 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[84] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:16 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 62.7 KiB, free 361.1 MiB)\n",
      "24/10/04 09:30:16 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 361.1 MiB)\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on c37eddbb00a9:40107 (size: 25.4 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:16 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[84] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:16 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:16 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 24) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4855 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:16 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.22.0.3:45645 (size: 25.4 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.22.0.3:45645 (size: 141.9 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 24) in 222 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:17 INFO DAGScheduler: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.225 s\n",
      "24/10/04 09:30:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:17 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 10.387333 ms\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Registering RDD 87 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Got map stage job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[87] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 59.8 KiB, free 361.0 MiB)\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 361.0 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on c37eddbb00a9:40107 (size: 23.8 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:17 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[87] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:17 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 25) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.22.0.3:45645 (size: 23.8 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.22.0.3:60010\n",
      "24/10/04 09:30:17 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 25) in 58 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:17 INFO DAGScheduler: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0) finished in 0.062 s\n",
      "24/10/04 09:30:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:17 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 3.434417 ms\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 4.741166 ms\n",
      "24/10/04 09:30:17 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Got job 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[92] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 63.0 KiB, free 360.9 MiB)\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 360.9 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on c37eddbb00a9:40107 (size: 25.5 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:17 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[92] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:17 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 26) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.22.0.3:45645 (size: 25.5 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.22.0.3:60010\n",
      "24/10/04 09:30:17 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 26) in 129 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:17 INFO DAGScheduler: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.132 s\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Job 22 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.135553 s\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 1536.0 KiB, free 359.4 MiB)\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 150.8 KiB, free 359.3 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on c37eddbb00a9:40107 (size: 150.8 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:17 INFO SparkContext: Created broadcast 40 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(6, 7), advisory target size: 67108864, actual target size 8583530, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(6, 8), advisory target size: 67108864, actual target size 8505649, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO ShufflePartitionsUtil: For shuffle(6, 8), advisory target size: 67108864, actual target size 8505649, minimum partition size: 1048576\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_31_piece0 on c37eddbb00a9:40107 in memory (size: 10.4 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.22.0.4:35591 in memory (size: 10.4 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.22.0.3:45645 in memory (size: 10.4 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 4.620166 ms\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_38_piece0 on c37eddbb00a9:40107 in memory (size: 23.8 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.22.0.3:45645 in memory (size: 23.8 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Registering RDD 95 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 11\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Got map stage job 23 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[95] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 13.7 KiB, free 359.4 MiB)\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 359.4 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on c37eddbb00a9:40107 (size: 6.9 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:17 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_34_piece0 on c37eddbb00a9:40107 in memory (size: 15.3 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[95] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:17 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 27) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4465 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.22.0.4:35591 in memory (size: 15.3 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.22.0.3:45645 in memory (size: 25.4 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_37_piece0 on c37eddbb00a9:40107 in memory (size: 25.4 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.22.0.3:45645 (size: 6.9 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.22.0.3:60010\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_39_piece0 on c37eddbb00a9:40107 in memory (size: 25.5 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.22.0.3:45645 in memory (size: 25.5 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.22.0.3:45645 (size: 150.8 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_33_piece0 on c37eddbb00a9:40107 in memory (size: 10.4 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.22.0.4:35591 in memory (size: 10.4 KiB, free: 366.1 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.22.0.3:45645 in memory (size: 10.4 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 38.062791 ms\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 26.89625 ms\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 4.329042 ms\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Registering RDD 102 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 12\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Got map stage job 24 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Final stage: ShuffleMapStage 35 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33, ShuffleMapStage 34)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[102] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 136.1 KiB, free 359.5 MiB)\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 52.3 KiB, free 359.4 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on c37eddbb00a9:40107 (size: 52.3 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:17 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[102] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Adding task set 35.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:17 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 28) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.22.0.4:35591 (size: 52.3 KiB, free: 366.0 MiB)\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 21.484 ms\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 27) in 157 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:17 INFO DAGScheduler: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0) finished in 0.164 s\n",
      "24/10/04 09:30:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:17 INFO DAGScheduler: running: Set(ShuffleMapStage 35)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 22.808125 ms\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 5.323291 ms\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Registering RDD 109 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 13\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Got map stage job 25 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Final stage: ShuffleMapStage 37 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34, ShuffleMapStage 36)\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 137.1 KiB, free 359.3 MiB)\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 52.6 KiB, free 359.2 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on c37eddbb00a9:40107 (size: 52.6 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:17 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Adding task set 37.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 13.432291 ms\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.22.0.4:36490\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 24.350625 ms\n",
      "24/10/04 09:30:17 INFO CodeGenerator: Code generated in 8.476584 ms\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Registering RDD 116 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 14\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Got map stage job 26 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Final stage: ShuffleMapStage 38 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34, ShuffleMapStage 36)\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 138.4 KiB, free 359.1 MiB)\n",
      "24/10/04 09:30:17 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 53.0 KiB, free 359.1 MiB)\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on c37eddbb00a9:40107 (size: 53.0 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:17 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:17 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:17 INFO TaskSchedulerImpl: Adding task set 38.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:17 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.22.0.4:35591 (size: 150.8 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.22.0.4:36490\n",
      "24/10/04 09:30:18 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 29) (172.22.0.4, executor 0, partition 1, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:18 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 28) in 521 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:18 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 30) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:18 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 29) in 167 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:18 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:18 INFO DAGScheduler: ShuffleMapStage 35 (count at NativeMethodAccessorImpl.java:0) finished in 0.694 s\n",
      "24/10/04 09:30:18 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:18 INFO DAGScheduler: running: Set(ShuffleMapStage 37, ShuffleMapStage 38)\n",
      "24/10/04 09:30:18 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:18 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:18 INFO ShufflePartitionsUtil: For shuffle(12), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:18 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.22.0.4:35591 (size: 52.6 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.22.0.4:36490\n",
      "24/10/04 09:30:18 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:18 INFO CodeGenerator: Code generated in 10.839375 ms\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Registering RDD 119 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 15\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Got map stage job 27 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Final stage: ShuffleMapStage 41 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.22.0.4:36490\n",
      "24/10/04 09:30:18 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 103.1 KiB, free 359.0 MiB)\n",
      "24/10/04 09:30:18 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 39.3 KiB, free 358.9 MiB)\n",
      "24/10/04 09:30:18 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on c37eddbb00a9:40107 (size: 39.3 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:18 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:18 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:18 INFO TaskSetManager: Starting task 1.0 in stage 37.0 (TID 31) (172.22.0.4, executor 0, partition 1, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:18 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 30) in 330 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:18 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 32) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:18 INFO TaskSetManager: Finished task 1.0 in stage 37.0 (TID 31) in 199 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:18 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:18 INFO DAGScheduler: ShuffleMapStage 37 (count at NativeMethodAccessorImpl.java:0) finished in 1.141 s\n",
      "24/10/04 09:30:18 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:18 INFO DAGScheduler: running: Set(ShuffleMapStage 38, ShuffleMapStage 41)\n",
      "24/10/04 09:30:18 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:18 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:18 INFO ShufflePartitionsUtil: For shuffle(13), advisory target size: 67108864, actual target size 2485069, minimum partition size: 1048576\n",
      "24/10/04 09:30:18 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.22.0.4:35591 (size: 53.0 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:18 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:18 INFO CodeGenerator: Code generated in 5.766416 ms\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Registering RDD 122 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 16\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Got map stage job 28 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Final stage: ShuffleMapStage 43 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[122] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:18 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 104.2 KiB, free 358.8 MiB)\n",
      "24/10/04 09:30:18 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 39.8 KiB, free 358.8 MiB)\n",
      "24/10/04 09:30:18 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on c37eddbb00a9:40107 (size: 39.8 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:18 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[122] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:18 INFO TaskSchedulerImpl: Adding task set 43.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 33) (172.22.0.4, executor 0, partition 1, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 32) in 297 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 34) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 33) in 219 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:19 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:19 INFO DAGScheduler: ShuffleMapStage 38 (count at NativeMethodAccessorImpl.java:0) finished in 1.580 s\n",
      "24/10/04 09:30:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:19 INFO DAGScheduler: running: Set(ShuffleMapStage 43, ShuffleMapStage 41)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:19 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:19 INFO ShufflePartitionsUtil: For shuffle(14), advisory target size: 67108864, actual target size 4914323, minimum partition size: 1048576\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.22.0.4:35591 (size: 39.3 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 172.22.0.4:36490\n",
      "24/10/04 09:30:19 INFO CodeGenerator: Code generated in 3.960792 ms\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Registering RDD 125 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 17\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Got map stage job 29 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Final stage: ShuffleMapStage 45 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[125] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:19 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 13.1 KiB, free 358.8 MiB)\n",
      "24/10/04 09:30:19 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 358.8 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on c37eddbb00a9:40107 (size: 6.5 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:19 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[125] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:19 INFO TaskSchedulerImpl: Adding task set 45.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 35) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 34) in 74 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:19 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:19 INFO DAGScheduler: ShuffleMapStage 41 (count at NativeMethodAccessorImpl.java:0) finished in 1.079 s\n",
      "24/10/04 09:30:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:19 INFO DAGScheduler: running: Set(ShuffleMapStage 45, ShuffleMapStage 43)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:19 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:19 INFO ShufflePartitionsUtil: For shuffle(15), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.22.0.4:35591 (size: 39.8 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 172.22.0.4:36490\n",
      "24/10/04 09:30:19 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:19 INFO CodeGenerator: Code generated in 7.221458 ms\n",
      "24/10/04 09:30:19 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Got job 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Final stage: ResultStage 49 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[128] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/10/04 09:30:19 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 105.4 KiB, free 358.7 MiB)\n",
      "24/10/04 09:30:19 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 358.6 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on c37eddbb00a9:40107 (size: 39.9 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:19 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[128] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:19 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Starting task 1.0 in stage 43.0 (TID 36) (172.22.0.4, executor 0, partition 1, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 35) in 71 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 37) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Finished task 1.0 in stage 43.0 (TID 36) in 42 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:19 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:19 INFO DAGScheduler: ShuffleMapStage 43 (count at NativeMethodAccessorImpl.java:0) finished in 0.664 s\n",
      "24/10/04 09:30:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:19 INFO DAGScheduler: running: Set(ShuffleMapStage 45, ResultStage 49)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:19 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.22.0.4:35591 (size: 6.5 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 172.22.0.4:36490\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Starting task 1.0 in stage 45.0 (TID 38) (172.22.0.4, executor 0, partition 1, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 37) in 88 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 39) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Finished task 1.0 in stage 45.0 (TID 38) in 76 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:19 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:19 INFO DAGScheduler: ShuffleMapStage 45 (count at NativeMethodAccessorImpl.java:0) finished in 0.327 s\n",
      "24/10/04 09:30:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:19 INFO DAGScheduler: running: Set(ResultStage 49)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:19 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:19 INFO ShufflePartitionsUtil: For shuffle(17), advisory target size: 67108864, actual target size 5031336, minimum partition size: 1048576\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.22.0.4:35591 (size: 39.9 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 172.22.0.4:36490\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_44_piece0 on c37eddbb00a9:40107 in memory (size: 53.0 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.22.0.4:35591 in memory (size: 53.0 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_42_piece0 on c37eddbb00a9:40107 in memory (size: 52.3 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.22.0.4:35591 in memory (size: 52.3 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 39) in 30 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:19 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:19 INFO DAGScheduler: ResultStage 49 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.266 s\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Job 30 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.270908 s\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_43_piece0 on c37eddbb00a9:40107 in memory (size: 52.6 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:19 INFO CodeGenerator: Code generated in 11.544459 ms\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.22.0.4:35591 in memory (size: 52.6 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_45_piece0 on c37eddbb00a9:40107 in memory (size: 39.3 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.22.0.4:35591 in memory (size: 39.3 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:19 INFO CodeGenerator: Code generated in 6.836708 ms\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_41_piece0 on c37eddbb00a9:40107 in memory (size: 6.9 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.22.0.3:45645 in memory (size: 6.9 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:19 INFO CodeGenerator: Code generated in 5.0095 ms\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_46_piece0 on c37eddbb00a9:40107 in memory (size: 39.8 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.22.0.4:35591 in memory (size: 39.8 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:19 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 1280.0 KiB, free 358.2 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_47_piece0 on c37eddbb00a9:40107 in memory (size: 6.5 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.22.0.4:35591 in memory (size: 6.5 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:19 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 154.9 KiB, free 358.1 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on c37eddbb00a9:40107 (size: 154.9 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:19 INFO SparkContext: Created broadcast 49 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Registering RDD 133 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 18\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Got map stage job 31 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Final stage: ShuffleMapStage 54 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 53)\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[133] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:19 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 108.0 KiB, free 358.0 MiB)\n",
      "24/10/04 09:30:19 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 41.5 KiB, free 357.9 MiB)\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on c37eddbb00a9:40107 (size: 41.5 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:19 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:19 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[133] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:19 INFO TaskSchedulerImpl: Adding task set 54.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:19 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 40) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:19 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.22.0.4:35591 (size: 41.5 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 172.22.0.4:36490\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Starting task 1.0 in stage 54.0 (TID 41) (172.22.0.4, executor 0, partition 1, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 40) in 251 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Finished task 1.0 in stage 54.0 (TID 41) in 112 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:20 INFO DAGScheduler: ShuffleMapStage 54 (count at NativeMethodAccessorImpl.java:0) finished in 0.366 s\n",
      "24/10/04 09:30:20 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:20 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:20 INFO ShufflePartitionsUtil: For shuffle(18), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:20 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Got job 32 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Final stage: ResultStage 60 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 59)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[135] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 7.2 KiB, free 357.9 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 357.9 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on c37eddbb00a9:40107 (size: 3.8 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[135] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 42) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4473 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.22.0.4:35591 (size: 3.8 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 172.22.0.4:36490\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 42) in 11 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:20 INFO DAGScheduler: ResultStage 60 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.015 s\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Job 32 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.016466 s\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 1536.0 KiB, free 356.4 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 189.0 KiB, free 356.3 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on c37eddbb00a9:40107 (size: 189.0 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 52 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/10/04 09:30:20 INFO ShufflePartitionsUtil: For shuffle(11, 16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:20 INFO CodeGenerator: Code generated in 4.774292 ms\n",
      "24/10/04 09:30:20 INFO CodeGenerator: Code generated in 4.052042 ms\n",
      "24/10/04 09:30:20 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:20 INFO CodeGenerator: Code generated in 5.261 ms\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Registering RDD 142 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 19\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Got map stage job 33 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Final stage: ShuffleMapStage 67 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 66, ShuffleMapStage 65)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting ShuffleMapStage 67 (MapPartitionsRDD[142] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 187.7 KiB, free 356.1 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 68.3 KiB, free 356.0 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on c37eddbb00a9:40107 (size: 68.3 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 67 (MapPartitionsRDD[142] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 43) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.22.0.3:45645 (size: 68.3 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 172.22.0.3:60010\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.22.0.3:45645 (size: 154.9 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 172.22.0.3:60010\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.22.0.3:45645 (size: 189.0 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 43) in 149 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:20 INFO DAGScheduler: ShuffleMapStage 67 (count at NativeMethodAccessorImpl.java:0) finished in 0.155 s\n",
      "24/10/04 09:30:20 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:20 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:20 INFO CodeGenerator: Code generated in 3.928375 ms\n",
      "24/10/04 09:30:20 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Got job 34 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Final stage: ResultStage 75 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 74)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[145] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 11.0 KiB, free 356.0 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 356.0 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on c37eddbb00a9:40107 (size: 5.5 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[145] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 44) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.22.0.3:45645 (size: 5.5 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 172.22.0.3:60010\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 44) in 24 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:20 INFO DAGScheduler: ResultStage 75 (count at NativeMethodAccessorImpl.java:0) finished in 0.027 s\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Job 34 finished: count at NativeMethodAccessorImpl.java:0, took 0.028728 s\n",
      "\n",
      "Result has 9499 records\n",
      "WARNING - saving to local csv is not scalable and is only implemented for testing\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, BadLoan: string, date: string ... 1 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#1852)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(gettimestamp(from_unixtime(cast(CheckOverflow((promote_precision(cast(FLOOR(CheckOverflow((promote_precision(cast(cast(unix_timestamp(gettimestamp(date#2701, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as decimal(20,0)) as decimal(21,1))) / 60.0), DecimalType(27,6), true)) as decimal(23,1))) * 60.0), DecimalType(26,1), true) as bigint), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM-dd HH:mm:ss, TimestampType, Some(Etc/UTC), false)),isnotnull(CustomerID#2699)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, date: string>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2058)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, AccountID: string, Date: string, Amount: string, Description: string ... 3 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(gettimestamp(from_unixtime(cast(CheckOverflow((promote_precision(cast(FLOOR(CheckOverflow((promote_precision(cast(cast(unix_timestamp(gettimestamp(date#2720, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as decimal(20,0)) as decimal(21,1))) / 60.0), DecimalType(27,6), true)) as decimal(23,1))) * 60.0), DecimalType(26,1), true) as bigint), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM-dd HH:mm:ss, TimestampType, Some(Etc/UTC), false)),isnotnull(CustomerID#2718)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, date: string>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2397)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, AccountID: string, Date: string, Amount: string, Description: string ... 3 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(gettimestamp(from_unixtime(cast(CheckOverflow((promote_precision(cast(FLOOR(CheckOverflow((promote_precision(cast(cast(unix_timestamp(gettimestamp(date#2560, yyyy-MM-dd, TimestampType, Some(Etc/UTC), false), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as decimal(20,0)) as decimal(21,1))) / 60.0), DecimalType(27,6), true)) as decimal(23,1))) * 60.0), DecimalType(26,1), true) as bigint), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM-dd HH:mm:ss, TimestampType, Some(Etc/UTC), false)),isnotnull(CustomerID#2558)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, date: string>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2552)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, AccountID: string, Date: string, Amount: string, Description: string ... 3 more fields>\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_50_piece0 on c37eddbb00a9:40107 in memory (size: 41.5 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.22.0.4:35591 in memory (size: 41.5 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_51_piece0 on c37eddbb00a9:40107 in memory (size: 3.8 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.22.0.4:35591 in memory (size: 3.8 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_48_piece0 on c37eddbb00a9:40107 in memory (size: 39.9 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.22.0.4:35591 in memory (size: 39.9 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_53_piece0 on c37eddbb00a9:40107 in memory (size: 68.3 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.22.0.3:45645 in memory (size: 68.3 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_54_piece0 on c37eddbb00a9:40107 in memory (size: 5.5 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.22.0.3:45645 in memory (size: 5.5 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:20 INFO CodeGenerator: Code generated in 3.41175 ms\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 339.4 KiB, free 356.2 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 356.2 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 55 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Registering RDD 149 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 20\n",
      "24/10/04 09:30:20 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Got map stage job 35 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Final stage: ShuffleMapStage 76 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting ShuffleMapStage 76 (MapPartitionsRDD[149] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 21.5 KiB, free 356.2 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 356.2 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on c37eddbb00a9:40107 (size: 10.2 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 339.4 KiB, free 355.8 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 76 (MapPartitionsRDD[149] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 45) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4853 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 355.8 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 57 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Registering RDD 153 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 21\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Got map stage job 36 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Final stage: ShuffleMapStage 77 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting ShuffleMapStage 77 (MapPartitionsRDD[153] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 28.0 KiB, free 355.8 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.22.0.4:35591 (size: 10.2 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 355.8 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on c37eddbb00a9:40107 (size: 13.5 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 77 (MapPartitionsRDD[153] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 46) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4853 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 339.4 KiB, free 355.4 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 355.4 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 59 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 172.22.0.3:45645 (size: 13.5 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Registering RDD 157 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 22\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Got map stage job 37 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Final stage: ShuffleMapStage 78 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting ShuffleMapStage 78 (MapPartitionsRDD[157] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 25.0 KiB, free 355.4 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 355.4 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on c37eddbb00a9:40107 (size: 11.2 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 78 (MapPartitionsRDD[157] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 339.4 KiB, free 355.0 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 355.0 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 61 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13901819 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Registering RDD 161 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 23\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Got map stage job 38 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Final stage: ShuffleMapStage 79 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting ShuffleMapStage 79 (MapPartitionsRDD[161] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 23.3 KiB, free 355.0 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 355.0 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on c37eddbb00a9:40107 (size: 10.4 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 79 (MapPartitionsRDD[161] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Adding task set 79.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 339.4 KiB, free 354.6 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 354.6 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 63 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13901819 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Registering RDD 165 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 24\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Got map stage job 39 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Final stage: ShuffleMapStage 80 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting ShuffleMapStage 80 (MapPartitionsRDD[165] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 23.3 KiB, free 354.6 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 354.6 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on c37eddbb00a9:40107 (size: 10.4 KiB, free: 365.1 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 80 (MapPartitionsRDD[165] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Adding task set 80.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 47) (172.22.0.3, executor 1, partition 0, PROCESS_LOCAL, 4853 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 46) in 81 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:20 INFO DAGScheduler: ShuffleMapStage 77 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.085 s\n",
      "24/10/04 09:30:20 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:20 INFO DAGScheduler: running: Set(ShuffleMapStage 78, ShuffleMapStage 79, ShuffleMapStage 76, ShuffleMapStage 80)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.22.0.3:45645 (size: 11.2 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 48) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:20 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 45) in 118 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:20 INFO DAGScheduler: ShuffleMapStage 76 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.123 s\n",
      "24/10/04 09:30:20 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:20 INFO DAGScheduler: running: Set(ShuffleMapStage 78, ShuffleMapStage 79, ShuffleMapStage 80)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.22.0.4:35591 (size: 10.4 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:20 INFO ShufflePartitionsUtil: For shuffle(21), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:20 INFO ShufflePartitionsUtil: For shuffle(21), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:20 INFO ShufflePartitionsUtil: For shuffle(21), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:20 INFO ShufflePartitionsUtil: For shuffle(21), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 365.9 MiB)\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:20 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:20 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:20 INFO SparkContext: Starting job: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Got job 40 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Final stage: ResultStage 82 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 81)\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[168] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 31.7 KiB, free 354.5 MiB)\n",
      "24/10/04 09:30:20 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 354.5 MiB)\n",
      "24/10/04 09:30:20 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on c37eddbb00a9:40107 (size: 15.3 KiB, free: 365.1 MiB)\n",
      "24/10/04 09:30:20 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[168] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:20 INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Starting task 1.0 in stage 79.0 (TID 49) (172.22.0.3, executor 1, partition 1, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 47) in 154 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:21 INFO DAGScheduler: ShuffleMapStage 78 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.223 s\n",
      "24/10/04 09:30:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:21 INFO DAGScheduler: running: Set(ResultStage 82, ShuffleMapStage 79, ShuffleMapStage 80)\n",
      "24/10/04 09:30:21 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.22.0.3:45645 (size: 10.4 KiB, free: 365.4 MiB)\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 50) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 48) in 410 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.22.0.4:35591 (size: 10.4 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Starting task 1.0 in stage 80.0 (TID 51) (172.22.0.3, executor 1, partition 1, PROCESS_LOCAL, 4860 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Finished task 1.0 in stage 79.0 (TID 49) in 330 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:21 INFO DAGScheduler: ShuffleMapStage 79 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.535 s\n",
      "24/10/04 09:30:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:21 INFO DAGScheduler: running: Set(ResultStage 82, ShuffleMapStage 80)\n",
      "24/10/04 09:30:21 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.22.0.3:45645 (size: 10.4 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.22.0.3:45645 (size: 32.7 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 52) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Finished task 1.0 in stage 80.0 (TID 51) in 319 ms on 172.22.0.3 (executor 1) (1/2)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.22.0.3:45645 (size: 15.3 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 172.22.0.3:60010\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 52) in 21 ms on 172.22.0.3 (executor 1) (1/1)\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:21 INFO DAGScheduler: ResultStage 82 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.722 s\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Job 40 finished: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76, took 0.726015 s\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 1536.0 KiB, free 353.0 MiB)\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 141.9 KiB, free 352.9 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on c37eddbb00a9:40107 (size: 141.9 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:21 INFO SparkContext: Created broadcast 66 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/04 09:30:21 INFO FileSourceStrategy: Output Data Schema: struct<CustomerID: string, loan_amnt: int, funded_amnt: int, term: string, int_rate: string ... 14 more fields>\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 50) in 414 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:21 INFO DAGScheduler: ShuffleMapStage 80 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.886 s\n",
      "24/10/04 09:30:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:21 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 339.4 KiB, free 352.5 MiB)\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 352.5 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on c37eddbb00a9:40107 (size: 32.7 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:21 INFO SparkContext: Created broadcast 67 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Registering RDD 172 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 25\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Got map stage job 41 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Final stage: ShuffleMapStage 83 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Submitting ShuffleMapStage 83 (MapPartitionsRDD[172] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 62.7 KiB, free 352.5 MiB)\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 352.4 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on c37eddbb00a9:40107 (size: 25.4 KiB, free: 364.9 MiB)\n",
      "24/10/04 09:30:21 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 83 (MapPartitionsRDD[172] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 53) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 4855 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_64_piece0 on c37eddbb00a9:40107 in memory (size: 10.4 KiB, free: 364.9 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.22.0.4:35591 in memory (size: 10.4 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.22.0.3:45645 in memory (size: 10.4 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.22.0.4:35591 in memory (size: 10.4 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_62_piece0 on c37eddbb00a9:40107 in memory (size: 10.4 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.22.0.3:45645 in memory (size: 10.4 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.22.0.4:35591 (size: 25.4 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_56_piece0 on c37eddbb00a9:40107 in memory (size: 10.2 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.22.0.4:35591 in memory (size: 10.2 KiB, free: 365.8 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_60_piece0 on c37eddbb00a9:40107 in memory (size: 11.2 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.22.0.3:45645 in memory (size: 11.2 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_58_piece0 on c37eddbb00a9:40107 in memory (size: 13.5 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 172.22.0.3:45645 in memory (size: 13.5 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_65_piece0 on c37eddbb00a9:40107 in memory (size: 15.3 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.22.0.3:45645 in memory (size: 15.3 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.22.0.4:35591 (size: 141.9 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 172.22.0.4:35591 (size: 32.7 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 53) in 132 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:21 INFO DAGScheduler: ShuffleMapStage 83 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.142 s\n",
      "24/10/04 09:30:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:21 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:21 INFO ShufflePartitionsUtil: For shuffle(25), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:21 INFO ShufflePartitionsUtil: For shuffle(25), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:21 INFO ShufflePartitionsUtil: For shuffle(25), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:21 INFO ShufflePartitionsUtil: For shuffle(25), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:21 INFO CodeGenerator: Code generated in 7.153166 ms\n",
      "24/10/04 09:30:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Registering RDD 175 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 26\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Got map stage job 42 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Final stage: ShuffleMapStage 85 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 84)\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Submitting ShuffleMapStage 85 (MapPartitionsRDD[175] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 63.8 KiB, free 352.6 MiB)\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 24.6 KiB, free 352.6 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on c37eddbb00a9:40107 (size: 24.6 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:21 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 85 (MapPartitionsRDD[175] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 54) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.22.0.4:35591 (size: 24.6 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:21 INFO CodeGenerator: Code generated in 9.875041 ms\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Registering RDD 178 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 27\n",
      "24/10/04 09:30:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to 172.22.0.4:36490\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Got map stage job 43 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Final stage: ShuffleMapStage 86 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 84)\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Submitting ShuffleMapStage 86 (MapPartitionsRDD[178] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 62.1 KiB, free 352.5 MiB)\n",
      "24/10/04 09:30:21 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 24.7 KiB, free 352.5 MiB)\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on c37eddbb00a9:40107 (size: 24.7 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:21 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 86 (MapPartitionsRDD[178] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 55) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:21 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 54) in 66 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:21 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:21 INFO DAGScheduler: ShuffleMapStage 85 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.070 s\n",
      "24/10/04 09:30:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:21 INFO DAGScheduler: running: Set(ShuffleMapStage 86)\n",
      "24/10/04 09:30:21 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:21 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:21 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 172.22.0.4:35591 (size: 24.7 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:22 INFO ShufflePartitionsUtil: For shuffle(26), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 7.128583 ms\n",
      "24/10/04 09:30:22 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 55) in 46 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:22 INFO DAGScheduler: ShuffleMapStage 86 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.102 s\n",
      "24/10/04 09:30:22 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:22 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:22 INFO SparkContext: Starting job: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Got job 44 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Final stage: ResultStage 89 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[183] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 69.0 KiB, free 352.4 MiB)\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 352.4 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on c37eddbb00a9:40107 (size: 27.0 KiB, free: 364.9 MiB)\n",
      "24/10/04 09:30:22 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[183] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:22 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 56) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:22 INFO ShufflePartitionsUtil: For shuffle(27), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:22 INFO ShufflePartitionsUtil: For shuffle(27), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:22 INFO ShufflePartitionsUtil: For shuffle(27), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.22.0.4:35591 (size: 27.0 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to 172.22.0.4:36490\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 3.605583 ms\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 5.066917 ms\n",
      "24/10/04 09:30:22 INFO SparkContext: Starting job: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Got job 45 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Final stage: ResultStage 91 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 90)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting ResultStage 91 (MapPartitionsRDD[188] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 65.3 KiB, free 352.3 MiB)\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 26.6 KiB, free 352.3 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on c37eddbb00a9:40107 (size: 26.6 KiB, free: 364.9 MiB)\n",
      "24/10/04 09:30:22 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[188] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added taskresult_56 in memory on 172.22.0.4:35591 (size: 1200.5 KiB, free: 364.4 MiB)\n",
      "24/10/04 09:30:22 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 57) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.22.0.4:35591 (size: 26.6 KiB, free: 364.4 MiB)\n",
      "24/10/04 09:30:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to 172.22.0.4:36490\n",
      "24/10/04 09:30:22 INFO TransportClientFactory: Successfully created connection to /172.22.0.4:35591 after 1 ms (0 ms spent in bootstraps)\n",
      "24/10/04 09:30:22 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 56) in 117 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:22 INFO DAGScheduler: ResultStage 89 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.121 s\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Job 44 finished: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76, took 0.122413 s\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed taskresult_56 on 172.22.0.4:35591 in memory (size: 1200.5 KiB, free: 365.5 MiB)\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 4.5 MiB, free 347.8 MiB)\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 1319.0 KiB, free 346.5 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on c37eddbb00a9:40107 (size: 1319.0 KiB, free: 363.6 MiB)\n",
      "24/10/04 09:30:22 INFO SparkContext: Created broadcast 73 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:22 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 57) in 48 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:22 INFO DAGScheduler: ResultStage 91 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.095 s\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 91: Stage finished\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Job 45 finished: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76, took 0.097519 s\n",
      "24/10/04 09:30:22 INFO ShufflePartitionsUtil: For shuffle(20), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 1536.0 KiB, free 345.0 MiB)\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 150.8 KiB, free 344.9 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on c37eddbb00a9:40107 (size: 150.8 KiB, free: 363.5 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_68_piece0 on c37eddbb00a9:40107 in memory (size: 25.4 KiB, free: 363.5 MiB)\n",
      "24/10/04 09:30:22 INFO SparkContext: Created broadcast 74 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.22.0.4:35591 in memory (size: 25.4 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_70_piece0 on c37eddbb00a9:40107 in memory (size: 24.7 KiB, free: 363.5 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 172.22.0.4:35591 in memory (size: 24.7 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_69_piece0 on c37eddbb00a9:40107 in memory (size: 24.6 KiB, free: 363.5 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.22.0.4:35591 in memory (size: 24.6 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_71_piece0 on c37eddbb00a9:40107 in memory (size: 27.0 KiB, free: 363.6 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.22.0.4:35591 in memory (size: 27.0 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_72_piece0 on c37eddbb00a9:40107 in memory (size: 26.6 KiB, free: 363.6 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.22.0.4:35591 in memory (size: 26.6 KiB, free: 365.7 MiB)\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 6.736833 ms\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Registering RDD 192 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 28\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Got map stage job 46 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Final stage: ShuffleMapStage 93 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 92)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting ShuffleMapStage 93 (MapPartitionsRDD[192] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 85.2 KiB, free 345.2 MiB)\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 345.2 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on c37eddbb00a9:40107 (size: 33.7 KiB, free: 363.6 MiB)\n",
      "24/10/04 09:30:22 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 93 (MapPartitionsRDD[192] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:22 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 58) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4465 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.22.0.4:35591 (size: 33.7 KiB, free: 365.6 MiB)\n",
      "24/10/04 09:30:22 INFO ShufflePartitionsUtil: For shuffle(22, 23), advisory target size: 67108864, actual target size 8583530, minimum partition size: 1048576\n",
      "24/10/04 09:30:22 INFO ShufflePartitionsUtil: For shuffle(22, 24), advisory target size: 67108864, actual target size 8505649, minimum partition size: 1048576\n",
      "24/10/04 09:30:22 INFO ShufflePartitionsUtil: For shuffle(22, 24), advisory target size: 67108864, actual target size 8505649, minimum partition size: 1048576\n",
      "24/10/04 09:30:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 172.22.0.4:36490\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 172.22.0.4:35591 (size: 1319.0 KiB, free: 364.3 MiB)\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 5.124792 ms\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 31.612875 ms\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 58) in 111 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:22 INFO DAGScheduler: ShuffleMapStage 93 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.142 s\n",
      "24/10/04 09:30:22 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:22 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Registering RDD 201 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 29\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Got map stage job 47 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Final stage: ShuffleMapStage 96 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting ShuffleMapStage 96 (MapPartitionsRDD[201] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 166.4 KiB, free 345.0 MiB)\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 63.5 KiB, free 345.0 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on c37eddbb00a9:40107 (size: 63.5 KiB, free: 363.5 MiB)\n",
      "24/10/04 09:30:22 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 96 (MapPartitionsRDD[201] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Adding task set 96.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:22 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 59) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 9.436958 ms\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 172.22.0.3:45645 (size: 63.5 KiB, free: 365.3 MiB)\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 12.66875 ms\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 16.778625 ms\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 3.876667 ms\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Registering RDD 210 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 30\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Got map stage job 48 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Final stage: ShuffleMapStage 98 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 97, ShuffleMapStage 94)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting ShuffleMapStage 98 (MapPartitionsRDD[210] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 167.1 KiB, free 344.8 MiB)\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 63.8 KiB, free 344.7 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on c37eddbb00a9:40107 (size: 63.8 KiB, free: 363.4 MiB)\n",
      "24/10/04 09:30:22 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 98 (MapPartitionsRDD[210] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Adding task set 98.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 16.53375 ms\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 28.645875 ms\n",
      "24/10/04 09:30:22 INFO CodeGenerator: Code generated in 10.639417 ms\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Registering RDD 217 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 31\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Got map stage job 49 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Final stage: ShuffleMapStage 99 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 97, ShuffleMapStage 94)\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting ShuffleMapStage 99 (MapPartitionsRDD[217] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 145.9 KiB, free 344.6 MiB)\n",
      "24/10/04 09:30:22 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 56.2 KiB, free 344.5 MiB)\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on c37eddbb00a9:40107 (size: 56.2 KiB, free: 363.4 MiB)\n",
      "24/10/04 09:30:22 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:22 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 99 (MapPartitionsRDD[217] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:22 INFO TaskSchedulerImpl: Adding task set 99.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 172.22.0.3:60010\n",
      "24/10/04 09:30:22 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 172.22.0.3:45645 (size: 150.8 KiB, free: 365.1 MiB)\n",
      "24/10/04 09:30:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 172.22.0.3:60010\n",
      "24/10/04 09:30:23 INFO TaskSetManager: Starting task 1.0 in stage 96.0 (TID 60) (172.22.0.3, executor 1, partition 1, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:23 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 59) in 757 ms on 172.22.0.3 (executor 1) (1/2)\n",
      "24/10/04 09:30:23 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 61) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:23 INFO TaskSetManager: Finished task 1.0 in stage 96.0 (TID 60) in 221 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:23 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:23 INFO DAGScheduler: ShuffleMapStage 96 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.983 s\n",
      "24/10/04 09:30:23 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:23 INFO DAGScheduler: running: Set(ShuffleMapStage 99, ShuffleMapStage 98)\n",
      "24/10/04 09:30:23 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:23 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:23 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 172.22.0.3:45645 (size: 63.8 KiB, free: 365.1 MiB)\n",
      "24/10/04 09:30:23 INFO ShufflePartitionsUtil: For shuffle(29), advisory target size: 67108864, actual target size 2666819, minimum partition size: 1048576\n",
      "24/10/04 09:30:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to 172.22.0.3:60010\n",
      "24/10/04 09:30:23 INFO BlockManagerInfo: Removed broadcast_75_piece0 on c37eddbb00a9:40107 in memory (size: 33.7 KiB, free: 363.4 MiB)\n",
      "24/10/04 09:30:23 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.22.0.4:35591 in memory (size: 33.7 KiB, free: 364.4 MiB)\n",
      "24/10/04 09:30:23 INFO BlockManagerInfo: Removed broadcast_76_piece0 on c37eddbb00a9:40107 in memory (size: 63.5 KiB, free: 363.5 MiB)\n",
      "24/10/04 09:30:23 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 172.22.0.3:45645 in memory (size: 63.5 KiB, free: 365.1 MiB)\n",
      "24/10/04 09:30:23 INFO CodeGenerator: Code generated in 6.079584 ms\n",
      "24/10/04 09:30:23 INFO DAGScheduler: Registering RDD 221 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 32\n",
      "24/10/04 09:30:23 INFO DAGScheduler: Got map stage job 50 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:23 INFO DAGScheduler: Final stage: ShuffleMapStage 102 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 101)\n",
      "24/10/04 09:30:23 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:23 INFO DAGScheduler: Submitting ShuffleMapStage 102 (MapPartitionsRDD[221] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:23 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 141.2 KiB, free 344.7 MiB)\n",
      "24/10/04 09:30:23 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 52.0 KiB, free 344.7 MiB)\n",
      "24/10/04 09:30:23 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on c37eddbb00a9:40107 (size: 52.0 KiB, free: 363.4 MiB)\n",
      "24/10/04 09:30:23 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:23 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 102 (MapPartitionsRDD[221] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:23 INFO TaskSchedulerImpl: Adding task set 102.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:24 INFO TaskSetManager: Starting task 1.0 in stage 98.0 (TID 62) (172.22.0.3, executor 1, partition 1, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:24 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 61) in 747 ms on 172.22.0.3 (executor 1) (1/2)\n",
      "24/10/04 09:30:24 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 63) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:24 INFO TaskSetManager: Finished task 1.0 in stage 98.0 (TID 62) in 546 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:24 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:24 INFO DAGScheduler: ShuffleMapStage 98 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 2.203 s\n",
      "24/10/04 09:30:24 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:24 INFO DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 99)\n",
      "24/10/04 09:30:24 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:24 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:24 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.22.0.3:45645 (size: 56.2 KiB, free: 365.1 MiB)\n",
      "24/10/04 09:30:24 INFO ShufflePartitionsUtil: For shuffle(30), advisory target size: 67108864, actual target size 14266870, minimum partition size: 1048576\n",
      "24/10/04 09:30:24 INFO CodeGenerator: Code generated in 3.494416 ms\n",
      "24/10/04 09:30:24 INFO DAGScheduler: Registering RDD 225 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 33\n",
      "24/10/04 09:30:24 INFO DAGScheduler: Got map stage job 51 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:24 INFO DAGScheduler: Final stage: ShuffleMapStage 104 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 103)\n",
      "24/10/04 09:30:24 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:24 INFO DAGScheduler: Submitting ShuffleMapStage 104 (MapPartitionsRDD[225] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:24 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 141.7 KiB, free 344.5 MiB)\n",
      "24/10/04 09:30:24 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 52.2 KiB, free 344.5 MiB)\n",
      "24/10/04 09:30:24 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on c37eddbb00a9:40107 (size: 52.2 KiB, free: 363.4 MiB)\n",
      "24/10/04 09:30:24 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:24 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 104 (MapPartitionsRDD[225] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:24 INFO TaskSchedulerImpl: Adding task set 104.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:25 INFO TaskSetManager: Starting task 1.0 in stage 99.0 (TID 64) (172.22.0.3, executor 1, partition 1, NODE_LOCAL, 4728 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:25 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 63) in 407 ms on 172.22.0.3 (executor 1) (1/2)\n",
      "24/10/04 09:30:25 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 65) (172.22.0.3, executor 1, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:25 INFO TaskSetManager: Finished task 1.0 in stage 99.0 (TID 64) in 301 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:25 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:25 INFO DAGScheduler: ShuffleMapStage 99 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 2.808 s\n",
      "24/10/04 09:30:25 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:25 INFO DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 104)\n",
      "24/10/04 09:30:25 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:25 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:25 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 172.22.0.3:45645 (size: 52.0 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:25 INFO ShufflePartitionsUtil: For shuffle(31), advisory target size: 67108864, actual target size 13773885, minimum partition size: 1048576\n",
      "24/10/04 09:30:25 INFO CodeGenerator: Code generated in 2.961333 ms\n",
      "24/10/04 09:30:25 INFO DAGScheduler: Registering RDD 228 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 34\n",
      "24/10/04 09:30:25 INFO DAGScheduler: Got map stage job 52 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:25 INFO DAGScheduler: Final stage: ShuffleMapStage 106 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 105)\n",
      "24/10/04 09:30:25 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:25 INFO DAGScheduler: Submitting ShuffleMapStage 106 (MapPartitionsRDD[228] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:25 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 15.6 KiB, free 344.5 MiB)\n",
      "24/10/04 09:30:25 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 344.5 MiB)\n",
      "24/10/04 09:30:25 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on c37eddbb00a9:40107 (size: 7.0 KiB, free: 363.4 MiB)\n",
      "24/10/04 09:30:25 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:25 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 106 (MapPartitionsRDD[228] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:25 INFO TaskSchedulerImpl: Adding task set 106.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to 172.22.0.3:60010\n",
      "24/10/04 09:30:27 INFO TaskSetManager: Starting task 1.0 in stage 102.0 (TID 66) (172.22.0.3, executor 1, partition 1, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:27 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 65) in 1879 ms on 172.22.0.3 (executor 1) (1/2)\n",
      "24/10/04 09:30:28 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 67) (172.22.0.4, executor 0, partition 0, ANY, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:28 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.22.0.4:35591 (size: 52.2 KiB, free: 364.3 MiB)\n",
      "24/10/04 09:30:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 172.22.0.4:36490\n",
      "24/10/04 09:30:28 INFO TaskSetManager: Starting task 1.0 in stage 104.0 (TID 68) (172.22.0.3, executor 1, partition 1, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:28 INFO TaskSetManager: Finished task 1.0 in stage 102.0 (TID 66) in 1478 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:28 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:28 INFO DAGScheduler: ShuffleMapStage 102 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 5.312 s\n",
      "24/10/04 09:30:28 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:28 INFO DAGScheduler: running: Set(ShuffleMapStage 104, ShuffleMapStage 106)\n",
      "24/10/04 09:30:28 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:28 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:28 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.22.0.3:45645 (size: 52.2 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 172.22.0.3:60010\n",
      "24/10/04 09:30:31 INFO TaskSetManager: Starting task 0.0 in stage 106.0 (TID 69) (172.22.0.4, executor 0, partition 0, ANY, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:31 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 67) in 3054 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:31 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.22.0.4:35591 (size: 7.0 KiB, free: 364.3 MiB)\n",
      "24/10/04 09:30:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 172.22.0.4:36490\n",
      "24/10/04 09:30:31 INFO TaskSetManager: Starting task 1.0 in stage 106.0 (TID 70) (172.22.0.3, executor 1, partition 1, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:31 INFO TaskSetManager: Finished task 1.0 in stage 104.0 (TID 68) in 2876 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:31 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:31 INFO DAGScheduler: ShuffleMapStage 104 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 6.906 s\n",
      "24/10/04 09:30:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:31 INFO DAGScheduler: running: Set(ShuffleMapStage 106)\n",
      "24/10/04 09:30:31 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:31 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:31 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.22.0.3:45645 (size: 7.0 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 172.22.0.3:60010\n",
      "24/10/04 09:30:31 INFO TaskSetManager: Finished task 0.0 in stage 106.0 (TID 69) in 180 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:31 INFO TaskSetManager: Finished task 1.0 in stage 106.0 (TID 70) in 218 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:31 INFO TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:31 INFO DAGScheduler: ShuffleMapStage 106 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 6.420 s\n",
      "24/10/04 09:30:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:31 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:31 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:31 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:31 INFO ShufflePartitionsUtil: For shuffle(34), advisory target size: 67108864, actual target size 13876701, minimum partition size: 1048576\n",
      "24/10/04 09:30:31 INFO CodeGenerator: Code generated in 2.628458 ms\n",
      "24/10/04 09:30:31 INFO CodeGenerator: Code generated in 3.0405 ms\n",
      "24/10/04 09:30:31 INFO DAGScheduler: Registering RDD 233 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 35\n",
      "24/10/04 09:30:31 INFO DAGScheduler: Got map stage job 53 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:31 INFO DAGScheduler: Final stage: ShuffleMapStage 111 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 110)\n",
      "24/10/04 09:30:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:31 INFO DAGScheduler: Submitting ShuffleMapStage 111 (MapPartitionsRDD[233] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:31 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 115.3 KiB, free 344.4 MiB)\n",
      "24/10/04 09:30:31 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 43.7 KiB, free 344.3 MiB)\n",
      "24/10/04 09:30:31 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on c37eddbb00a9:40107 (size: 43.7 KiB, free: 363.3 MiB)\n",
      "24/10/04 09:30:31 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 111 (MapPartitionsRDD[233] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:31 INFO TaskSchedulerImpl: Adding task set 111.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:31 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 71) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:31 INFO TaskSetManager: Starting task 1.0 in stage 111.0 (TID 72) (172.22.0.3, executor 1, partition 1, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:31 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 172.22.0.4:35591 (size: 43.7 KiB, free: 364.3 MiB)\n",
      "24/10/04 09:30:31 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 172.22.0.3:45645 (size: 43.7 KiB, free: 364.9 MiB)\n",
      "24/10/04 09:30:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 172.22.0.3:60010\n",
      "24/10/04 09:30:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 172.22.0.4:36490\n",
      "24/10/04 09:30:32 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 71) in 258 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:32 INFO TaskSetManager: Finished task 1.0 in stage 111.0 (TID 72) in 290 ms on 172.22.0.3 (executor 1) (2/2)\n",
      "24/10/04 09:30:32 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:32 INFO DAGScheduler: ShuffleMapStage 111 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.294 s\n",
      "24/10/04 09:30:32 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:32 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:32 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:32 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:32 INFO ShufflePartitionsUtil: For shuffle(35), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/10/04 09:30:32 INFO SparkContext: Starting job: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Got job 54 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 1 output partitions\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Final stage: ResultStage 117 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 116)\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[235] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:32 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 7.2 KiB, free 344.3 MiB)\n",
      "24/10/04 09:30:32 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 344.3 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on c37eddbb00a9:40107 (size: 3.8 KiB, free: 363.3 MiB)\n",
      "24/10/04 09:30:32 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[235] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/04 09:30:32 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks resource profile 0\n",
      "24/10/04 09:30:32 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 73) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4473 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 172.22.0.4:35591 (size: 3.8 KiB, free: 364.3 MiB)\n",
      "24/10/04 09:30:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to 172.22.0.4:36490\n",
      "24/10/04 09:30:32 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 73) in 16 ms on 172.22.0.4 (executor 0) (1/1)\n",
      "24/10/04 09:30:32 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:32 INFO DAGScheduler: ResultStage 117 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.018 s\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Job 54 finished: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76, took 0.021755 s\n",
      "24/10/04 09:30:32 INFO CodeGenerator: Code generated in 3.804584 ms\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_78_piece0 on c37eddbb00a9:40107 in memory (size: 56.2 KiB, free: 363.4 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.22.0.3:45645 in memory (size: 56.2 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_79_piece0 on c37eddbb00a9:40107 in memory (size: 52.0 KiB, free: 363.4 MiB)\n",
      "24/10/04 09:30:32 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 2.5 MiB, free 342.2 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 172.22.0.3:45645 in memory (size: 52.0 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_83_piece0 on c37eddbb00a9:40107 in memory (size: 3.8 KiB, free: 363.4 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 172.22.0.4:35591 in memory (size: 3.8 KiB, free: 364.3 MiB)\n",
      "24/10/04 09:30:32 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 623.1 KiB, free 341.6 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on c37eddbb00a9:40107 (size: 623.1 KiB, free: 362.8 MiB)\n",
      "24/10/04 09:30:32 INFO SparkContext: Created broadcast 84 from toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_81_piece0 on c37eddbb00a9:40107 in memory (size: 7.0 KiB, free: 362.8 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.22.0.3:45645 in memory (size: 7.0 KiB, free: 365.0 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.22.0.4:35591 in memory (size: 7.0 KiB, free: 364.3 MiB)\n",
      "24/10/04 09:30:32 INFO ShufflePartitionsUtil: For shuffle(28, 32, 33), advisory target size: 67108864, actual target size 11066914, minimum partition size: 1048576\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_80_piece0 on c37eddbb00a9:40107 in memory (size: 52.2 KiB, free: 362.9 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.22.0.3:45645 in memory (size: 52.2 KiB, free: 365.1 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.22.0.4:35591 in memory (size: 52.2 KiB, free: 364.3 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_82_piece0 on c37eddbb00a9:40107 in memory (size: 43.7 KiB, free: 362.9 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 172.22.0.4:35591 in memory (size: 43.7 KiB, free: 364.4 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 172.22.0.3:45645 in memory (size: 43.7 KiB, free: 365.1 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_77_piece0 on c37eddbb00a9:40107 in memory (size: 63.8 KiB, free: 363.0 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 172.22.0.3:45645 in memory (size: 63.8 KiB, free: 365.2 MiB)\n",
      "24/10/04 09:30:32 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage53.processNext is 9294 bytes\n",
      "24/10/04 09:30:32 INFO CodeGenerator: Code generated in 12.921709 ms\n",
      "24/10/04 09:30:32 INFO CodeGenerator: Code generated in 9.153875 ms\n",
      "24/10/04 09:30:32 INFO CodeGenerator: Code generated in 5.9535 ms\n",
      "24/10/04 09:30:32 INFO CodeGenerator: Code generated in 3.384166 ms\n",
      "24/10/04 09:30:32 INFO CodeGenerator: Code generated in 8.203625 ms\n",
      "24/10/04 09:30:32 INFO CodeGenerator: Code generated in 4.951333 ms\n",
      "24/10/04 09:30:32 INFO SparkContext: Starting job: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Got job 55 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Final stage: ResultStage 127 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 121, ShuffleMapStage 126, ShuffleMapStage 124)\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Submitting ResultStage 127 (MapPartitionsRDD[251] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:32 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 462.4 KiB, free 341.7 MiB)\n",
      "24/10/04 09:30:32 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 142.8 KiB, free 341.6 MiB)\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on c37eddbb00a9:40107 (size: 142.8 KiB, free: 362.8 MiB)\n",
      "24/10/04 09:30:32 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 127 (MapPartitionsRDD[251] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:32 INFO TaskSchedulerImpl: Adding task set 127.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:32 INFO TaskSetManager: Starting task 0.0 in stage 127.0 (TID 74) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4790 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on 172.22.0.4:35591 (size: 142.8 KiB, free: 364.2 MiB)\n",
      "24/10/04 09:30:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to 172.22.0.4:36490\n",
      "24/10/04 09:30:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to 172.22.0.4:36490\n",
      "24/10/04 09:30:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to 172.22.0.4:36490\n",
      "24/10/04 09:30:32 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.22.0.4:35591 (size: 623.1 KiB, free: 363.6 MiB)\n",
      "24/10/04 09:30:33 INFO TaskSetManager: Starting task 1.0 in stage 127.0 (TID 75) (172.22.0.4, executor 0, partition 1, NODE_LOCAL, 4790 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:33 INFO TaskSetManager: Finished task 0.0 in stage 127.0 (TID 74) in 1406 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:34 INFO TaskSetManager: Finished task 1.0 in stage 127.0 (TID 75) in 883 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:34 INFO TaskSchedulerImpl: Removed TaskSet 127.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:34 INFO DAGScheduler: ResultStage 127 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 2.292 s\n",
      "24/10/04 09:30:34 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 127: Stage finished\n",
      "24/10/04 09:30:34 INFO DAGScheduler: Job 55 finished: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76, took 2.294186 s\n",
      "24/10/04 09:30:34 INFO DAGScheduler: Registering RDD 252 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) as input to shuffle 36\n",
      "24/10/04 09:30:34 INFO DAGScheduler: Got map stage job 56 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:34 INFO DAGScheduler: Final stage: ShuffleMapStage 137 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 135, ShuffleMapStage 136, ShuffleMapStage 134)\n",
      "24/10/04 09:30:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:34 INFO DAGScheduler: Submitting ShuffleMapStage 137 (MapPartitionsRDD[252] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:34 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 468.4 KiB, free 341.1 MiB)\n",
      "24/10/04 09:30:34 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 143.6 KiB, free 341.0 MiB)\n",
      "24/10/04 09:30:34 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on c37eddbb00a9:40107 (size: 143.6 KiB, free: 362.7 MiB)\n",
      "24/10/04 09:30:34 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:34 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 137 (MapPartitionsRDD[252] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:34 INFO TaskSchedulerImpl: Adding task set 137.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:34 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 76) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4779 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:34 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 172.22.0.4:35591 (size: 143.6 KiB, free: 363.5 MiB)\n",
      "24/10/04 09:30:35 INFO TaskSetManager: Starting task 1.0 in stage 137.0 (TID 77) (172.22.0.4, executor 0, partition 1, NODE_LOCAL, 4779 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:35 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 76) in 894 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:36 INFO TaskSetManager: Finished task 1.0 in stage 137.0 (TID 77) in 799 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:36 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:36 INFO DAGScheduler: ShuffleMapStage 137 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 1.701 s\n",
      "24/10/04 09:30:36 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/04 09:30:36 INFO DAGScheduler: running: Set()\n",
      "24/10/04 09:30:36 INFO DAGScheduler: waiting: Set()\n",
      "24/10/04 09:30:36 INFO DAGScheduler: failed: Set()\n",
      "24/10/04 09:30:36 INFO ShufflePartitionsUtil: For shuffle(36), advisory target size: 67108864, actual target size 4257071, minimum partition size: 1048576\n",
      "24/10/04 09:30:36 INFO CodeGenerator: Code generated in 8.978875 ms\n",
      "24/10/04 09:30:36 INFO SparkContext: Starting job: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76\n",
      "24/10/04 09:30:36 INFO DAGScheduler: Got job 57 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) with 2 output partitions\n",
      "24/10/04 09:30:36 INFO DAGScheduler: Final stage: ResultStage 148 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76)\n",
      "24/10/04 09:30:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 147)\n",
      "24/10/04 09:30:36 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/04 09:30:36 INFO DAGScheduler: Submitting ResultStage 148 (MapPartitionsRDD[255] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76), which has no missing parents\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Removed broadcast_85_piece0 on c37eddbb00a9:40107 in memory (size: 142.8 KiB, free: 362.8 MiB)\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 172.22.0.4:35591 in memory (size: 142.8 KiB, free: 363.6 MiB)\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Removed broadcast_86_piece0 on c37eddbb00a9:40107 in memory (size: 143.6 KiB, free: 363.0 MiB)\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 172.22.0.4:35591 in memory (size: 143.6 KiB, free: 363.8 MiB)\n",
      "24/10/04 09:30:36 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 410.1 KiB, free 341.8 MiB)\n",
      "24/10/04 09:30:36 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 120.2 KiB, free 341.7 MiB)\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on c37eddbb00a9:40107 (size: 120.2 KiB, free: 362.9 MiB)\n",
      "24/10/04 09:30:36 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1478\n",
      "24/10/04 09:30:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 148 (MapPartitionsRDD[255] at toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/10/04 09:30:36 INFO TaskSchedulerImpl: Adding task set 148.0 with 2 tasks resource profile 0\n",
      "24/10/04 09:30:36 INFO TaskSetManager: Starting task 0.0 in stage 148.0 (TID 78) (172.22.0.4, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.22.0.4:35591 (size: 120.2 KiB, free: 363.7 MiB)\n",
      "24/10/04 09:30:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 172.22.0.4:36490\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Added taskresult_78 in memory on 172.22.0.4:35591 (size: 3.8 MiB, free: 359.8 MiB)\n",
      "24/10/04 09:30:36 INFO TaskSetManager: Starting task 1.0 in stage 148.0 (TID 79) (172.22.0.4, executor 0, partition 1, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "24/10/04 09:30:36 INFO TaskSetManager: Finished task 0.0 in stage 148.0 (TID 78) in 125 ms on 172.22.0.4 (executor 0) (1/2)\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Removed taskresult_78 on 172.22.0.4:35591 in memory (size: 3.8 MiB, free: 363.7 MiB)\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Added taskresult_79 in memory on 172.22.0.4:35591 (size: 3.9 MiB, free: 359.8 MiB)\n",
      "24/10/04 09:30:36 INFO TaskSetManager: Finished task 1.0 in stage 148.0 (TID 79) in 79 ms on 172.22.0.4 (executor 0) (2/2)\n",
      "24/10/04 09:30:36 INFO TaskSchedulerImpl: Removed TaskSet 148.0, whose tasks have all completed, from pool \n",
      "24/10/04 09:30:36 INFO DAGScheduler: ResultStage 148 (toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76) finished in 0.180 s\n",
      "24/10/04 09:30:36 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/04 09:30:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 148: Stage finished\n",
      "24/10/04 09:30:36 INFO DAGScheduler: Job 57 finished: toPandas at /opt/spark-apps/DataRobotRunSSSQL.py:76, took 0.188581 s\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Removed taskresult_79 on 172.22.0.4:35591 in memory (size: 3.9 MiB, free: 363.7 MiB)\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Removed broadcast_87_piece0 on c37eddbb00a9:40107 in memory (size: 120.2 KiB, free: 363.0 MiB)\n",
      "24/10/04 09:30:36 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.22.0.4:35591 in memory (size: 120.2 KiB, free: 363.8 MiB)\n",
      "\n",
      "Saved result to CSV file: /opt/spark/spark-warehouse/result.csv\n",
      "24/10/04 09:30:37 INFO SparkUI: Stopped Spark web UI at http://c37eddbb00a9:4040\n",
      "24/10/04 09:30:37 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/10/04 09:30:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "24/10/04 09:30:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/10/04 09:30:37 INFO MemoryStore: MemoryStore cleared\n",
      "24/10/04 09:30:37 INFO BlockManager: BlockManager stopped\n",
      "24/10/04 09:30:37 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/10/04 09:30:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/10/04 09:30:37 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/10/04 09:30:37 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/10/04 09:30:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-e765b420-fec3-4cd9-9dcf-4179095c347c\n",
      "24/10/04 09:30:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-231ee436-6c35-4a24-b081-bed68cc20f68\n",
      "24/10/04 09:30:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-231ee436-6c35-4a24-b081-bed68cc20f68/pyspark-829c9d45-bc4c-46f0-8a37-e77748283bf0\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it spark-master /opt/spark/bin/spark-submit \\\n",
    "  --conf \"spark.sql.legacy.timeParserPolicy=LEGACY\" \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark-libs/spark-udf-assembly-0.1.0.jar \\\n",
    "  -c spark.sql.caseSensitive=true \\\n",
    "  /opt/spark-apps/DataRobotRunSSSQL.py \\\n",
    "    /opt/spark-apps/LC_FD_SQL.sql \\\n",
    "    --input=csv,primary_dataset=/opt/spark-apps/LC_train.csv \\\n",
    "    --input=csv,LC_profile=/opt/spark-apps/LC_profile.csv \\\n",
    "    --input=csv,LC_transactions=/opt/spark-apps/LC_transactions.csv \\\n",
    "    --output=csvfile,/opt/spark/spark-warehouse/result.csv  # output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e5ee4-96a1-4f27-811a-27742804f2c4",
   "metadata": {},
   "source": [
    "### 5. Check the result\n",
    "Once the job has been executed, you can view the result directly from the notebook. For example, to view the first few lines of the result CSV, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597c2a5b-5b76-4fb9-b534-a6e96064c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74df0b9f-09ac-416a-b2b2-81ee98e5f0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LC_transactions (days since previous event by CustomerID) (30 days max)</th>\n",
       "      <th>LC_transactions[Date] (Day of Week) (1 week most frequent)</th>\n",
       "      <th>LC_transactions[Date] (Day of Week) (30 days latest)</th>\n",
       "      <th>LC_transactions (days since previous event by CustomerID) (30 days avg)</th>\n",
       "      <th>BadLoan</th>\n",
       "      <th>LC_transactions[Amount] (30 days std)</th>\n",
       "      <th>LC_transactions[Amount] (1 week min)</th>\n",
       "      <th>LC_transactions[Description] (30 days unique count)</th>\n",
       "      <th>LC_transactions[Date] (Day of Month) (30 days latest)</th>\n",
       "      <th>LC_transactions[Amount] (30 days min)</th>\n",
       "      <th>...</th>\n",
       "      <th>LC_transactions[Date] (Day of Week) (30 days most frequent)</th>\n",
       "      <th>date (days from LC_transactions[Date]) (1 week std)</th>\n",
       "      <th>LC_transactions[Amount] (30 days median)</th>\n",
       "      <th>LC_transactions[Amount] (30 days max)</th>\n",
       "      <th>LC_transactions[AccountID] (1 week tokens)</th>\n",
       "      <th>LC_profile[emp_length]</th>\n",
       "      <th>LC_transactions[AccountID] (word count) (30 days sum)</th>\n",
       "      <th>LC_transactions[Description] (1 week counts)</th>\n",
       "      <th>LC_transactions[Date] (Day of Month) (1 week most frequent)</th>\n",
       "      <th>LC_profile[zip_code]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>No</td>\n",
       "      <td>23.653558</td>\n",
       "      <td>14.99</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>10.25</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.118034</td>\n",
       "      <td>10.25</td>\n",
       "      <td>8.42</td>\n",
       "      <td>{\"a355056969\" : 6.0}</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>15.0</td>\n",
       "      <td>{\"internet payment\" : 2.0, \"international tran...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>782xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>No</td>\n",
       "      <td>597.190780</td>\n",
       "      <td>1.49</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.542778</td>\n",
       "      <td>32.25</td>\n",
       "      <td>95.00</td>\n",
       "      <td>{\"a366458676\" : 12.0, \"a547274624\" : 4.0, \"a38...</td>\n",
       "      <td>3 years</td>\n",
       "      <td>88.0</td>\n",
       "      <td>{\"amortisation\" : 1.0, \"internet payment\" : 2....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>010xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.241071</td>\n",
       "      <td>No</td>\n",
       "      <td>439.613390</td>\n",
       "      <td>131.50</td>\n",
       "      <td>44.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.114924</td>\n",
       "      <td>61.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"a643390547\" : 2.0, \"a346560403\" : 10.0}</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>113.0</td>\n",
       "      <td>{\"internet payment\" : 1.0, \"telco\" : 1.0, \"sup...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>890xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>No</td>\n",
       "      <td>74.973167</td>\n",
       "      <td>205.29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"a458669971\" : 1.0}</td>\n",
       "      <td>9 years</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{\"internet payment\" : 1.0}</td>\n",
       "      <td>12.0</td>\n",
       "      <td>277xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>No</td>\n",
       "      <td>316.576980</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.699673</td>\n",
       "      <td>39.00</td>\n",
       "      <td>73.30</td>\n",
       "      <td>{\"a249156803\" : 4.0, \"a448123747\" : 1.0, \"a505...</td>\n",
       "      <td>5 years</td>\n",
       "      <td>41.0</td>\n",
       "      <td>{\"internet payment\" : 1.0, \"retail furniture\" ...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>939xx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LC_transactions (days since previous event by CustomerID) (30 days max)  \\\n",
       "0                                                5.0                         \n",
       "1                                                3.0                         \n",
       "2                                                4.0                         \n",
       "3                                               14.0                         \n",
       "4                                                3.0                         \n",
       "\n",
       "   LC_transactions[Date] (Day of Week) (1 week most frequent)  \\\n",
       "0                                                5.0            \n",
       "1                                                6.0            \n",
       "2                                                3.0            \n",
       "3                                                1.0            \n",
       "4                                                0.0            \n",
       "\n",
       "   LC_transactions[Date] (Day of Week) (30 days latest)  \\\n",
       "0                                                5.0      \n",
       "1                                                6.0      \n",
       "2                                                3.0      \n",
       "3                                                1.0      \n",
       "4                                                1.0      \n",
       "\n",
       "   LC_transactions (days since previous event by CustomerID) (30 days avg)  \\\n",
       "0                                           1.714286                         \n",
       "1                                           0.310345                         \n",
       "2                                           0.241071                         \n",
       "3                                           6.250000                         \n",
       "4                                           0.675000                         \n",
       "\n",
       "  BadLoan  LC_transactions[Amount] (30 days std)  \\\n",
       "0      No                              23.653558   \n",
       "1      No                             597.190780   \n",
       "2      No                             439.613390   \n",
       "3      No                              74.973167   \n",
       "4      No                             316.576980   \n",
       "\n",
       "   LC_transactions[Amount] (1 week min)  \\\n",
       "0                                 14.99   \n",
       "1                                  1.49   \n",
       "2                                131.50   \n",
       "3                                205.29   \n",
       "4                               1000.00   \n",
       "\n",
       "   LC_transactions[Description] (30 days unique count)  \\\n",
       "0                                               12.0     \n",
       "1                                               33.0     \n",
       "2                                               44.0     \n",
       "3                                                5.0     \n",
       "4                                               25.0     \n",
       "\n",
       "   LC_transactions[Date] (Day of Month) (30 days latest)  \\\n",
       "0                                               23.0       \n",
       "1                                                3.0       \n",
       "2                                               18.0       \n",
       "3                                               12.0       \n",
       "4                                               12.0       \n",
       "\n",
       "   LC_transactions[Amount] (30 days min)  ...  \\\n",
       "0                                  10.25  ...   \n",
       "1                                   0.83  ...   \n",
       "2                                   0.28  ...   \n",
       "3                                   2.50  ...   \n",
       "4                                   1.45  ...   \n",
       "\n",
       "   LC_transactions[Date] (Day of Week) (30 days most frequent)  \\\n",
       "0                                                5.0             \n",
       "1                                                5.0             \n",
       "2                                                2.0             \n",
       "3                                                4.0             \n",
       "4                                                0.0             \n",
       "\n",
       "  date (days from LC_transactions[Date]) (1 week std)  \\\n",
       "0                                           1.118034    \n",
       "1                                           1.542778    \n",
       "2                                           1.114924    \n",
       "3                                           0.000000    \n",
       "4                                           1.699673    \n",
       "\n",
       "   LC_transactions[Amount] (30 days median)  \\\n",
       "0                                     10.25   \n",
       "1                                     32.25   \n",
       "2                                     61.50   \n",
       "3                                     55.98   \n",
       "4                                     39.00   \n",
       "\n",
       "   LC_transactions[Amount] (30 days max)  \\\n",
       "0                                   8.42   \n",
       "1                                  95.00   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                  73.30   \n",
       "\n",
       "          LC_transactions[AccountID] (1 week tokens)  LC_profile[emp_length]  \\\n",
       "0                               {\"a355056969\" : 6.0}                < 1 year   \n",
       "1  {\"a366458676\" : 12.0, \"a547274624\" : 4.0, \"a38...                 3 years   \n",
       "2          {\"a643390547\" : 2.0, \"a346560403\" : 10.0}               10+ years   \n",
       "3                               {\"a458669971\" : 1.0}                 9 years   \n",
       "4  {\"a249156803\" : 4.0, \"a448123747\" : 1.0, \"a505...                 5 years   \n",
       "\n",
       "   LC_transactions[AccountID] (word count) (30 days sum)  \\\n",
       "0                                               15.0       \n",
       "1                                               88.0       \n",
       "2                                              113.0       \n",
       "3                                                5.0       \n",
       "4                                               41.0       \n",
       "\n",
       "        LC_transactions[Description] (1 week counts)  \\\n",
       "0  {\"internet payment\" : 2.0, \"international tran...   \n",
       "1  {\"amortisation\" : 1.0, \"internet payment\" : 2....   \n",
       "2  {\"internet payment\" : 1.0, \"telco\" : 1.0, \"sup...   \n",
       "3                         {\"internet payment\" : 1.0}   \n",
       "4  {\"internet payment\" : 1.0, \"retail furniture\" ...   \n",
       "\n",
       "   LC_transactions[Date] (Day of Month) (1 week most frequent)  \\\n",
       "0                                               23.0             \n",
       "1                                                3.0             \n",
       "2                                               18.0             \n",
       "3                                               12.0             \n",
       "4                                               11.0             \n",
       "\n",
       "   LC_profile[zip_code]  \n",
       "0                 782xx  \n",
       "1                 010xx  \n",
       "2                 890xx  \n",
       "3                 277xx  \n",
       "4                 939xx  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/result.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429668b-607d-40fa-9be1-dffc2bda6ff9",
   "metadata": {},
   "source": [
    "### 6. Stop the cluster and clean up\n",
    "Once you are done with the tasks, you can stop and remove the Spark cluster containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4acd8402-befe-45b0-8a41-60ea9eb7db8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 0/0\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m0.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m0.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m0.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m0.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m0.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m0.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m0.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m0.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m0.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m0.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m0.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m0.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m0.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m0.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m0.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m0.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                      \u001b[34m1.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                      \u001b[34m1.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m1.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m1.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m1.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m1.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m1.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m1.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m1.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m1.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m1.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m1.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m1.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m1.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m1.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m1.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m1.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m1.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m1.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m1.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                      \u001b[34m2.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                      \u001b[34m2.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m2.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m2.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m2.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m2.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m2.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m2.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m2.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m2.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m2.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m2.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m2.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m2.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m2.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m2.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m2.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m2.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m2.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m2.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                      \u001b[34m3.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                      \u001b[34m3.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m3.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m3.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m3.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m3.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m3.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m3.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m3.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m3.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m3.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m3.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m3.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m3.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m3.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m3.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m3.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m3.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m3.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m3.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                      \u001b[34m4.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                      \u001b[34m4.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m4.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m4.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m4.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m4.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m4.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m4.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m4.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m4.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m4.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m4.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m4.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m4.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m4.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m4.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m4.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m4.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m4.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m4.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                      \u001b[34m5.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                      \u001b[34m5.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m5.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m5.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m5.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m5.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m5.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m5.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m5.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m5.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m5.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m5.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m5.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m5.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m5.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m5.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m5.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m5.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m5.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m5.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                      \u001b[34m6.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                      \u001b[34m6.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m6.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m6.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m6.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m6.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m6.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m6.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m6.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m6.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m6.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m6.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m6.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m6.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m6.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m6.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m6.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m6.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m6.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m6.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                      \u001b[34m7.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                      \u001b[34m7.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m7.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m7.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m7.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m7.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m7.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m7.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m7.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m7.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m7.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m7.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m7.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m7.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m7.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m7.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m7.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m7.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m7.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m7.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                      \u001b[34m8.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                      \u001b[34m8.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m8.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m8.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m8.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m8.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m8.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m8.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m8.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m8.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m8.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m8.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m8.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m8.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m8.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m8.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m8.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m8.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m8.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m8.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                      \u001b[34m9.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                      \u001b[34m9.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                      \u001b[34m9.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                      \u001b[34m9.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                      \u001b[34m9.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                      \u001b[34m9.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠹ Container spark-worker-a  Stopping                                      \u001b[34m9.3s \u001b[0m\n",
      " ⠹ Container spark-worker-b  Stopping                                      \u001b[34m9.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠸ Container spark-worker-a  Stopping                                      \u001b[34m9.4s \u001b[0m\n",
      " ⠸ Container spark-worker-b  Stopping                                      \u001b[34m9.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠼ Container spark-worker-a  Stopping                                      \u001b[34m9.5s \u001b[0m\n",
      " ⠼ Container spark-worker-b  Stopping                                      \u001b[34m9.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠴ Container spark-worker-a  Stopping                                      \u001b[34m9.6s \u001b[0m\n",
      " ⠴ Container spark-worker-b  Stopping                                      \u001b[34m9.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠦ Container spark-worker-a  Stopping                                      \u001b[34m9.7s \u001b[0m\n",
      " ⠦ Container spark-worker-b  Stopping                                      \u001b[34m9.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠧ Container spark-worker-a  Stopping                                      \u001b[34m9.8s \u001b[0m\n",
      " ⠧ Container spark-worker-b  Stopping                                      \u001b[34m9.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠇ Container spark-worker-a  Stopping                                      \u001b[34m9.9s \u001b[0m\n",
      " ⠇ Container spark-worker-b  Stopping                                      \u001b[34m9.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠏ Container spark-worker-a  Stopping                                     \u001b[34m10.0s \u001b[0m\n",
      " ⠏ Container spark-worker-b  Stopping                                     \u001b[34m10.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠋ Container spark-worker-a  Stopping                                     \u001b[34m10.1s \u001b[0m\n",
      " ⠋ Container spark-worker-b  Stopping                                     \u001b[34m10.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/2\n",
      " ⠙ Container spark-worker-a  Stopping                                     \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-worker-b  Stopping                                     \u001b[34m10.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 2/2\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m0.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m0.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m0.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m0.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m0.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m0.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m0.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                      \u001b[34m1.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m1.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m1.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m1.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m1.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m1.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m1.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m1.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m1.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m1.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                      \u001b[34m2.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m2.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m2.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m2.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m2.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m2.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m2.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m2.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m2.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m2.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                      \u001b[34m3.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m3.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m3.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m3.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m3.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m3.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m3.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m3.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m3.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m3.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                      \u001b[34m4.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m4.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m4.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m4.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m4.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m4.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m4.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m4.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m4.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m4.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                      \u001b[34m5.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m5.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m5.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m5.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m5.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m5.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m5.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m5.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m5.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m5.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                      \u001b[34m6.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m6.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m6.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m6.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m6.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m6.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m6.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m6.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m6.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m6.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                      \u001b[34m7.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m7.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m7.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m7.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m7.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m7.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m7.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m7.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m7.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m7.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                      \u001b[34m8.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m8.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m8.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m8.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m8.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m8.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m8.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m8.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m8.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m8.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                      \u001b[34m9.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                      \u001b[34m9.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠙ Container spark-master    Stopping                                      \u001b[34m9.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠹ Container spark-master    Stopping                                      \u001b[34m9.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠸ Container spark-master    Stopping                                      \u001b[34m9.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠼ Container spark-master    Stopping                                      \u001b[34m9.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠴ Container spark-master    Stopping                                      \u001b[34m9.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠦ Container spark-master    Stopping                                      \u001b[34m9.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠧ Container spark-master    Stopping                                      \u001b[34m9.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠇ Container spark-master    Stopping                                      \u001b[34m9.9s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠏ Container spark-master    Stopping                                     \u001b[34m10.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b  \u001b[32mRemoved\u001b[0m                                      \u001b[34m10.2s \u001b[0m\n",
      " ⠋ Container spark-master    Stopping                                     \u001b[34m10.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 3/3\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mRemoved\u001b[0m                              \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mRemoved\u001b[0m                              \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mRemoved\u001b[0m                              \u001b[34m10.1s \u001b[0m\n",
      " ⠋ Network safer-recipe-sql_default  Removing                              \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 4/4\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-a          \u001b[32mRemoved\u001b[0m                              \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-worker-b          \u001b[32mRemoved\u001b[0m                              \u001b[34m10.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container spark-master            \u001b[32mRemoved\u001b[0m                              \u001b[34m10.1s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network safer-recipe-sql_default  \u001b[32mRemoved\u001b[0m                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25hUntagged: apache-spark:3.2.1\n",
      "Deleted: sha256:a5328dffb2c685c95621ae7149a6cae1f127138872a1042212219a4eba5439e5\n"
     ]
    }
   ],
   "source": [
    "# Stop the running containers\n",
    "!docker-compose down\n",
    "\n",
    "# Remove the Spark Docker image\n",
    "!docker image rm apache-spark:3.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756204d-10cd-428c-8e29-33ca1fc54418",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This tutorial provides a step-by-step guide on running Feature Discovery-generated SQL in an external Spark cluster, such as a Docker-based Spark environment. It demonstrates how to execute complex SQL queries, including custom User Defined Functions (UDFs), in a more scalable Spark cluster. By following this framework, users can offload resource-intensive SQL tasks to larger Spark clusters, allowing for greater flexibility in handling large datasets. The same approach can be adapted to other Spark platforms, enabling seamless integration with various infrastructure setups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
