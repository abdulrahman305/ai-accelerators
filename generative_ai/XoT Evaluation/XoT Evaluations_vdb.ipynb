{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7085d5",
   "metadata": {},
   "source": [
    "# Everything of Thoughts (XoT) implementation in DataRobot\n",
    "\n",
    "Authors: yifu.gu@datarobot.com, greig.bloom@datarobot.com, mitsuo.yamamoto@datarobot.com\n",
    "\n",
    "## Summary\n",
    "\n",
    "This accelerator introduces the implementation and evaluation of XoT (Everything of Thoughts) in DataRobot, which is the latest approach to make generative AI \"think like humans.\" In the world of generative AI, various methods (called thought generation) are being researched to help AI acquire more human-like \"thinking patterns.\" In particular, XoT aims to produce more accurate answers by teaching generative AI the \"thinking process.\" There are two main methods to achieve XoT:\n",
    "\n",
    "* Chain-of-Thought (CoT)[1]: A method of thinking by connecting multiple thoughts like a chain and reasoning through them.\n",
    "\n",
    "* Retrieval Augmented Thought Tree (RATT)[2]: A method of thinking by expanding multiple possibilities like tree branches and retrieving relevant information from the external knowledge base.\n",
    "\n",
    "This accelerator explains how to implement these methods. Specifically, it introduces how to set up and compare three types of LLM prompts: direct, Chain-of-Thought, and RATT. \"Direct\" referring to the well-known \"you are a helpful assistant.\" The accelerator also explains how to conduct performance evaluations using sample datasets, comparing the accuracy and efficiency of each method, and analyze using multiple evaluation metrics.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This script uses Pulumi to set up the DataRobot environment. If Pulumi is not already installed, install the CLI by following the instructions [here](https://www.pulumi.com/docs/iac/download-install/). After installing for the first time, restart your terminal and run:\n",
    "\n",
    "```bash\n",
    "pulumi login --local  # omit --local to use Pulumi Cloud (requires separate account)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c6149",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This section imports necessary Python packages and sets up the environment. Configure the DataRobot client and define paths for input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63eb5262-2f43-4a9d-b107-e4fc0b402d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import datarobot as dr\n",
    "from dotenv import load_dotenv\n",
    "from drops import get_prompt_count, get_trace_data\n",
    "import pandas as pd\n",
    "import pulumi\n",
    "from pulumi import automation as auto\n",
    "import pulumi_datarobot as datarobot\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# display all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b449f7b8-4c12-4780-806b-2195a43ca637",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PULUMI_CONFIG_PASSPHRASE\"] = \"dr\"\n",
    "client = dr.Client()\n",
    "\n",
    "path_csv = \"https://s3.us-east-1.amazonaws.com/datarobot_public_datasets/ai_accelerators/ragbench_test_demo.csv\"\n",
    "df_sample = pd.read_csv(path_csv)\n",
    "prompt_count = df_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd9509e6-9ca3-4a63-93f1-d87aa96403a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_up(project_name: str, stack_name: str, program: callable) -> auto.Stack:\n",
    "    # create (or select if one already exists) a stack that uses our inline program\n",
    "    stack = auto.create_or_select_stack(\n",
    "        stack_name=stack_name, project_name=project_name, program=program\n",
    "    )\n",
    "\n",
    "    stack.refresh(on_output=print)\n",
    "\n",
    "    stack.up(on_output=print)\n",
    "    return stack\n",
    "\n",
    "\n",
    "def destroy_project(stack: auto.Stack):\n",
    "    \"\"\"Destroy pulumi project\"\"\"\n",
    "    stack_name = stack.name\n",
    "    stack.destroy(on_output=print)\n",
    "\n",
    "    stack.workspace.remove_stack(stack_name)\n",
    "    print(f\"stack {stack_name} in project removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44755472-9cd3-4363-845d-4b16ea7482f5",
   "metadata": {},
   "source": [
    "## Create GenAI components\n",
    "\n",
    "This section creates the components needed to implement three different prompt styles:\n",
    "\n",
    "1. Single stage question-answering (\"You are a helpful assistant\")\n",
    "2. Chain of thought reasoning\n",
    "3. Retrieval-augmented thought tree (RATT)\n",
    "\n",
    "For each style, prepare system prompts, LLM configurations, and evaluation datasets.\n",
    "\n",
    "Note that while RATT typically achieves more accurate responses through multiple iterations of questioning using LLM outputs, in this implementation you can adopt an approach that aims to obtain accurate responses in a single pass by carefully crafting the input prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4691bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read prompt from *.md\n",
    "with open(\"system_prompt_chain_of_thought.md\", \"r\") as file:\n",
    "    system_prompt_chain_of_thought = file.read()\n",
    "with open(\"system_prompt_ratt.md\", \"r\") as file:\n",
    "    system_prompt_ratt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8486a72-0953-4337-b3dc-95f5e8a7eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"XOT Evaluations Demo\"\n",
    "LLM_ID = \"azure-openai-gpt-4-o-mini\"\n",
    "\n",
    "\n",
    "def make_vdb_and_playground():\n",
    "    # Usecase\n",
    "    use_case = datarobot.UseCase(resource_name=PROJECT)\n",
    "\n",
    "    # Eval Dataset\n",
    "    dataset_eval = datarobot.DatasetFromFile(\n",
    "        resource_name=f\"{PROJECT} - Dataset Eval\",\n",
    "        file_path=path_csv,\n",
    "        use_case_ids=[use_case.id],\n",
    "    )\n",
    "\n",
    "    # Playground\n",
    "    playground = datarobot.Playground(\n",
    "        resource_name=f\"{PROJECT} - Playground\", use_case_id=use_case.id\n",
    "    )\n",
    "\n",
    "    # LLM BPs\n",
    "    llm_bp_direct = datarobot.LlmBlueprint(\n",
    "        resource_name=f\"Direct BP\",\n",
    "        llm_id=LLM_ID,\n",
    "        playground_id=playground.id,\n",
    "        llm_settings=datarobot.LlmBlueprintLlmSettingsArgs(\n",
    "            max_completion_length=4096,\n",
    "            system_prompt=\"You are a helpful assistant. Answer the question in a direct manner. Be straightforward.\",\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "        ),\n",
    "        prompt_type=\"ONE_TIME_PROMPT\",\n",
    "    )\n",
    "    llm_bp_cot = datarobot.LlmBlueprint(\n",
    "        resource_name=f\"Chain-of-thought BP\",\n",
    "        llm_id=LLM_ID,\n",
    "        playground_id=playground.id,\n",
    "        llm_settings=datarobot.LlmBlueprintLlmSettingsArgs(\n",
    "            max_completion_length=4096,\n",
    "            system_prompt=system_prompt_chain_of_thought,\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "        ),\n",
    "        prompt_type=\"ONE_TIME_PROMPT\",\n",
    "    )\n",
    "    llm_bp_ratt = datarobot.LlmBlueprint(\n",
    "        resource_name=f\"RATT BP\",\n",
    "        llm_id=LLM_ID,\n",
    "        playground_id=playground.id,\n",
    "        llm_settings=datarobot.LlmBlueprintLlmSettingsArgs(\n",
    "            max_completion_length=4096,\n",
    "            system_prompt=system_prompt_ratt,\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "        ),\n",
    "        prompt_type=\"ONE_TIME_PROMPT\",\n",
    "    )\n",
    "    pulumi.export(\"use_case_id\", use_case.id)\n",
    "    pulumi.export(\"dataset_eval_id\", dataset_eval.id)\n",
    "    pulumi.export(\"playground_id\", playground.id)\n",
    "    pulumi.export(\"llm_bp_direct_id\", llm_bp_direct.id)\n",
    "    pulumi.export(\"llm_bp_cot_id\", llm_bp_cot.id)\n",
    "    pulumi.export(\"llm_bp_ratt_id\", llm_bp_ratt.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d44a0-f2d8-4744-83c9-42ec7433098d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "project = \"Evaluating_XoT_strategies\"\n",
    "stack_name = \"eval_XoT_strategies_demo\"\n",
    "stack = stack_up(project_name=project, stack_name=stack_name, program=make_vdb_and_playground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7599d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = stack.outputs()\n",
    "use_case_id = output[\"use_case_id\"].value\n",
    "playground_id = output[\"playground_id\"].value\n",
    "dataset_eval_id = output[\"dataset_eval_id\"].value\n",
    "playground_id = output[\"playground_id\"].value\n",
    "llm_bp_direct_id = output[\"llm_bp_direct_id\"].value\n",
    "llm_bp_cot_id = output[\"llm_bp_cot_id\"].value\n",
    "llm_bp_ratt_id = output[\"llm_bp_ratt_id\"].value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f998b2-a2d6-4856-8b74-b593fee2853f",
   "metadata": {},
   "source": [
    "## Running evaluations\n",
    "\n",
    "This section uses the created components to perform actual evaluations.\n",
    "The evaluation process consists of the following steps:\n",
    "\n",
    "1. Set up evaluation datasets.\n",
    "2. Define evaluation metrics (correctness, latency, token count, etc.).\n",
    "3. Generate responses using each prompt style,\n",
    "4. Collect and analyze results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e8449bf-51e1-41f2-a265-86685e92aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dr.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b24bf0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_config = (\n",
    "    dr.models.genai.evaluation_dataset_configuration.EvaluationDatasetConfiguration.create(\n",
    "        name=\"XoT Evaluation Dataset\",\n",
    "        use_case_id=use_case_id,\n",
    "        dataset_id=dataset_eval_id,\n",
    "        prompt_column_name=\"text\",\n",
    "        playground_id=playground_id,\n",
    "        is_synthetic_dataset=False,\n",
    "        response_column_name=\"response\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bab7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ootb metrics\n",
    "ootb_metrics = dr.models.genai.ootb_metric_configuration.PlaygroundOOTBMetricConfiguration.create(\n",
    "    playground_id=playground_id,\n",
    "    ootb_metric_configurations=[\n",
    "        dr.models.genai.insights_configuration.InsightsConfiguration(\n",
    "            **{\n",
    "                \"ootb_metric_name\": \"citations\",\n",
    "                \"insight_name\": \"Citations\",\n",
    "            }\n",
    "        ),\n",
    "        dr.models.genai.insights_configuration.InsightsConfiguration(\n",
    "            **{\n",
    "                \"ootb_metric_name\": \"latency\",\n",
    "                \"insight_name\": \"Latency\",\n",
    "            }\n",
    "        ),\n",
    "        dr.models.genai.insights_configuration.InsightsConfiguration(\n",
    "            **{\n",
    "                \"ootb_metric_name\": \"response_tokens\",\n",
    "                \"insight_name\": \"Response tokens\",\n",
    "            }\n",
    "        ),\n",
    "        dr.models.genai.insights_configuration.InsightsConfiguration(\n",
    "            **{\n",
    "                \"ootb_metric_name\": \"correctness\",\n",
    "                \"insight_name\": \"Correctness\",\n",
    "                \"llm_id\": \"azure-openai-gpt-4-o\",\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1af748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_configuration = dr.models.genai.metric_insights.MetricInsights.list(\n",
    "    playground=playground_id\n",
    ")\n",
    "path = \"genai/evaluationDatasetMetricAggregations/\"\n",
    "payload = {\n",
    "    \"chat_name\": \"eval1\",\n",
    "    \"llm_blueprint_ids\": [llm_bp_direct_id, llm_bp_cot_id, llm_bp_ratt_id],\n",
    "    \"evaluation_dataset_configuration_id\": eval_dataset_config.id,\n",
    "    \"insights_configuration\": [insight.to_dict() for insight in insights_configuration],\n",
    "}\n",
    "response = client.post(path, json=payload)\n",
    "job_id = response.json()[\"jobId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa4b4ad",
   "metadata": {},
   "source": [
    "Retrieve the result from DataRobot.\n",
    "\n",
    "Run the following only **AFTER** aggreration jobs on DataRobot finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for aggregation job to finish(=all prompts are processed)\n",
    "while True:\n",
    "    processed_count = get_prompt_count(\n",
    "        playground_id=playground_id, headers=client.headers, endpoint=client.endpoint\n",
    "    )\n",
    "    print(f\"Processed count: {processed_count}\")\n",
    "    if processed_count >= prompt_count * 3:  # incase other prompts are processed\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e205cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trace = get_trace_data(\n",
    "    playground_id=playground_id, headers=client.headers, endpoint=client.endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trace.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8eb24",
   "metadata": {},
   "source": [
    "Use the following cells to generate a comparison chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa1631",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_chat = dr.models.genai.comparison_chat.ComparisonChat.create(\n",
    "    \"EvalCompare\", playground=playground_id\n",
    ")\n",
    "compare_chat_id = compare_chat.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6abc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_prompt_ids = []\n",
    "\n",
    "for q in tqdm(df_sample[\"text\"]):\n",
    "    compare_prompt = dr.models.genai.comparison_prompt.ComparisonPrompt.create(\n",
    "        llm_blueprints=[llm_bp_direct_id, llm_bp_cot_id, llm_bp_ratt_id],\n",
    "        text=q,\n",
    "        comparison_chat=compare_chat_id,\n",
    "        wait_for_completion=True,  # need to wait to submit the next prompt\n",
    "    )\n",
    "    compare_prompt_ids.append(compare_prompt.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9413d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a link to open DataRobot\n",
    "url = f\"https://app.datarobot.com/usecases/{use_case_id}/playgrounds/{playground_id}/comparison/aggregations?comparisonChatId={compare_chat_id}&llmBlueprintId1={llm_bp_direct_id}&llmBlueprintId2={llm_bp_cot_id}&llmBlueprintId3={llm_bp_ratt_id}\"\n",
    "print(f\"Comparison URL: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa2ace",
   "metadata": {},
   "source": [
    "## Analyze results\n",
    "\n",
    "This section analyzes the evaluation results and compares the performance of each prompt style (the result can be viewed in the DataRobot UI).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "639a6886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llmBlueprintName</th>\n",
       "      <th>metrics_correctness</th>\n",
       "      <th>metrics_latency</th>\n",
       "      <th>metrics_response_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chain-of-thought BP-587bec2</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.72108</td>\n",
       "      <td>384.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Direct BP-3f2ec8d</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.81980</td>\n",
       "      <td>27.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RATT BP-c04cd5e</td>\n",
       "      <td>3.07</td>\n",
       "      <td>8.49068</td>\n",
       "      <td>636.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              llmBlueprintName  metrics_correctness  metrics_latency  \\\n",
       "0  Chain-of-thought BP-587bec2                 3.75          4.72108   \n",
       "1            Direct BP-3f2ec8d                 3.22          0.81980   \n",
       "2              RATT BP-c04cd5e                 3.07          8.49068   \n",
       "\n",
       "   metrics_response_tokens  \n",
       "0                   384.38  \n",
       "1                    27.56  \n",
       "2                   636.24  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trace.groupby(\"llmBlueprintName\")[\n",
    "    [\n",
    "        \"metrics_correctness\",\n",
    "        \"metrics_latency\",\n",
    "        \"metrics_response_tokens\",\n",
    "    ]\n",
    "].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead904a",
   "metadata": {},
   "source": [
    "The results show that the Chain of Thought approach demonstrates higher accuracy compared to other approaches.\n",
    "This suggests the effectiveness of explicitly making generative AI show its thought process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2471b6",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860417f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean up\n",
    "# project = \"Evaluating_XoT_strategies\"\n",
    "# stack_name = \"eval_XoT_strategies_demo\"\n",
    "# stack = auto.create_or_select_stack(\n",
    "#     stack_name=stack_name, project_name=project, program=lambda: None\n",
    "# )\n",
    "# destroy_project(stack)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
