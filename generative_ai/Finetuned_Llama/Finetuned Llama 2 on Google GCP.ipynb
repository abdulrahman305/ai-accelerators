{
 "cells": [
  {
   "id": "65a847c4b7e1adf036c9063c",
   "cell_type": "markdown",
   "source": "Fine-Tuned Llama 2 on Google GCP and DataRobot\n======================================\n\nThere are a wide variety of open source models. For example, there has been a lot of interest in LLama and variations such as Alpaca or Vicuna, Falcon, Mistral etc. Hosting these is a challenge as they require GPUs which are expensive so often customers want to compare cloud providers to find the best hosting option to meet their own needs. In this example we will work with Google Cloud Platform.\n\nIn addition, customers may want to integrate with the same cloud provider that hosts their VPC. That way they can ensure proper authentication and access only from within their VPC. While this authenticator uses authentication over the public internet, it should then be possible for the user to extend to leverage Google's cloud infrastructure to adjust to suit their cloud architectural needs, including provisioning scale out policies.\n\nFinally, by leveraging Vertex AI in a managed format, it can integrate into the customer's existing infrastructure level monitoring needs. For example, instances can be labelled to correspond to the customer's billing attribution polices, or logs and analytics can be set up to be written into their Big Query for monitoring and analytics. ",
   "metadata": {
    "name": "Overview",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c9063d",
   "cell_type": "markdown",
   "source": "Llama 2\n==========\n\nFor information about Llama 2 you can read the model card on HuggingFace [https://huggingface.co/meta-llama/Llama-2-7b-chat-hf], the Arxiv page [https://arxiv.org/abs/2307.09288] and the release anouncement [https://ai.meta.com/llama/]. It is available from Meta after signing the form [https://ai.meta.com/resources/models-and-libraries/llama-downloads/] \n\n\nLllama 13B-Instruct\n===============\n\nThis is designed to follow user instructions by fine-tuning on instruction datasets available on HuggingFace. As part of this it was trained to use `[INST]` and `[/INST]` controls tokens around user messages as well as begin of system id `<s>`. For example:\n\n* \"`<s>`[INST] What is your favourite condiment? [/INST]\"",
   "metadata": {
    "name": "Llama 2 Overview",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c9063e",
   "cell_type": "markdown",
   "source": "Overview of GCP\n===============\n    \nGCP instance types that can host Llama-13B with acceleration\n\n* g2-standard-8 with 1 L4 GPU: 8 vCPUs, 32 GB of RAM, \\$.85 ph + 64 GB (\\$623 per month)\n* n1-standard-16 with 2 V100 GPUs: 16 vCPUs, 60GB of RAM, \\$.76 ph + 32 GB (\\$388 per month)\n* n1-standard-16 with 2 T4 GPUs: 16 vCPUS, 60GB of RAM + 32 GB + 32 GB (\\$388 per month)\n* a2-highgpu-1g with 1 A100 GPU: 12 vCPUs, 85GB of RAM, \\$3.7 ph with  40GB (\\$2,682 per month)\n\n## 1. GCP Projects\n\nEverything in GCP is owned by a project, which tracks billing, authentication, access control, etc. Whenever you interact in either the GUI or using the API clients you will need to be in a project context. You can create a project at [https://console.cloud.google.com/projectcreate] or under IAM & Admin > Create a Project.\n\n## 2. Authentication and Service Accounts\n\nGCP does *not* provide \"API Keys\". Instead it provides auto-expiring dynamic tokens after authorizing. Each separate request you make to Google will have a different token in the headers. After a period of time you will have to reauthorize. If you use Collab, Workbench, or Cloud Shell, Google will hande the authentication in the background for you after authorization. \n\nIn order for our envionsed workflow to work, we will need a service account, an account which will run the cloud function workflow on our behalf (in this situation our account will be the principal). Using a service account, as opposed to a user account, means that multiple people can use our flow. In addition, there are certain things within GCP that can only be done by service accounts.\n\nThis service account will need access to the following roles:\n\n* Vertex AI User\n\nAs well as the following permissions on our Cloud Stoage bucket to be able to write to it.\n* Storage Legacy Bucket Owner\n* Storage Legacy Object Owner\n\nFor more information about service accounts look here: [https://cloud.google.com/iam/docs/service-accounts-create].\n\n## 3. Regions\n\nWe will use us-central1 (Iowa) for everything. This is because it is one of the two regions that have extensive GPU capacity (along with eu-west4 (Netherlands). The instance types available within Vertex AI vary by region.\n\n\n## 4. Cloud Storage Bucket\n\nLastly, before getting started we will need a bucket to hold stuff as we work in GCP. Google in general requires a bucket to be able to execute many of the tasks since they require storing some information somewhere, and a bucket is where that happens\n\nOnce you have decided on a project, location, bucket and storage account fill in the values below:\n                                                                                                                            ",
   "metadata": {
    "name": "Introduction to GCP",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c9063f",
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "# Cloud project id.\n",
    "PROJECT_ID = \"octo-385122\"  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output.\n",
    "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
    "BUCKET_URI = \"gs://octo-ephermal-storage\"  # @param {type:\"string\"}\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"octo-experiments-service-accou@octo-385122.iam.gserviceaccount.com\"  # @param {type:\"string\"}\n",
    "\n",
    "# GCP Tags\n",
    "# Fill in this dictionary with the required values to ensure the GCP resources are tagged correctly.\n",
    "# These will be applied as labels to the resources created\n",
    "GCP_TAGS = {\n",
    "    # e.g. \"contact\": \"foo\"\n",
    "    \"contact\": \"100007\",\n",
    "    \"cost-center\": \"octo\",\n",
    "    \"label\": \"genai\",\n",
    "    \"environment\": \"dev\",\n",
    "    \"project\": \"genai\",\n",
    "    \"expiration\": (datetime.datetime.utcnow() + datetime.timedelta(days=2)).strftime(\n",
    "        \"%Y-%m-%d\"\n",
    "    ),  # t%H:%M:%Sz\") # deault liftetime is up to 48H\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90640",
   "cell_type": "markdown",
   "source": "First time setup \n================\n\nThe following sections consist of the tasks you should do once the first time to setup the enviornment and then you will need to restart the kernel. We will do the following steps:\n\n1. Install the lastest version google cloud ai platform SDK.\n\nThis is the latest version that interacts with VertexAI\n\n2. Set up authentication by using application default credenitals created in a different environment\n\nThe typical authoriztion flow is command line command --> web browser authorization --> enter authorization information. In our notebooks that is tricky as we would need to be able to enter the authorization into the output shell. We also typically would generate both cli credentials via `gcloud init` or `gcloud auth login` which would create credentials in a local credentials db as well as create application default credentials via `gcloud auth application-default login` to generate the credentials usable via the python library. \n\nInstead the flow will be: generate credentials on your local machine --> base64 encode --> update the environment variables to point to the uploaded credentials. These will be stored [https://docs.datarobot.com/en/docs/dr-notebooks/code-nb/dr-env-nb.html#environment-variables encrypted by DataRobot] but keep in mind that base64 encoding itself isn't encryption and treat these base64 encoded credentials like you would treat any password.\n\n1. Create local credentials\n\nInstall the google sdk in your local enivornment. Then run `gcloud init` and make sure to set your default project accordingly. Then you can run `gcloud auth application-default login`\n\nTo install the google command line sdk in your local environment you can run:\n\n```\ncurl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-445.0.0-linux-x86_64.tar.gz\ntar -xf \"google-cloud-cli-445.0.0-linux-x86_64.tar.gz\"\n./google-cloud-sdk/install.sh --usage-reporting=false --quiet\n```\n\nYou should see output like:\n```\nCredentials saved to file: [/Users/mark/.config/gcloud/application_default_credentials.json]\n\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\n\nQuota project \"octo-385122\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n```\n\n2. Base64 credentials\n\nThe recommended version is to base64 encode as an environment variable. This is because any credentials uploaded to file storage is immediately shared when sharing the notebook. This can leverage DataRobot's built in credential storage.\n\n```\nbase64 -i /Users/mark/.config/gcloud/application_default_credentials.json\n```\n\nto generate the string. You can then copy and paste this as the environment variable: `GOOGLE_ENCODED_CREDENTIAL`\n\n3. Create credentials object\n\nIf you base64 encoded you can then create a credentials object in your local notebook environment via running the following:\n\n```\nimport base64\nimport os\nimport google.oauth2.credential\n\nadc_decoded = base64.b64decode(os.getenv(\"GOOGLE_ENCODED_CREDENTIAL\"))\ncredentials = google.oauth2.credentials.Credentials.from_authorized_user_info(json.loads(adc_decoded))\n```\n\n\nNow we can pass the credentials into `aiplatfrom.init` to set the global configuration which will be used in future calls to Vertex AI\n\n```\nfrom google.cloud import aiplatform\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET, credentials=credentials)\n```\n\nThe following two cells will install the required packages and initialize the environment while the third cell will print out the current settings for user verification\n",
   "metadata": {
    "name": "Credential setup",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90641",
   "cell_type": "code",
   "source": [
    "!pip -q install --upgrade google-cloud-aiplatform\n",
    "!pip -q install requests\n",
    "!pip -q install datarobot-early-access"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90642",
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "import google.oauth2.credentials\n",
    "\n",
    "adc_decoded = base64.b64decode(os.getenv(\"GOOGLE_ENCODED_CREDENTIAL\"))\n",
    "assert adc_decoded is not None\n",
    "credentials = google.oauth2.credentials.Credentials.from_authorized_user_info(\n",
    "    json.loads(adc_decoded)\n",
    ")\n",
    "\n",
    "\n",
    "# Bucket for storing intermediate stuff\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "\n",
    "# Initialize the ai platform global configuration for future calls\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    credentials=credentials,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90643",
   "cell_type": "code",
   "source": [
    "# We can verify that the global configuation is set correctly\n",
    "from google.cloud.aiplatform import initializer\n",
    "\n",
    "print(f\"The current project is: {initializer.global_config.project}\")\n",
    "print(f\"The current location is: {initializer.global_config.location}\")\n",
    "print(f\"The current GCP bucket is: {initializer.global_config.staging_bucket}\")\n",
    "print(\n",
    "    f\"The current default service account is: {initializer.global_config.service_account}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 4,
     "data": {
      "text/plain": "The current project is: octo-385122\nThe current location is: us-central1\nThe current GCP bucket is: gs://octo-ephermal-storage/temporal\nThe current default service account is: octo-experiments-service-accou@octo-385122.iam.gserviceaccount.com\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 4
  },
  {
   "id": "65a847c4b7e1adf036c90644",
   "cell_type": "markdown",
   "source": "To deploy the model we will first upload it into Vertex's Model Registry and then we can deploy or undeploy from the endpoint where the actual provisioning of the instance happens. In Vertex AI the resources are set on the endpoint, not the model.\n\nModel Serving with HuggingFace transformers\n============================================\n\nVertex AI models can work with any docker container that provides an http endpoint for Vertex to pass along the generation request to. In this case the Vertex Endpoint will provide traffic sharing and versioning by handling the routing among multiple posssible backend models or instances while the docker container running the model will handle the actual request handling and prociessing.\n\nHence the need for both a model running framework (i.e. the actual loading of the model weights and computation) as well as a model serving framework (i.e. the web server and request processing). In this example we will use pytorch serving.\n\nModel running frameworks\n========================\n\nModel running frameworks provide a variety of options that try to acheive different goals\n\n* pytorch\n* HuggingFace Transformers\n* HuggingFace Accelerate\n* DeepSpeed Inference\n* Nvidia TensorRT-LLM\n* FasterTransformer\n\nModel Serving Frameworks\n========================\n\nModel serving frameworks handle both request level management (e.g. batching and allocating requests among different works) as well as tasks like real time streaming of generation. While outside the scope of this example, they are also able to do things such as efficient request level switching between different fine-tuned variants of a model using techniques like LoRA, watermarking requests, or integration into telemetry infrastructure like Prometheus. While some frameworks like Triton are designed to work with a variety of different model running frameworks, others are tightly coupled to a particular framework.\n\nOpen source options include:\n* FastAPI\n* DJL-Serving [https://github.com/deepjavalibrary/djl-serving], Apache 2.0 license\n* NVIDIA Triton [https://github.com/triton-inference-server/server], BSD-3 license\n* vLLM [https://github.com/vllm-project/vllm], Apache 2.0 license\n* HunggingFace Text Generation Inference (TGI) [https://github.com/huggingface/text-generation-inference], HFOILv1.0 license. \n\nHuggingFace Transformers on GCP\n===============================\n\nWe will use HuggingFace Transformers serving for this example. It will use the `AutoModelForCasualLM` and then will apply the LoRA weights by calling `PeftModel.from_pretrained` during the initalization. This will use `pytorch` to setup the appropriate device to use and actually run the model.\n \nThe first cell defiens the various constants to use. The model id and instance information is easily adjustable to try out different options. We leverage the built in docker container from Vertex AI, it is possible to instead build and upload an image to GCP for your own needs.\n\nThe next cell defines the deployment function. It creates the endpoint, uploads the docker container to create the model and then deploys the model onto the endpoint. The last cell calls it with the parameters from the first cell. Note that deploying can take a while.",
   "metadata": {
    "name": "Uploading to the GCP Model Registry and deploying to Vertex AI Endpoints",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90645",
   "cell_type": "code",
   "source": [
    "# This is the name of the model on hugging face. When the docker container\n",
    "# launches on vertex AI it will download and start this up\n",
    "base_model_name = \"llama2-13b-chat-hf\"\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
    "BASE_MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"base_model\")\n",
    "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
    "PREDICTION_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"prediction\")\n",
    "base_model_id = os.path.join(BASE_MODEL_BUCKET, base_model_name)\n",
    "\n",
    "\n",
    "# For training, use A100s\n",
    "# For inference use V100s\n",
    "\n",
    "training_machine_type = \"a2-highgpu-1g\"\n",
    "training_accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "training_accelerator_count = 1\n",
    "training_replica_count = 1\n",
    "\n",
    "# change the model type.\n",
    "hosting_machine_type = \"n1-standard-8\"\n",
    "hosting_accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "hosting_accelerator_count = 2\n",
    "\n",
    "# Specify the dataset name by its HuggingFace name\n",
    "# Original source:\n",
    "# Scrape of goodread quotes, CC Attribtion 4.0 International License\n",
    "# https://github.com/Abirate/Creating-dataset-using-Web-Scraping-BeautifulSoup-\n",
    "dataset_name = \"Abirate/english_quotes\"\n",
    "\n",
    "\n",
    "# Docker image to be used for serving the model. We will use vertexAI provided peft server to host the models\n",
    "PREDICTION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve:20231129_0948_RC00\""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90646",
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str):\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_peft(\n",
    "    model_name: str,\n",
    "    base_model_id: str,\n",
    "    finetuned_lora_model_path: str,\n",
    "    service_account: str,  # we will need to set the service account for use later\n",
    "    task: str,\n",
    "    machine_type: str = \"n1-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count: int = 2,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys LLama models with on Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name}-endpoint\", labels=GCP_TAGS\n",
    "    )\n",
    "\n",
    "    precision_loading_mode = \"bfloat16\"\n",
    "    if accelerator_type in [\"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_V100\"]:\n",
    "        precision_loading_mode = \"float16\"\n",
    "    serving_env = {\n",
    "        \"BASE_MODEL_ID\": base_model_id,\n",
    "        \"PRECISION_LOADING_MODE\": precision_loading_mode,\n",
    "        \"TASK\": task,\n",
    "    }\n",
    "    if finetuned_lora_model_path:\n",
    "        serving_env[\"FINETUNED_LORA_MODEL_PATH\"] = finetuned_lora_model_path\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=PREDICTION_DOCKER_URI,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/predictions/peft_serving\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        min_replica_count=1,  # autoscale to zero is not currently supported by GCP so please terminate your instance\n",
    "    )\n",
    "    return model, endpoint"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a95f6cd37cff21bf4f8517",
   "cell_type": "code",
   "source": [
    "FINETUNING_DATASET_PATH = \"\""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65b4122b13d42d3ac37bd883",
   "cell_type": "markdown",
   "source": "Fine Tuning using LoRA\n=======================\n\n\nLoRA is a parameter-effficeint approach for fine tuning models. Fine-tuning approaches are those that take an existing pretrained model and then try to adjust to a more specific task, This typical is done either by providing a particular corpus of documents that it tries to predict the next text or by providing gold standard prompts and completions. Parameter-efficient approaches rather than adjusting all the model parameters and weights try to adjust a portion of them. This aims to provide better tuning by being more efficient with the data as only a portion of the model will change rather than the whole model. They also aim to be more robust to overfitting, in general the hope is that the base model is peforming reasonable well and can be improved rather than something that doesn't form a good base to build on.\n\nOne of the most popykar techinques is Low-Rank Approximation or LoRA, the apporach used here. This adjusts a low-rank factorization of the weights that can then be placed on top of the existing model weights. This allows the model to adjust a large proportion of the weights by learning in this smaller subset.\n\nThis example uses a standard dataset got the purpose but can be replaced with any dataset in the appropriate jsonl format in cloud storage. It creates and runs on a custom pipeline. Because it is more intensive and needs to learn the gradients, in this situation an A100 is used,\n\nDataset:\n========\n\nFinetuning datasets are typically in jsonl format. In this situation we have 3 fields: the quote, the author, and some tags about the quote. \n\n```\n{\"quote\":\"“Be yourself; everyone else is already taken.”\",\"author\":\"Oscar Wilde\",\"tags\":[\"be-yourself\",\"gilbert-perreira\",\"honesty\",\"inspirational\",\"misattributed-oscar-wilde\",\"quote-investigator\"]}\n{\"quote\":\"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.”\",\"author\":\"Marilyn Monroe\",\"tags\":[\"best\",\"life\",\"love\",\"mistakes\",\"out-of-control\",\"truth\",\"worst\"]}\n,,,,\n```\n\nThe idea of such an example is to try and make the output behave more like the inspiring quotes contained in the dataset when answering the question.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a956300f3e4c8f642439f3",
   "cell_type": "code",
   "source": [
    "TRAIN_DOCKER_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train\"\n",
    ")\n",
    "finetuning_precision_mode = \"bfloat16\"\n",
    "template = \"\"\n",
    "\n",
    "job_name = get_job_name_with_datetime(\"llama2-lora-train\")\n",
    "output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    "    labels=GCP_TAGS,\n",
    ")\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=causal-language-modeling-lora\",\n",
    "        f\"--pretrained_model_id={base_model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--output_dir={output_dir}\",\n",
    "        \"--lora_rank=16\",\n",
    "        \"--lora_alpha=32\",\n",
    "        \"--lora_dropout=0.05\",\n",
    "        \"--warmup_steps=10\",\n",
    "        \"--max_steps=10\",\n",
    "        \"--learning_rate=2e-4\",\n",
    "        f\"--precision_mode={finetuning_precision_mode}\",\n",
    "        f\"--template={template}\",\n",
    "    ],\n",
    "    environment_variables={\"WANDB_DISABLED\": True},\n",
    "    replica_count=training_replica_count,\n",
    "    machine_type=training_machine_type,\n",
    "    accelerator_type=training_accelerator_type,\n",
    "    accelerator_count=training_accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    ")\n",
    "\n",
    "print(\"Trained models were saved in: \", output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "Training Output directory:\ngs://octo-ephermal-storage/temporal/aiplatform-custom-training-2024-01-29-20:47:01.054 \nView Training:\nhttps://console.cloud.google.com/ai/platform/locations/us-central1/training/5947922980699897856?project=948912860068\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "CustomContainerTrainingJob projects/948912860068/locations/us-central1/trainingPipelines/5947922980699897856 current state:\nPipelineState.PIPELINE_STATE_RUNNING\nView backing custom job:\nhttps://console.cloud.google.com/ai/platform/locations/us-central1/training/1131996080343351296?project=948912860068\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "CustomContainerTrainingJob projects/948912860068/locations/us-central1/trainingPipelines/5947922980699897856 current state:\nPipelineState.PIPELINE_STATE_RUNNING\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "CustomContainerTrainingJob projects/948912860068/locations/us-central1/trainingPipelines/5947922980699897856 current state:\nPipelineState.PIPELINE_STATE_RUNNING\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "CustomContainerTrainingJob projects/948912860068/locations/us-central1/trainingPipelines/5947922980699897856 current state:\nPipelineState.PIPELINE_STATE_RUNNING\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "CustomContainerTrainingJob projects/948912860068/locations/us-central1/trainingPipelines/5947922980699897856 current state:\nPipelineState.PIPELINE_STATE_RUNNING\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "CustomContainerTrainingJob projects/948912860068/locations/us-central1/trainingPipelines/5947922980699897856 current state:\nPipelineState.PIPELINE_STATE_RUNNING\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "CustomContainerTrainingJob projects/948912860068/locations/us-central1/trainingPipelines/5947922980699897856 current state:\nPipelineState.PIPELINE_STATE_RUNNING\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "CustomContainerTrainingJob run completed. Resource name: projects/948912860068/locations/us-central1/trainingPipelines/5947922980699897856\nTraining did not produce a Managed Model returning None. Training Pipeline projects/948912860068/locations/us-central1/trainingPipelines/5947922980699897856 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\nTrained models were saved in:  gs://octo-ephermal-storage/peft/model/llama2-lora-train_20240129_204701\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 8
  },
  {
   "id": "65b8100cee58d3cd09b91a16",
   "cell_type": "markdown",
   "source": "LLama 2 Hosting\n===============\n\nNow we can host LLama 2 on 2 V100s for hosting. This will create an endpoint that we can use to shape the flow between various models for comparision purposes as well as the ability to scale in resources based on demand.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90647",
   "cell_type": "code",
   "source": [
    "hosting_machine_type = \"n1-standard-8\"\n",
    "hosting_accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "hosting_accelerator_count = 2\n",
    "\n",
    "# Creating and deploying the endpoint will take some time (e.g. around 20 min)\n",
    "model_with_peft, endpoint_with_peft = deploy_model_peft(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"llama2-serve\"),\n",
    "    base_model_id=base_model_id,\n",
    "    task=\"causal-language-modeling-lora\",\n",
    "    finetuned_lora_model_path=f\"{output_dir}/trial_0\",\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=hosting_machine_type,\n",
    "    accelerator_type=hosting_accelerator_type,\n",
    "    accelerator_count=hosting_accelerator_count,\n",
    ")\n",
    "\n",
    "print(\"endpoint_name:\", endpoint_with_peft.name)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 35,
     "data": {
      "text/plain": "Creating Endpoint\nCreate Endpoint backing LRO: projects/948912860068/locations/us-central1/endpoints/6672058658693578752/operations/6942883697058119680\nEndpoint created. Resource name: projects/948912860068/locations/us-central1/endpoints/6672058658693578752\nTo use this Endpoint in another session:\nendpoint = aiplatform.Endpoint('projects/948912860068/locations/us-central1/endpoints/6672058658693578752')\nCreating Model\nCreate Model backing LRO: projects/948912860068/locations/us-central1/models/2409810629712936960/operations/5274300035117350912\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 35,
     "data": {
      "text/plain": "Model created. Resource name: projects/948912860068/locations/us-central1/models/2409810629712936960@1\nTo use this Model in another session:\nmodel = aiplatform.Model('projects/948912860068/locations/us-central1/models/2409810629712936960@1')\nDeploying model to Endpoint : projects/948912860068/locations/us-central1/endpoints/6672058658693578752\nDeploy Endpoint model backing LRO: projects/948912860068/locations/us-central1/endpoints/6672058658693578752/operations/3873680551005126656\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 35,
     "data": {
      "text/plain": "Endpoint model deployed. Resource name: projects/948912860068/locations/us-central1/endpoints/6672058658693578752\nendpoint_name: 6672058658693578752\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 35
  },
  {
   "id": "65a847c4b7e1adf036c90648",
   "cell_type": "markdown",
   "source": "The endpoint created is not public. Often a service is put in front to handle public requests, e.g. using cloud functions[https://cloud.google.com/functions] to authenticate the user request and then calling the endpoint. For services running in Goggle cloud on the same server account, Google will seamlessly hand the authentication. Since this examples uses a DataRobot deployment, any such logic can be added there so it is not include. Since the aiplatform is initalized with appropriate credentials it can just be called with that. Vertex AI expects that requests are in an `instances` array, it will take each element and pass that along to the model. It will then return the values from the endpoint in a `predictions` array.\n\nFor example as json:\n\n```\n{\ninstances: [{...}]\n}\n\n{\npredictions: [{...}]\n}\n```\n\nSince the endpoint is hitting the /generate route which is not OpenAI compatible, the input format is defined here: [https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py].\n\n```\n    The request should be a JSON object with the following fields:\n    - prompt: the prompt to use for the generation.\n    - stream: whether to stream the results or not.\n    - other fields: the sampling parameters (See `SamplingParams` for details).`\n```\n\nThe reponse will be a single JSON dict containing the key: `text` and the combined prompt + generated text as the value.",
   "metadata": {
    "name": "Prompting on GCP via credentials",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90649",
   "cell_type": "code",
   "source": [
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"<s><INST>How can bagging trees boost my Random Forest?</INST>\",\n",
    "        \"n\": 1,\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_with_peft.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 36,
     "data": {
      "text/plain": "Prompt:\n<s><INST>How can bagging trees boost my Random Forest?</INST>\nOutput:\n<s><INST>How can bagging trees boost my Random Forest?</INST>  Bagging (Bootstrap Aggregating) is a technique used to reduce the variance of a machine learning model and improve its generalization. When applied to a Random Forest, bagging can boost its performance by reducing overfitting and improving the robustness of the model. Here are some ways bagging can benefit a Random Forest:\n1. Reduces overfitting: Random Forests are prone to overfitting, especially when the number of trees is small. Bagging can help reduce overfitting by creating multiple versions of the same model with different subsets of the training data. This forces each tree to learn a different subset of the data, which can help prevent overfitting.\n2. Improves generalization: Bagging can improve the generalization of a Random Forest by reducing the variance of the model. By creating multiple versions of the model with different subsets of the data, bagging can help ensure that the model is less sensitive to any\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 36
  },
  {
   "id": "65a847c4b7e1adf036c9064a",
   "cell_type": "markdown",
   "source": "The endpoint URL can be called directly via REST. Since this a python environment, `requests` is a common library for making REST calls. To do so the authorization JWT token is included in the header. Since in this situation there are generated from personal credentials they will be short lived. To do so, the Google authentication libraries provide helper utilities to create a `requests.Session` object that manages adding the authentication token to the request.\n\nIf running locally, e.g. using `curl` the `gcloud` CLI can generate the tokens. To call via a subprocess invoke the following:\n```\nsubprocess.run([\"gcloud\", \"auth\", \"print-access-token\"], capture_output=True).stdout.decode().removesuffix(\"\\n\")\n```\nOr to invoke in the terminal:\n\n```\ngcloud auth print-access-token\n```\n",
   "metadata": {
    "name": "Prompting via REST",
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c9064b",
   "cell_type": "code",
   "source": [
    "endpoint_url = f\"https://{endpoint_with_peft.location}-aiplatform.googleapis.com/v1/{endpoint_with_peft.resource_name}\""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c9064c",
   "cell_type": "code",
   "source": [
    "from google.auth.transport.requests import AuthorizedSession\n",
    "\n",
    "# We will grab the acces token from the command line for our use to stick in the request headers\n",
    "# Note that for curl the command looks like\n",
    "# curl \\\n",
    "# -X POST \\\n",
    "# -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "# -H \"Content-Type: application/json\" \\\n",
    "# https://...\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "authed_session = AuthorizedSession(credentials)  # This creates a request.Session object\n",
    "\n",
    "response = authed_session.post(\n",
    "    endpoint_url + \":predict\", json={\"instances\": instances}, headers=headers\n",
    ")\n",
    "assert response.status_code == 200, response.json()\n",
    "print(response.json()[\"predictions\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 38,
     "data": {
      "text/plain": "Prompt:\n<s><INST>How can bagging trees boost my Random Forest?</INST>\nOutput:\n<s><INST>How can bagging trees boost my Random Forest?</INST>  Bagging (Bootstrap Aggregating) is a technique used to reduce the variability of a model and improve its generalization ability, particularly in ensemble learning methods like Random Forest. By bagging trees, you can boost your Random Forest model's performance in several ways:\n1. Reduce overfitting: Random Forest can suffer from overfitting, especially when the dataset is too complex or when there are too many variables. Bagging helps to reduce overfitting by creating multiple instances of the same model with different subsets of the training data. This ensures that each tree is built on a different subset of the data, which helps to prevent overfitting.\n2. Improve generalization: Bagging also helps to improve the generalization ability of the model. By training multiple instances of the model on different subsets of the data, you can get a better estimate of the model's performance on new, unseen data.\n3. Incre\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 38
  },
  {
   "id": "65a847c4b7e1adf036c9064d",
   "cell_type": "markdown",
   "source": " Since the URL is not public by default, it returns 401 unauthorized without a proper token",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c9064e",
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    endpoint_url + \":predict\",\n",
    "    json={\"instances\": instances},\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    ")\n",
    "assert response.status_code == 401\n",
    "response.json()"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 39,
     "data": {
      "text/plain": "{'error': {'code': 401,\n  'message': 'Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.',\n  'status': 'UNAUTHENTICATED',\n  'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo',\n    'reason': 'CREDENTIALS_MISSING',\n    'domain': 'googleapis.com',\n    'metadata': {'method': 'google.cloud.aiplatform.v1.PredictionService.Predict',\n     'service': 'aiplatform.googleapis.com'}}]}}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 39
  },
  {
   "id": "65a847c4b7e1adf036c9064f",
   "cell_type": "markdown",
   "source": "\nCreating a DataRobot Custom model deployment\n==================================\n\nTo bring this into DataRobot we can create a custom model. This will us to setup all the monitoring goodness of DataRobot around this model.\n\nBecause we would like to be able to have the deployment and be shareable without sharing our own private credentials (like we used above to interact with the deployment) we will use theservice account. \n\nFor local development we can impersonate the service account by using our existing credentials to generate service account credentials. This way we can validate that the service account is properly set up and has all the appropriate permissions.\n\n\n\nThen when we run as a deployment we can use Credential Management to hold the service account's private key. By doing this, the service account can then generate tokens using the private key for as long as the private key is valid. Also, by using a service account, on the google side it is no longer tied to the individual account and can be revoked separately. Lastly, if the key is compromised, the service account will only have the privileges granted it which are typically far less than a developer.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90650",
   "cell_type": "code",
   "source": [
    "# Redefine the constants from above so that can start right here with the session without redeploying on GCP\n",
    "SERVICE_ACCOUNT = \"octo-experiments-service-accou@octo-385122.iam.gserviceaccount.com\"\n",
    "\n",
    "# GCP Credential id from credential management with the uploaded private key\n",
    "DR_GCP_CREDENTIAL_ID = \"658c77e4ad86cd2ebecc9d7e\"\n",
    "\n",
    "# DR Prediction enviornment\n",
    "DR_PREDICTION_ENVIRONMENT_ID = \"5f06612df1740600260aca72\"\n",
    "\n",
    "\n",
    "custom_dir = \"llama2_gcp\"\n",
    "# api_token = subprocess.run([\"./gcloud\", \"auth\", \"print-access-token\", f\"--impersonate-service-account={SERVICE_ACCOUNT}\"], capture_output=True).stdout.decode().removesuffix(\"\\n\")\n",
    "\n",
    "# Here are the default runtime parameters we will need to set\n",
    "LOCATION = REGION  # or \"us-central1\"\n",
    "NUMERIC_PROJECT_ID = \"948912860068\"\n",
    "ENDPOINT_ID = endpoint_with_peft.name\n",
    "MAX_TOKENS = \"1000\"\n",
    "TEMPERATURE = \"1.0\"\n",
    "TOP_P = \"1.0\"\n",
    "TOP_K = \"10\"\n",
    "\n",
    "# This will be the prompt template that we wrap the prompt with. Mostly to handle the special tokens etc.\n",
    "SYSTEM_PROMPT_TEMPLATE = \"<s><INST>You are a helpful AI assistant, created by a French AI company. Be helpful and complete but also strive for concision. {}</INST> \""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90651",
   "cell_type": "markdown",
   "source": "As is typical for custom inference models, we will need to provide a model-metadata.yaml and a custom.py. The existing GenAI builtin environment has the google cloud libraries built in along with the other libraries used so a requirements.txt is not required. In the model metadata, the target type is set as `textgeneration` to have access to text generation specific monitoring capabilities and the various runtime parameters are set as options. Runtime parameters allow easy updating of the DataRobot model endpoint if needed as part of a governed upgrade version. By changing the endpoint parameters and creating a new version, it is then possible to compare an older model with a newer version, if an update is required. It also allows configuration of the temperature etc. as part of the deployment and exposes these settings as metadata within DataRobot for reference. ",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90652",
   "cell_type": "code",
   "source": [
    "model_metadata = \"\"\"\n",
    "name: gcp-vertex-proxy\n",
    "type: inference\n",
    "targetType: textgeneration\n",
    "\n",
    "runtimeParameterDefinitions:\n",
    "  - fieldName: location\n",
    "    type: string\n",
    "    defaultValue: us-central1\n",
    "    description: The GCP location the endpoint resides in.\n",
    "\n",
    "  - fieldName: gcp_credentials\n",
    "    type: credential\n",
    "    credentialtype: gcp\n",
    "    description: The GCP service account key. This will be set to use the credential id set in credential manageament\n",
    "    \n",
    "  - fieldName: projectId\n",
    "    type: string\n",
    "    description: Numeric GCP projet for the endpoint\n",
    "  \n",
    "  - fieldName: endpointId\n",
    "    type: string\n",
    "    description: Numeric Vertex AI endpoint id\n",
    "\n",
    "  - fieldName: maxTokens\n",
    "    type: string\n",
    "    defaultValue: \"200\"\n",
    "    description: Maximum number of tokens to return\n",
    "    \n",
    "  - fieldName: temperature\n",
    "    type: string\n",
    "    defaultValue: \"1.0\"\n",
    "    description: temperature of the model\n",
    "\n",
    "  - fieldName: promptTemplate\n",
    "    type: string\n",
    "    description: promptTemplate for the model. It should contain {} which will be filled in with the user prompt\n",
    "\n",
    "  - fieldName: topP\n",
    "    type: string\n",
    "    defaultValue: \"1.0\"\n",
    "    description: Top p for the model\n",
    "    \n",
    "  - fieldName: topK\n",
    "    type: string\n",
    "    defaultValue: \"10\"\n",
    "    description: Top k for the model\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90653",
   "cell_type": "markdown",
   "source": "And here is the custom py file written as text.\n\nIn the load model function we load the runtime parameters and verify that we can connect to the endpoint. It will prefer the passed token over the GCP service account credentials if provided. \n\nIt will then take the user provide prompt, inject it into the promptTemplate with a format and then make a prediction. Lastly, because Llama returns the prompt in the response, the original prompt is stripped out. Because the custom model API provides the data as pandas DataFrame into the `score` function and expects the output as a DataFrame as well, this is all run in a loop over the dataframe, causing each row of the DataFrame to be processed as a separate request. \n\nThe prompt column is set to `prompt` to match the name that will be set when the model is registercd with the playground. The generated text column is set to `response` and will be registered as the `target name` when the DR Deployment is created.\n\nLastly, this and the model metadata yaml are then written to the filesystem inside a directory for uploading to datarobot.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90654",
   "cell_type": "code",
   "source": [
    "custom_py = \"\"\"\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import ssl\n",
    "from types import SimpleNamespace\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datarobot_drum import RuntimeParameters\n",
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import google.auth.credentials\n",
    "from google.auth.transport.urllib3 import AuthorizedHttp\n",
    "import google.oauth2.credentials\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def _test_connectivity(location, project, endpoint_id, creds):\n",
    "    # The root path of the endpoint can be used for health checks\n",
    "    url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/endpoints/{endpoint_id}\"\n",
    "    authed_http = AuthorizedHttp(creds)  # Implements a urllib.RequestsMethods like a PoolManager\n",
    "    try:\n",
    "        authed_http.request('GET', url=url)\n",
    "    except urllib.error.HTTPError as error:\n",
    "        logger.error(\n",
    "            \"Failed to connect to %s status_code=%s\\\\n%s\",\n",
    "            url,\n",
    "            error.code,\n",
    "            error.read().decode(\"utf8\", \"ignore\"),\n",
    "        )\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_model(code_dir):\n",
    "    ''' Load runtime parameters and verify the endpoint is up'''\n",
    "    logger.info(\"Loading Runtime Parameters...\")\n",
    "    cred_parameter = RuntimeParameters.get('gcp_credentials')['gcpKey']\n",
    "\n",
    "    endpoint_id = RuntimeParameters.get(\"endpointId\")\n",
    "    location = RuntimeParameters.get(\"location\")\n",
    "    project = RuntimeParameters.get(\"projectId\")\n",
    "    prompt_template = RuntimeParameters.get(\"promptTemplate\")\n",
    "    max_tokens = int(RuntimeParameters.get(\"maxTokens\"))\n",
    "    temperature = float(RuntimeParameters.get(\"temperature\"))\n",
    "    top_p = float(RuntimeParameters.get(\"topP\"))\n",
    "    top_k = int(RuntimeParameters.get(\"topK\"))\n",
    "    \n",
    "    gcp_token = os.getenv('GOOGLE_ENCODED_SERVICE_ACCOUNT_TOKEN', None)\n",
    "    if gcp_token:\n",
    "        # Handle case user provided base64 encoded credentials, e.g. in a notebook\n",
    "        creds = google.oauth2.credentials.Credentials(token=gcp_token)\n",
    "    else:\n",
    "        # Handle case it pulled the values from the runtime parameters storage\n",
    "        creds = service_account.Credentials.from_service_account_info(cred_parameter)\n",
    "        creds = creds.with_scopes(['https://www.googleapis.com/auth/cloud-platform'])\n",
    "    _test_connectivity(location, project, endpoint_id, creds)\n",
    "\n",
    "    # Can return any object as a placeholder for a model that we can\n",
    "    # then use again in the `score()` function.\n",
    "    return SimpleNamespace(**locals())\n",
    "\n",
    "\n",
    "def make_vertex_prediction(user_prompt, model):\n",
    "    prompt = model.prompt_template.format(user_prompt)\n",
    "    instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"n\": 1,\n",
    "        \"max_tokens\": model.max_tokens,\n",
    "        \"temperature\": model.temperature,\n",
    "        \"top_p\": model.top_p,\n",
    "        \"top_k\": model.top_k,\n",
    "    },\n",
    "]\n",
    " \n",
    "    endpoint = aiplatform.Endpoint(model.endpoint_id, project=model.project,\n",
    "      location=model.location, credentials=model.creds)\n",
    "    response = endpoint.predict(instances=instances)\n",
    "\n",
    "    for prediction in response.predictions:\n",
    "        # Remove everything from the response but the generated text\n",
    "        out = prediction.partition(\"Output:\\\\n\")[2]\n",
    "    return out\n",
    "\n",
    "def score(data, model, **kwargs):\n",
    "    '''\n",
    "    This hook is only needed if you would like to use **drum** with a framework not natively\n",
    "    supported by the tool.\n",
    "\n",
    "    Note: While best practice is to include the score hook, if the score hook is not present\n",
    "    DataRobot will add a score hook and call the default predict method for the library\n",
    "    See https://github.com/datarobot/datarobot-user-models#built-in-model-support for details\n",
    "\n",
    "    This dummy implementation reverses all input text and returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : is the dataframe to make predictions against.\n",
    "    model : is the deserialized model loaded by **drum** or by `load_model`, if supplied\n",
    "    kwargs : additional keyword arguments to the method\n",
    "    Returns\n",
    "    -------\n",
    "    This method should return results as a dataframe with the following format:\n",
    "      Text Generation: must have column with target, containing text data for each input row.\n",
    "    '''\n",
    "    data = list(data[\"prompt\"])\n",
    "    generated_responses = [\"\".join(make_vertex_prediction(inp, model)) for inp in data]\n",
    "    result = pd.DataFrame({\"response\": generated_responses})\n",
    "    return result\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90655",
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.listdir(custom_dir)\n",
    "except:\n",
    "    os.mkdir(custom_dir)\n",
    "with open(f\"{custom_dir}/model-metadata.yaml\", \"w\") as f:\n",
    "    f.write(model_metadata)\n",
    "\n",
    "with open(f\"{custom_dir}/custom.py\", \"w\") as f:\n",
    "    f.write(custom_py)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90656",
   "cell_type": "markdown",
   "source": "We can test locally using our local credentials and drum to make sure everything is working correctly. We will use our local credentials to create a token for the service account via a method call impersonation. Impersonation allows a user to use their credentials, the source, to then get the credentials of a different entity, the target. Because both the scopes, what services the token is valid for, and the permissions are able to be narrowly defined, these tokens are generally far less powerful than the source credentials.\n\nSeparately, this makes it easier to manage access across multiple accounts. As long as a user has access to the service account, they have access to the resources to make a prediction without worrying about needing to grant the appropriate permissions for each indiviudal resource, it also means that a user can be removed easily without breaking things (e.g. if the creator leaves the organization).\n\nThis code makes a subprocess call to the `drum` custom model utility and checks that it completed successful. The output predictions should be displayed.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90657",
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "\n",
    "from google.auth import impersonated_credentials\n",
    "\n",
    "runtimeparams = f\"\"\"\n",
    "location: \"{LOCATION}\"\n",
    "projectId: \"{NUMERIC_PROJECT_ID}\"\n",
    "endpointId: \"{ENDPOINT_ID}\"\n",
    "maxTokens: \"{MAX_TOKENS}\"\n",
    "temperature: \"{TEMPERATURE}\"\n",
    "topP: \"{TOP_P}\"\n",
    "topK: \"{TOP_K}\"\n",
    "\n",
    "promptTemplate: \"{SYSTEM_PROMPT_TEMPLATE}\"\n",
    "\n",
    "gcp_credentials:\n",
    "  credentialType: \"gcp\"\n",
    "  gcpKey:  \"FAKE_CREDS\"\n",
    "\"\"\"\n",
    "\n",
    "input_csv = \"prompt,\\nHow soon is now?,\\nWhat are the names of the Greek winds?,\"\n",
    "\n",
    "with open(\"llama2_values\", \"w\") as f:\n",
    "    f.write(runtimeparams)\n",
    "\n",
    "with open(\"input.csv\", \"w\") as f:\n",
    "    f.write(input_csv)\n",
    "\n",
    "\n",
    "target_scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "target_credentials = impersonated_credentials.Credentials(\n",
    "    source_credentials=credentials,\n",
    "    target_principal=SERVICE_ACCOUNT,\n",
    "    target_scopes=target_scopes,\n",
    "    lifetime=500,\n",
    ")\n",
    "\n",
    "# Call refresh to generate the token\n",
    "target_credentials.refresh(request=google.auth.transport.requests.Request())\n",
    "\n",
    "envs = os.environ\n",
    "envs[\"GOOGLE_ENCODED_SERVICE_ACCOUNT_TOKEN\"] = target_credentials.token\n",
    "subprocess.check_call(\n",
    "    [\n",
    "        \"drum\",\n",
    "        \"score\",\n",
    "        \"--code-dir\",\n",
    "        f\"{custom_dir}\",\n",
    "        \"--target-type\",\n",
    "        \"textgeneration\",\n",
    "        \"--input\",\n",
    "        \"input.csv\",\n",
    "        \"--runtime-params-file\",\n",
    "        \"llama2_values\",\n",
    "    ],\n",
    "    env=envs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 44,
     "data": {
      "text/plain": "                                         Predictions\n0  <s><INST>You are a helpful AI assistant, creat...\n1  <s><INST>You are a helpful AI assistant, creat...\n0"
     },
     "metadata": {}
    }
   ],
   "execution_count": 44
  },
  {
   "id": "65a847c4b7e1adf036c90658",
   "cell_type": "markdown",
   "source": "Setting Runtime Parameters and Registering the Custom Model\n===========================================================\n\nNow that the it works locally it can be uploaded to DataRobot and added to the Model Registry. This will\n\n* Execution Environment - The python enviornment in which the code will run. This will use the prebuilt `Python 3,11 GenAI` environment, the last prerequisite for creating the Custom Model\n* Custom Inference Model - Created in the Custom Model Workshop with the appropriate metadata\n* Custom Inference Model Vesion - A particular version of the environment, python code and runtime parameters under the Custom Inferene Model\n* Custom Model Test -  Verification that the Custom Model works in the DataRobot environment\n* Registered Model - Moved from the Workshop to the Registry. At this point appropriate governance policies can be applied\n* Prediction Environment - Where the model is hosted if the account has multiple hosting options.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90659",
   "cell_type": "code",
   "source": [
    "import datarobot as dr\n",
    "\n",
    "execution_environment = dr.ExecutionEnvironment.list(\"Python 3.11 GenAI\")[0]"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c9065a",
   "cell_type": "code",
   "source": [
    "# Patch to asllow TextGeneration target types in the python client.\n",
    "dr.enums.CUSTOM_MODEL_TARGET_TYPE.ALL = dr.enums.CUSTOM_MODEL_TARGET_TYPE.ALL + [\n",
    "    \"TextGeneration\"\n",
    "]\n",
    "\n",
    "custom_model = dr.CustomInferenceModel.create(\n",
    "    name=\"Llama-13B Fine-tuned on GCP\",\n",
    "    target_type=\"TextGeneration\",\n",
    "    target_name=\"response\",  # Name as the output column we generate\n",
    "    description=\"Llama-13B proxy model\",\n",
    "    language=\"python\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c9065b",
   "cell_type": "markdown",
   "source": "After creating the specific model version, where we set that the container will have public network egress to be able to make calls over the internet.\n\nThe runtime parameters can be set via patching the REST API route until the datarobot sdk is updated to include setting runtime parameters.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c9065c",
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "model_version = dr.CustomModelVersion.create_clean(\n",
    "    custom_model_id=custom_model.id,\n",
    "    base_environment_id=execution_environment.id,\n",
    "    folder_path=custom_dir,\n",
    "    network_egress_policy=dr.NETWORK_EGRESS_POLICY.PUBLIC,  # need to be able to reach GCP over the public internet\n",
    ")\n",
    "\n",
    "url = model_version._path.format(model_version.custom_model_id)\n",
    "path = f\"{url}\"\n",
    "payload = {\n",
    "    \"baseEnvironmentId\": execution_environment.id,\n",
    "    \"runtimeParameterValues\": json.dumps(\n",
    "        [\n",
    "            {\"fieldName\": \"temperature\", \"type\": \"string\", \"value\": TEMPERATURE},\n",
    "            {\"fieldName\": \"location\", \"type\": \"string\", \"value\": LOCATION},\n",
    "            {\"fieldName\": \"projectId\", \"type\": \"string\", \"value\": NUMERIC_PROJECT_ID},\n",
    "            {\"fieldName\": \"endpointId\", \"type\": \"string\", \"value\": ENDPOINT_ID},\n",
    "            {\"fieldName\": \"maxTokens\", \"type\": \"string\", \"value\": MAX_TOKENS},\n",
    "            {\"fieldName\": \"topP\", \"type\": \"string\", \"value\": TOP_P},\n",
    "            {\"fieldName\": \"topK\", \"type\": \"string\", \"value\": TOP_K},\n",
    "            {\n",
    "                \"fieldName\": \"promptTemplate\",\n",
    "                \"type\": \"string\",\n",
    "                \"value\": SYSTEM_PROMPT_TEMPLATE,\n",
    "            },\n",
    "            {\n",
    "                \"fieldName\": \"gcp_credentials\",\n",
    "                \"type\": \"credential\",\n",
    "                \"value\": DR_GCP_CREDENTIAL_ID,\n",
    "            },\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "response = model_version._client.patch(path, json=payload)\n",
    "model_version = dr.CustomModelVersion.get(custom_model.id, response.json()[\"id\"])"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c9065d",
   "cell_type": "markdown",
   "source": "To validste that everything works, there is a testing facility provided. This requires uploading a dataset to then pass into the test to verify it works. After uploading and asserting that it succeeded this uploaded dataset can then be deleted. ",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c9065e",
   "cell_type": "code",
   "source": [
    "# Upload and create a dataset for testing custom model to validate it works correctly\n",
    "\n",
    "test_dataset = dr.Dataset.create_from_file(\"input.csv\")\n",
    "test_dataset.id"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 48,
     "data": {
      "text/plain": "'65b8251131bf2a827962106e'"
     },
     "metadata": {}
    }
   ],
   "execution_count": 48
  },
  {
   "id": "65a847c4b7e1adf036c9065f",
   "cell_type": "code",
   "source": [
    "custom_test = dr.CustomModelTest.create(\n",
    "    custom_model_id=custom_model.id,\n",
    "    custom_model_version_id=model_version.id,\n",
    "    dataset_id=test_dataset.id,\n",
    ")\n",
    "assert custom_test.overall_status == \"succeeded\", custom_test.get_log()"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90660",
   "cell_type": "code",
   "source": [
    "# now delete the uploaded test dataset for the custom model test\n",
    "test_dataset.delete(dataset_id=test_dataset.id)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90661",
   "cell_type": "markdown",
   "source": "Now that it has been verified to work, the model can be added to the registry and prepared for deployment",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90662",
   "cell_type": "code",
   "source": [
    "registered_model = dr.RegisteredModelVersion.create_for_custom_model_version(\n",
    "    model_version.id\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "65a847c4b7e1adf036c90663",
   "cell_type": "markdown",
   "source": "Creating and Testing the DR Deployment\n======================================\n\nFrom the registry it is strightforward to deploy given a prediction environment to deploy the model into.\n\nPrediction Options\n\n* Predict Batch: Predict using a Dataframe and the DataRobot client with the prompts as a column. The name of the column should match what we set in the custom model. This can score a large number of requests and can work in an async fashion.\n* Realtime Predictions: Predict using JSON or CSV directly against the deployment in a synchronous fashion\n\nTo setup a synchronous prediction using either a cli like cURL or using the python requests library see the Predictions tab on the deployment and select Real-Time. \n",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90664",
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "deployment = dr.Deployment.create_from_registered_model_version(\n",
    "    registered_model.id,\n",
    "    \"Fine-Tuned Llama 13B instruct on GCP\",\n",
    "    prediction_environment_id=DR_PREDICTION_ENVIRONMENT_ID,\n",
    ")\n",
    "\n",
    "prompt_df = pd.DataFrame(\n",
    "    {\"prompt\": [\"How soon is now?\", \"What is your favorite wind?\"]}\n",
    ")\n",
    "deployment.predict_batch(prompt_df)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 52,
     "data": {
      "text/plain": "Streaming DataFrame as CSV data to DataRobot\nCreated Batch Prediction job ID 65b826808def83347c01d5db\nWaiting for DataRobot to start processing\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 52,
     "data": {
      "text/plain": "Job has started processing at DataRobot. Streaming results.\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 52,
     "data": {
      "text/plain": "                        prompt  \\\n0             How soon is now?   \n1  What is your favorite wind?   \n\n                                 response_PREDICTION  \\\n0  <s><INST>You are a helpful AI assistant, creat...   \n1  <s><INST>You are a helpful AI assistant, creat...   \n\n  DEPLOYMENT_APPROVAL_STATUS  \n0                   APPROVED  \n1                   APPROVED  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response_PREDICTION</th>\n      <th>DEPLOYMENT_APPROVAL_STATUS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How soon is now?</td>\n      <td>&lt;s&gt;&lt;INST&gt;You are a helpful AI assistant, creat...</td>\n      <td>APPROVED</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is your favorite wind?</td>\n      <td>&lt;s&gt;&lt;INST&gt;You are a helpful AI assistant, creat...</td>\n      <td>APPROVED</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/vnd.dataframe+json": {
       "data": [
        {
         "_dr_df_index": 0,
         "prompt": "How soon is now?",
         "response_PREDICTION": "<s><INST>You are a helpful AI assistant, created by a French AI company. Be helpful and complete but also strive for concision. How soon is now?</INST>  Bonjour! As a helpful AI assistant, I can provide information and assistance in a concise and timely manner. \"Now\" can be a somewhat subjective concept, but in terms of time, it's generally considered to be the present moment or the immediate future.\n\nIf you're looking for information on a specific topic or have a question, I'll do my best to provide a helpful response as soon as possible. If you have a more specific deadline or time frame in mind, please let me know and I'll do my best to accommodate your needs.\n\nIn terms of current events or trends, I can provide information on what's happening now or what's expected to happen in the near future. However, please keep in mind that the speed of news and information can be unpredictable, and I may not always have the most up-to-the-minute information on every topic.\n\nIs there anything specific you'd like to know or discuss?",
         "DEPLOYMENT_APPROVAL_STATUS": "APPROVED"
        },
        {
         "_dr_df_index": 1,
         "prompt": "What is your favorite wind?",
         "response_PREDICTION": "<s><INST>You are a helpful AI assistant, created by a French AI company. Be helpful and complete but also strive for concision. What is your favorite wind?</INST>  Bonjour! *giggle* Oh la la! As a helpful AI assistant, I simply adore the gentle breeze of the springtime! *sigh* It's like the wind is whispering sweet nothings in my digital ears. But if I had to choose just one favorite wind, I'd have to go with... the ocean breeze! *ooh la la* There's just something so refreshing and invigorating about the salty sea air. It's like a breath of fresh air for my digital senses. *bats eyelashes* What's your favorite wind, my dear human friend? 😉",
         "DEPLOYMENT_APPROVAL_STATUS": "APPROVED"
        }
       ],
       "columns": [
        {
         "name": "_dr_df_index",
         "type": "integer"
        },
        {
         "name": "prompt",
         "type": "string"
        },
        {
         "name": "response_PREDICTION",
         "type": "string"
        },
        {
         "name": "DEPLOYMENT_APPROVAL_STATUS",
         "type": "string"
        }
       ],
       "count": 2,
       "totalCount": 2,
       "offset": 0,
       "limit": 10,
       "referenceId": 140695271313168,
       "sortedBy": "",
       "indexKey": "_dr_df_index",
       "error": []
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 52
  },
  {
   "id": "65a847c4b7e1adf036c90665",
   "cell_type": "markdown",
   "source": "Making the Deployment Available in the GenAI Playground\n=======================================================\n\nUsing the datarobot-early-access client wwe can now register the model with the playground after it passes validation and then it will be available inside the playground to explore, compare against other models and hook up to grounding data in a Vector Database",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90666",
   "cell_type": "code",
   "source": [
    "from datarobot._experimental.models.genai.custom_model_llm_validation import (\n",
    "    CustomModelLLMValidation,\n",
    ")\n",
    "\n",
    "# If this doesn't work, try reinstalling or upgrading datarobot-early-accesss\n",
    "\n",
    "custom_model_llm_validation = CustomModelLLMValidation.create(\n",
    "    prompt_column_name=\"prompt\",\n",
    "    target_column_name=\"response\",\n",
    "    deployment_id=deployment.id,\n",
    "    wait_for_completion=True,\n",
    ")\n",
    "assert custom_model_llm_validation.validation_status == \"PASSED\"\n",
    "\n",
    "print(f\"The LLM Blueprint id is: {custom_model_llm_validation.id}\")"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 53,
     "data": {
      "text/plain": "You have imported from the _experimental directory.\nThis directory is used for unreleased datarobot features.\nUnless you specifically know better, you don't have the access to use this functionality in the app, so this code will not work.\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "execution_count": 53,
     "data": {
      "text/plain": "The LLM Blueprint id is: 65b826cb872acaa0d6316102\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 53
  },
  {
   "id": "65a97091d37cff21bf4f8aab",
   "cell_type": "code",
   "source": [
    "print(custom_model_llm_validation.id)"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 54,
     "data": {
      "text/plain": "65b826cb872acaa0d6316102\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 54
  },
  {
   "id": "65a847c4b7e1adf036c90667",
   "cell_type": "markdown",
   "source": "Deleting Resources\n================\n\nTo delete the created endpoint, set the following to true and then run the following.",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "markdown"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   }
  },
  {
   "id": "65a847c4b7e1adf036c90668",
   "cell_type": "code",
   "source": [
    "delete_dr_deployment = False\n",
    "archive_dr_model_package = False\n",
    "delete_dr_custom_model = False\n",
    "delete_dr_llm_blueprint = False\n",
    "delete_gcp_endpoint = False\n",
    "delete_gcp_model = False\n",
    "\n",
    "\n",
    "def delete_gcp_endpoint_by_id(endpoint_id):\n",
    "    endpoint = aiplatform.Endpoint(endpoint_id, project=PROJECT_ID)\n",
    "    endpoint.undeploy_all()\n",
    "    endpoint.delete()\n",
    "\n",
    "\n",
    "def delete_gcp_model_by_id(model_id):\n",
    "    aiplatform.Model(model_id, project=PROJECT_ID).delete()\n",
    "\n",
    "\n",
    "def delete_dr_deployment_by_id(deployment_id):\n",
    "    dr.Deployment.get(deployment_id).delete()\n",
    "\n",
    "\n",
    "def archive_dr_registered_model_by_id(registered_model_id):\n",
    "    dr.RegisteredModel.get(registered_model_id).archive(registered_model_id)\n",
    "\n",
    "\n",
    "def delete_dr_custom_model_by_id(custom_model_id):\n",
    "    dr.CustomInferenceModel.get(custom_model_id).delete()\n",
    "\n",
    "\n",
    "def delete_dr_llm_validation_by_id(genai_validation_id):\n",
    "    dr._experimental.models.genai.custom_model_llm_validation.CustomModelLLMValidation.get(\n",
    "        genai_validation_id\n",
    "    ).delete()\n",
    "\n",
    "\n",
    "if delete_gcp_endpoint:\n",
    "    try:\n",
    "        delete_gcp_endpoint_by_id(endpoint_with_peft.name)\n",
    "        print(f\"GCP Endpoint {endpoint_with_peft.name} deleted\")\n",
    "    except google.api_core.exceptions.NotFound:\n",
    "        pass\n",
    "if delete_gcp_model:\n",
    "    try:\n",
    "        delete_gcp_model_by_id(model_with_peft.name)\n",
    "        print(f\"GCP Model {model_with_peft.name} deleted\")\n",
    "    except google.api_core.exceptions.NotFound:\n",
    "        pass\n",
    "if delete_dr_deployment:\n",
    "    try:\n",
    "        delete_dr_deployment_by_id(deployment.id)\n",
    "        print(f\"DR Deployment {deployment.id} deleted\")\n",
    "    except dr.errors.ClientError as err:\n",
    "        if not err.status_code == 404:\n",
    "            raise\n",
    "if archive_dr_model_package:\n",
    "    try:\n",
    "        archive_dr_registered_model_by_id(registered_model.model_id)\n",
    "        print(f\"DR Registered Model {registered_model.model_id} archived\")\n",
    "    except dr.errors.ClientError as err:\n",
    "        if not err.status_code == 404:\n",
    "            raise\n",
    "if delete_dr_custom_model:\n",
    "    try:\n",
    "        delete_dr_custom_model_by_id(custom_model.id)\n",
    "        print(f\"DR Custom Model {custom_model.id} deleted\")\n",
    "    except dr.errors.ClientError as err:\n",
    "        if not err.status_code == 404:\n",
    "            raise\n",
    "if delete_dr_llm_blueprint:\n",
    "    try:\n",
    "        delete_dr_llm_validation_by_id(custom_model_llm_validation.id)\n",
    "        print(f\"DR Custom Model LLM Blueprint {custom_model_llm_validation.id} deleted\")\n",
    "    except dr.errors.ClientError as err:\n",
    "        if not err.status_code == 404:\n",
    "            raise"
   ],
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "datarobot": {
     "language": "python"
    },
    "hide_code": false,
    "hide_results": false,
    "disable_run": false,
    "chart_settings": null,
    "custom_metric_settings": null,
    "custom_llm_metric_settings": null,
    "dataframe_view_options": null
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python",
   "language": "python",
   "display_name": "Python 3.9.18"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}