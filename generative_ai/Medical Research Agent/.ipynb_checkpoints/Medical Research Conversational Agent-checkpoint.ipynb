{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d5cc09937002b973c53689",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "# Conversational Agent on Medical Research Papers\n",
    "Medical professionals have to constantly keep abreast of the latest research in the field not only limited to their specialization. With the rate of research and research papers and reports flooding the internet, it becomes tough for them to ramp up on the trusted and approved research papers. Having access to trusted repositories helps them a bit, but there are many sources like Nature, PubMed, Assorted Journals which is still a lot of work. Having a knowledge system that curates trusted papers and then allows fast retrieval with a Question and Answer agent will immensely simplify the medical professionals knowledge initiatives.\n",
    "\n",
    "Another key point to note is that an LLM can halucinate and provide answers to a question, having the knowledge base provides contextual data for the LLM to ground itself and not halucinate. Also, the knowledge base provides the LLM with information that it has not been trained on.\n",
    "\n",
    "This Notebook aims to provide instructions on how to build one such system using DataRobot's Generative AI Solution framework. We show how users can build a pipeline to create a knowledge base with only trusted research papers, and build a conversational agent that can answer questions from medical professionals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d7762",
   "metadata": {
    "chart_settings": null,
    "collapsed": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "# Setup\n",
    "## READ BEFORE STARTING NOTEBOOK\n",
    "1. Enable the following **feature flags** on your account:\n",
    "    - Enable Notebooks Filesystem Management\n",
    "    - Enable Proxy models\n",
    "    - Enable Public Network Access for all Custom Models \n",
    "\t- Enable the Injection of Runtime Parameters for Custom Models\n",
    "    - Enable Monitoring Support for Generative Models (Staging-only)\n",
    "    - Enable Custom Inference Models (GA: on by default)\n",
    "2. Enable the notebook filesystem for this notebook in the notebook sidebar\n",
    "3. Add the notebook environment variable `OPENAI_API_KEY`, `OPENAI_ORGANIZATION`,\n",
    "   `OPENAI_API_BASE` and set the values with your Azure OpenAI credentials\n",
    "4. Set the notebook session timeout to 180 minutes\n",
    "5. Restart the notebook container using at least a \"Medium\" (16GB ram) instance\n",
    "6. Upload your documents archive to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7763",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": null,
    "hide_code": null,
    "hide_results": null,
    "scrolled": null
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    assert 'OPENAI_API_KEY' in os.environ\n",
    "    assert 'OPENAI_ORGANIZATION' in os.environ\n",
    "    assert 'OPENAI_API_BASE' in os.environ\n",
    "    assert os.path.isfile('./storage/files.zip')\n",
    "except Exception as e:\n",
    "    raise RuntimeError('Please follow the setup steps before running the notebook.') from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5ceb2937002b973c5375e",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Installing prerequisite libraries\n",
    "We will be using <a href='https://python.langchain.com/docs/get_started/introduction.html'>Langchain</a> for developing the Agent, and <a href='https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/'>FAISS</a>, <a href='https://www.sbert.net/'>Sentence Transformers</a> for the <a href='https://arxiv.org/abs/2005.11401'>RAG</a> system. The LLM is an OpenAI model hosted on Azure. DataRobot provides the freedom to users to use their preferred components in their stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7764",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting langchain==0.0.244\n",
       "  Downloading langchain-0.0.244-py3-none-any.whl (1.4 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 1.4 MB 16.5 MB/s \n",
       "\u001b[?25hCollecting faiss-cpu==1.7.4\n",
       "  Downloading faiss_cpu-1.7.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 17.6 MB 62.7 MB/s \n",
       "\u001b[?25hCollecting sentence-transformers==2.2.2\n",
       "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 85 kB 7.9 MB/s \n",
       "\u001b[?25hCollecting unstructured==0.8.4\n",
       "  Downloading unstructured-0.8.4-py3-none-any.whl (1.4 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 1.4 MB 59.5 MB/s \n",
       "\u001b[?25hCollecting openai==0.27.8\n",
       "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 73 kB 3.7 MB/s \n",
       "\u001b[?25hCollecting datarobotx==0.1.14\n",
       "  Downloading datarobotx-0.1.14-py3-none-any.whl (171 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 171 kB 65.4 MB/s \n",
       "\u001b[?25hCollecting openapi-schema-pydantic<2.0,>=1.2\n",
       "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 90 kB 12.8 MB/s \n",
       "\u001b[?25h"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Collecting SQLAlchemy<3,>=1.4\n",
       "  Downloading SQLAlchemy-2.0.20-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 3.0 MB 58.4 MB/s \n",
       "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain==0.0.244) (2.31.0)\n",
       "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain==0.0.244) (3.8.5)\n",
       "Requirement already satisfied: pydantic<2,>=1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain==0.0.244) (1.10.8)\n",
       "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain==0.0.244) (4.0.3)\n",
       "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
       "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
       "Collecting numexpr<3.0.0,>=2.8.4\n",
       "  Downloading numexpr-2.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (382 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 382 kB 50.5 MB/s \n",
       "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain==0.0.244) (6.0.1)\n",
       "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain==0.0.244) (8.2.3)\n",
       "Requirement already satisfied: numpy<2,>=1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain==0.0.244) (1.23.3)\n",
       "Collecting langsmith<0.1.0,>=0.0.11\n",
       "  Downloading langsmith-0.0.30-py3-none-any.whl (35 kB)\n",
       "Collecting transformers<5.0.0,>=4.6.0\n",
       "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 7.5 MB 60.3 MB/s \n",
       "\u001b[?25hRequirement already satisfied: tqdm in /etc/system/kernel/.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (4.64.1)\n",
       "Collecting torch>=1.6.0\n",
       "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
       "\r",
       "\u001b[K     |███                             | 57.6 MB 60.5 MB/s eta 0:00:10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |███████████████▎                | 296.4 MB 62.2 MB/s eta 0:00:06"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████    | 542.9 MB 61.4 MB/s eta 0:00:02"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████████| 619.9 MB 77.1 MB/s eta 0:00:01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████████| 619.9 MB 11 kB/s \n",
       "\u001b[?25hCollecting torchvision\n",
       "  Downloading torchvision-0.15.2-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 6.0 MB 60.2 MB/s \n",
       "\u001b[?25hRequirement already satisfied: scikit-learn in /etc/system/kernel/.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.1.2)\n",
       "Requirement already satisfied: scipy in /etc/system/kernel/.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.10.1)\n",
       "Collecting nltk\n",
       "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 1.5 MB 58.2 MB/s \n",
       "\u001b[?25hCollecting sentencepiece\n",
       "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 1.3 MB 56.9 MB/s \n",
       "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
       "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 268 kB 57.7 MB/s \n",
       "\u001b[?25hCollecting chardet\n",
       "  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 199 kB 59.5 MB/s \n",
       "\u001b[?25hCollecting xlrd\n",
       "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 96 kB 9.0 MB/s \n",
       "\u001b[?25hCollecting pypandoc\n",
       "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Collecting python-docx\n",
       "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 5.6 MB 47.2 MB/s \n",
       "\u001b[?25hCollecting python-pptx\n",
       "  Downloading python_pptx-0.6.22-py3-none-any.whl (471 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 471 kB 56.2 MB/s \n",
       "\u001b[?25hCollecting msg-parser\n",
       "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 101 kB 18.2 MB/s \n",
       "\u001b[?25hCollecting filetype\n",
       "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
       "Collecting pdf2image\n",
       "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
       "Collecting markdown\n",
       "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 94 kB 5.0 MB/s \n",
       "\u001b[?25hRequirement already satisfied: pillow in /etc/system/kernel/.venv/lib/python3.9/site-packages (from unstructured==0.8.4) (9.3.0)\n",
       "Collecting python-magic\n",
       "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
       "Requirement already satisfied: tabulate in /etc/system/kernel/.venv/lib/python3.9/site-packages (from unstructured==0.8.4) (0.9.0)\n",
       "Collecting pdfminer.six\n",
       "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 5.6 MB 54.1 MB/s \n",
       "\u001b[?25hRequirement already satisfied: openpyxl in /etc/system/kernel/.venv/lib/python3.9/site-packages (from unstructured==0.8.4) (3.0.10)\n",
       "Requirement already satisfied: pandas in /etc/system/kernel/.venv/lib/python3.9/site-packages (from unstructured==0.8.4) (1.5.1)\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Collecting lxml\n",
       "  Downloading lxml-4.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 7.1 MB 49.6 MB/s \n",
       "\u001b[?25hRequirement already satisfied: urllib3<2.0.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx==0.1.14) (1.26.16)\n",
       "Collecting datarobot-early-access\n",
       "  Downloading datarobot_early_access-3.3.0.2023.8.28-py3-none-any.whl (553 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 553 kB 58.9 MB/s \n",
       "\u001b[?25hRequirement already satisfied: IPython in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx==0.1.14) (8.14.0)\n",
       "Requirement already satisfied: setuptools in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx==0.1.14) (66.0.0)\n",
       "Requirement already satisfied: termcolor in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx==0.1.14) (2.3.0)\n",
       "Collecting names-generator\n",
       "  Downloading names_generator-0.1.0-py3-none-any.whl (26 kB)\n",
       "Requirement already satisfied: altair in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx==0.1.14) (4.2.0)\n",
       "Collecting ipywidgets\n",
       "  Downloading ipywidgets-8.1.0-py3-none-any.whl (139 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 139 kB 63.5 MB/s \n",
       "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.244) (4.7.1)\n",
       "Collecting greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))\n",
       "  Downloading greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 610 kB 53.0 MB/s \n",
       "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.244) (3.2.0)\n",
       "Requirement already satisfied: idna<4,>=2.5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.244) (3.4)\n",
       "Requirement already satisfied: certifi>=2017.4.17 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.244) (2023.7.22)\n",
       "Requirement already satisfied: frozenlist>=1.1.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.244) (1.4.0)\n",
       "Requirement already satisfied: aiosignal>=1.1.2 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.244) (1.3.1)\n",
       "Requirement already satisfied: multidict<7.0,>=4.5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.244) (6.0.4)\n",
       "Requirement already satisfied: attrs>=17.3.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.244) (23.1.0)\n",
       "Requirement already satisfied: yarl<2.0,>=1.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.244) (1.9.2)\n",
       "Collecting typing-inspect<1,>=0.4.0\n",
       "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
       "Collecting marshmallow<4.0.0,>=3.18.0\n",
       "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 49 kB 9.2 MB/s \n",
       "\u001b[?25h"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
       "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 7.8 MB 54.6 MB/s \n",
       "\u001b[?25hCollecting safetensors>=0.3.1\n",
       "  Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 1.3 MB 56.6 MB/s \n",
       "\u001b[?25hCollecting regex!=2019.12.17\n",
       "  Downloading regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 771 kB 54.7 MB/s \n",
       "\u001b[?25hRequirement already satisfied: filelock in /etc/system/kernel/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (3.12.3)\n",
       "Requirement already satisfied: packaging>=20.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (23.1)\n",
       "Requirement already satisfied: networkx in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1)\n",
       "Collecting sympy\n",
       "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 5.7 MB 53.7 MB/s \n",
       "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 849 kB 55.8 MB/s \n",
       "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
       "\r",
       "\u001b[K     |▉                               | 2.5 MB 48.2 MB/s eta 0:00:03"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████████| 102.6 MB 74.9 MB/s \n",
       "\u001b[?25hCollecting triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 63.3 MB 247 kB/s \n",
       "\u001b[?25h"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Collecting nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████▏       | 239.0 MB 60.6 MB/s eta 0:00:02"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████████| 317.1 MB 32 kB/s \n",
       "\u001b[?25h"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Collecting nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
       "\r",
       "\u001b[K     |██████████████▍                 | 250.8 MB 77.7 MB/s eta 0:00:04"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████▋   | 498.2 MB 64.9 MB/s eta 0:00:01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████████| 557.1 MB 64.9 MB/s eta 0:00:01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████████| 557.1 MB 8.5 kB/s \n",
       "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
       "\r",
       "\u001b[K     |██████████████████████████████▋ | 169.4 MB 58.1 MB/s eta 0:00:01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████████| 177.1 MB 149 kB/s \n",
       "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 11.8 MB 60.4 MB/s \n",
       "\u001b[?25hRequirement already satisfied: jinja2 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\n",
       "Collecting nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
       "\r",
       "\u001b[K     |█████████████████████████▎      | 136.8 MB 76.8 MB/s eta 0:00:01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████████| 173.2 MB 53 kB/s \n",
       "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 98 kB 13.0 MB/s \n",
       "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
       "\r",
       "\u001b[K     |█████████████████████▋          | 113.8 MB 70.0 MB/s eta 0:00:01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "\u001b[K     |████████████████████████████████| 168.4 MB 143 kB/s \n",
       "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 54.6 MB 381 kB/s \n",
       "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
       "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 21.0 MB 52.8 MB/s \n",
       "\u001b[?25h"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: threadpoolctl>=2.0.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.2.0)\n",
       "Requirement already satisfied: joblib>=1.0.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers==2.2.2) (1.2.0)\n",
       "Requirement already satisfied: click in /etc/system/kernel/.venv/lib/python3.9/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
       "Requirement already satisfied: fsspec in /etc/system/kernel/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
       "Collecting XlsxWriter>=0.5.7\n",
       "  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 153 kB 59.4 MB/s \n",
       "\u001b[?25hCollecting olefile>=0.46\n",
       "  Downloading olefile-0.46.zip (112 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 112 kB 61.6 MB/s \n",
       "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from markdown->unstructured==0.8.4) (6.8.0)\n",
       "Requirement already satisfied: cryptography>=36.0.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from pdfminer.six->unstructured==0.8.4) (41.0.3)\n",
       "Requirement already satisfied: et-xmlfile in /etc/system/kernel/.venv/lib/python3.9/site-packages (from openpyxl->unstructured==0.8.4) (1.1.0)\n",
       "Requirement already satisfied: pytz>=2020.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from pandas->unstructured==0.8.4) (2023.3)\n",
       "Requirement already satisfied: python-dateutil>=2.8.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from pandas->unstructured==0.8.4) (2.8.2)\n",
       "Requirement already satisfied: mypy-extensions<2,>=0.4.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobot-early-access->datarobotx==0.1.14) (1.0.0)\n",
       "Requirement already satisfied: requests-toolbelt>=0.6 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobot-early-access->datarobotx==0.1.14) (1.0.0)\n",
       "Requirement already satisfied: trafaret!=1.1.0,<2.2,>=0.7 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobot-early-access->datarobotx==0.1.14) (2.1.1)\n",
       "Requirement already satisfied: jedi>=0.16 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (0.19.0)\n",
       "Requirement already satisfied: stack-data in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (0.6.2)\n",
       "Requirement already satisfied: matplotlib-inline in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (0.1.6)\n",
       "Requirement already satisfied: pygments>=2.4.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (2.16.1)\n",
       "Requirement already satisfied: traitlets>=5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (5.9.0)\n",
       "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (3.0.39)\n",
       "Requirement already satisfied: backcall in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (0.2.0)\n",
       "Requirement already satisfied: decorator in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (5.1.1)\n",
       "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (4.8.0)\n",
       "Requirement already satisfied: pickleshare in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx==0.1.14) (0.7.5)\n",
       "Collecting cmdkit>=2.1.2\n",
       "  Downloading cmdkit-2.6.1-py3-none-any.whl (28 kB)\n",
       "Requirement already satisfied: toolz in /etc/system/kernel/.venv/lib/python3.9/site-packages (from altair->datarobotx==0.1.14) (0.12.0)\n",
       "Requirement already satisfied: jsonschema>=3.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from altair->datarobotx==0.1.14) (4.19.0)\n",
       "Requirement already satisfied: entrypoints in /etc/system/kernel/.venv/lib/python3.9/site-packages (from altair->datarobotx==0.1.14) (0.4)\n",
       "Collecting widgetsnbextension~=4.0.7\n",
       "  Downloading widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 2.3 MB 58.3 MB/s \n",
       "\u001b[?25hCollecting jupyterlab-widgets~=3.0.7\n",
       "  Downloading jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 214 kB 64.6 MB/s \n",
       "\u001b[?25hRequirement already satisfied: comm>=0.1.3 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from ipywidgets->datarobotx==0.1.14) (0.1.4)\n",
       "Collecting mpmath>=0.19\n",
       "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 536 kB 59.4 MB/s \n",
       "\u001b[?25hCollecting wheel\n",
       "  Downloading wheel-0.41.2-py3-none-any.whl (64 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 64 kB 5.6 MB/s \n",
       "\u001b[?25h"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Collecting cmake\n",
       "  Downloading cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 26.1 MB 53.3 MB/s \n",
       "\u001b[?25hCollecting lit\n",
       "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 153 kB 63.9 MB/s \n",
       "\u001b[?25h  Installing build dependencies ... \u001b[?25l-"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\b \b\\\b \b|\b \bdone\n",
       "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n",
       "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\b \b\\\b \bdone\n",
       "\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\b \bdone\n",
       "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.2)\n",
       "Requirement already satisfied: zipp>=0.5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown->unstructured==0.8.4) (3.16.2)\n",
       "Requirement already satisfied: cffi>=1.12 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured==0.8.4) (1.15.1)\n",
       "Requirement already satisfied: six>=1.5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->unstructured==0.8.4) (1.16.0)\n",
       "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jedi>=0.16->IPython->datarobotx==0.1.14) (0.8.3)\n",
       "Requirement already satisfied: asttokens>=2.1.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from stack-data->IPython->datarobotx==0.1.14) (2.2.1)\n",
       "Requirement already satisfied: executing>=1.2.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from stack-data->IPython->datarobotx==0.1.14) (1.2.0)\n",
       "Requirement already satisfied: pure-eval in /etc/system/kernel/.venv/lib/python3.9/site-packages (from stack-data->IPython->datarobotx==0.1.14) (0.2.2)\n",
       "Requirement already satisfied: wcwidth in /etc/system/kernel/.venv/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->datarobotx==0.1.14) (0.2.6)\n",
       "Requirement already satisfied: ptyprocess>=0.5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from pexpect>4.3; sys_platform != \"win32\"->IPython->datarobotx==0.1.14) (0.7.0)\n",
       "Requirement already satisfied: referencing>=0.28.4 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair->datarobotx==0.1.14) (0.30.2)\n",
       "Requirement already satisfied: rpds-py>=0.7.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair->datarobotx==0.1.14) (0.10.0)\n",
       "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair->datarobotx==0.1.14) (2023.7.1)\n",
       "Requirement already satisfied: pycparser in /etc/system/kernel/.venv/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured==0.8.4) (2.21)\n",
       "Using legacy 'setup.py install' for sentence-transformers, since package 'wheel' is not installed.\n",
       "Using legacy 'setup.py install' for python-docx, since package 'wheel' is not installed.\n",
       "Using legacy 'setup.py install' for olefile, since package 'wheel' is not installed.\n",
       "Building wheels for collected packages: lit\n",
       "  Building wheel for lit (PEP 517) ... \u001b[?25l-\b \b\\\b \bdone\n",
       "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=d9635c06392ff6ac75d069497a332c6a7d61a8b336f70b7bf9322dac4170d7ac\n",
       "  Stored in directory: /home/notebooks/.cache/pip/wheels/a5/36/d6/cac2e6fb891889b33a548f2fddb8b4b7726399aaa2ed32b188\n",
       "Successfully built lit\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Installing collected packages: openapi-schema-pydantic, greenlet, SQLAlchemy, typing-inspect, marshmallow, dataclasses-json, numexpr, langsmith, langchain, faiss-cpu, tokenizers, safetensors, huggingface-hub, regex, transformers, mpmath, sympy, wheel, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, cmake, lit, triton, nvidia-cudnn-cu11, nvidia-nccl-cu11, nvidia-cuda-cupti-cu11, nvidia-cusparse-cu11, nvidia-nvtx-cu11, nvidia-cufft-cu11, nvidia-curand-cu11, nvidia-cuda-nvrtc-cu11, torch, torchvision, nltk, sentencepiece, sentence-transformers, chardet, xlrd, pypandoc, lxml, python-docx, XlsxWriter, python-pptx, olefile, msg-parser, filetype, pdf2image, markdown, python-magic, pdfminer.six, unstructured, openai, datarobot-early-access, cmdkit, names-generator, widgetsnbextension, jupyterlab-widgets, ipywidgets, datarobotx\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "    Running setup.py install for sentence-transformers ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
       "\u001b[?25h    Running setup.py install for python-docx ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
       "\u001b[?25h    Running setup.py install for olefile ... \u001b[?25l"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-\b \b\\\b \b|\b \bdone\n",
       "\u001b[?25hSuccessfully installed SQLAlchemy-2.0.20 XlsxWriter-3.1.2 chardet-5.2.0 cmake-3.27.2 cmdkit-2.6.1 dataclasses-json-0.5.14 datarobot-early-access-3.3.0.2023.8.28 datarobotx-0.1.14 faiss-cpu-1.7.4 filetype-1.2.0 greenlet-2.0.2 huggingface-hub-0.16.4 ipywidgets-8.1.0 jupyterlab-widgets-3.0.8 langchain-0.0.244 langsmith-0.0.30 lit-16.0.6 lxml-4.9.3 markdown-3.4.4 marshmallow-3.20.1 mpmath-1.3.0 msg-parser-1.2.0 names-generator-0.1.0 nltk-3.8.1 numexpr-2.8.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 olefile-0.46 openai-0.27.8 openapi-schema-pydantic-1.2.4 pdf2image-1.16.3 pdfminer.six-20221105 pypandoc-1.11 python-docx-0.8.11 python-magic-0.4.27 python-pptx-0.6.22 regex-2023.8.8 safetensors-0.3.3 sentence-transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 torchvision-0.15.2 transformers-4.32.1 triton-2.0.0 typing-inspect-0.9.0 unstructured-0.8.4 wheel-0.41.2 widgetsnbextension-4.0.8 xlrd-2.0.1\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install \"langchain==0.0.244\" \\\n",
    "             \"faiss-cpu==1.7.4\" \\\n",
    "             \"sentence-transformers==2.2.2\" \\\n",
    "             \"unstructured==0.8.4\" \\\n",
    "             \"openai==0.27.8\" \\\n",
    "             \"datarobotx==0.1.14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edd147020c36f5a06ccf37",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": true,
    "name": "Installing DataRobotX",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: datarobotx[llm] in /etc/system/kernel/.venv/lib/python3.9/site-packages (0.1.14)\n",
       "Collecting json2html\n",
       "  Downloading json2html-1.3.0.tar.gz (7.0 kB)\n",
       "Requirement already satisfied: names-generator in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (0.1.0)\n",
       "Requirement already satisfied: pandas in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (1.5.1)\n",
       "Requirement already satisfied: aiohttp in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (3.8.5)\n",
       "Requirement already satisfied: urllib3<2.0.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (1.26.16)\n",
       "Requirement already satisfied: PyYaml in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (6.0.1)\n",
       "Requirement already satisfied: setuptools in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (66.0.0)\n",
       "Requirement already satisfied: IPython in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (8.14.0)\n",
       "Requirement already satisfied: termcolor in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (2.3.0)\n",
       "Requirement already satisfied: altair in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (4.2.0)\n",
       "Requirement already satisfied: tqdm in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (4.64.1)\n",
       "Requirement already satisfied: tenacity in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (8.2.3)\n",
       "Requirement already satisfied: datarobot-early-access in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (3.3.0.2023.8.28)\n",
       "Requirement already satisfied: ipywidgets in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (8.1.0)\n",
       "Requirement already satisfied: openai; extra == \"llm\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (0.27.8)\n",
       "Requirement already satisfied: langchain; extra == \"llm\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (0.0.244)\n",
       "Requirement already satisfied: torch; extra == \"llm\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobotx[llm]) (2.0.1)\n",
       "Collecting tiktoken; extra == \"llm\"\n",
       "  Downloading tiktoken-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
       "\r",
       "\u001b[K     |████████████████████████████████| 1.7 MB 18.3 MB/s \n",
       "\u001b[?25hRequirement already satisfied: cmdkit>=2.1.2 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from names-generator->datarobotx[llm]) (2.6.1)\n",
       "Requirement already satisfied: python-dateutil>=2.8.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from pandas->datarobotx[llm]) (2.8.2)\n",
       "Requirement already satisfied: pytz>=2020.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from pandas->datarobotx[llm]) (2023.3)\n",
       "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from pandas->datarobotx[llm]) (1.23.3)\n",
       "Requirement already satisfied: attrs>=17.3.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp->datarobotx[llm]) (23.1.0)\n",
       "Requirement already satisfied: multidict<7.0,>=4.5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp->datarobotx[llm]) (6.0.4)\n",
       "Requirement already satisfied: frozenlist>=1.1.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp->datarobotx[llm]) (1.4.0)\n",
       "Requirement already satisfied: aiosignal>=1.1.2 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp->datarobotx[llm]) (1.3.1)\n",
       "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp->datarobotx[llm]) (4.0.3)\n",
       "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp->datarobotx[llm]) (3.2.0)\n",
       "Requirement already satisfied: yarl<2.0,>=1.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from aiohttp->datarobotx[llm]) (1.9.2)\n",
       "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (4.7.1)\n",
       "Requirement already satisfied: pygments>=2.4.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (2.16.1)\n",
       "Requirement already satisfied: jedi>=0.16 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (0.19.0)\n",
       "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (3.0.39)\n",
       "Requirement already satisfied: decorator in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (5.1.1)\n",
       "Requirement already satisfied: pickleshare in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (0.7.5)\n",
       "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (4.8.0)\n",
       "Requirement already satisfied: backcall in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (0.2.0)\n",
       "Requirement already satisfied: traitlets>=5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (5.9.0)\n",
       "Requirement already satisfied: stack-data in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (0.6.2)\n",
       "Requirement already satisfied: matplotlib-inline in /etc/system/kernel/.venv/lib/python3.9/site-packages (from IPython->datarobotx[llm]) (0.1.6)\n",
       "Requirement already satisfied: toolz in /etc/system/kernel/.venv/lib/python3.9/site-packages (from altair->datarobotx[llm]) (0.12.0)\n",
       "Requirement already satisfied: jinja2 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from altair->datarobotx[llm]) (3.1.2)\n",
       "Requirement already satisfied: jsonschema>=3.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from altair->datarobotx[llm]) (4.19.0)\n",
       "Requirement already satisfied: entrypoints in /etc/system/kernel/.venv/lib/python3.9/site-packages (from altair->datarobotx[llm]) (0.4)\n",
       "Requirement already satisfied: trafaret!=1.1.0,<2.2,>=0.7 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobot-early-access->datarobotx[llm]) (2.1.1)\n",
       "Requirement already satisfied: mypy-extensions<2,>=0.4.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobot-early-access->datarobotx[llm]) (1.0.0)\n",
       "Requirement already satisfied: requests>=2.28.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobot-early-access->datarobotx[llm]) (2.31.0)\n",
       "Requirement already satisfied: requests-toolbelt>=0.6 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from datarobot-early-access->datarobotx[llm]) (1.0.0)\n",
       "Requirement already satisfied: widgetsnbextension~=4.0.7 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from ipywidgets->datarobotx[llm]) (4.0.8)\n",
       "Requirement already satisfied: comm>=0.1.3 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from ipywidgets->datarobotx[llm]) (0.1.4)\n",
       "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from ipywidgets->datarobotx[llm]) (3.0.8)\n",
       "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain; extra == \"llm\"->datarobotx[llm]) (0.0.30)\n",
       "Requirement already satisfied: pydantic<2,>=1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain; extra == \"llm\"->datarobotx[llm]) (1.10.8)\n",
       "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain; extra == \"llm\"->datarobotx[llm]) (0.5.14)\n",
       "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain; extra == \"llm\"->datarobotx[llm]) (2.0.20)\n",
       "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain; extra == \"llm\"->datarobotx[llm]) (1.2.4)\n",
       "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from langchain; extra == \"llm\"->datarobotx[llm]) (2.8.5)\n",
       "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (11.7.91)\n",
       "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (11.7.101)\n",
       "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (11.4.0.1)\n",
       "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (2.0.0)\n",
       "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (11.7.99)\n",
       "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (11.7.4.91)\n",
       "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (11.10.3.66)\n",
       "Requirement already satisfied: networkx in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (3.1)\n",
       "Requirement already satisfied: filelock in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (3.12.3)\n",
       "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (11.7.99)\n",
       "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (2.14.3)\n",
       "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (10.9.0.58)\n",
       "Requirement already satisfied: sympy in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (1.12)\n",
       "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (10.2.10.91)\n",
       "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /etc/system/kernel/.venv/lib/python3.9/site-packages (from torch; extra == \"llm\"->datarobotx[llm]) (8.5.0.96)\n",
       "Requirement already satisfied: regex>=2022.1.18 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from tiktoken; extra == \"llm\"->datarobotx[llm]) (2023.8.8)\n",
       "Requirement already satisfied: six>=1.5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datarobotx[llm]) (1.16.0)\n",
       "Requirement already satisfied: idna>=2.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp->datarobotx[llm]) (3.4)\n",
       "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jedi>=0.16->IPython->datarobotx[llm]) (0.8.3)\n",
       "Requirement already satisfied: wcwidth in /etc/system/kernel/.venv/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->datarobotx[llm]) (0.2.6)\n",
       "Requirement already satisfied: ptyprocess>=0.5 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from pexpect>4.3; sys_platform != \"win32\"->IPython->datarobotx[llm]) (0.7.0)\n",
       "Requirement already satisfied: pure-eval in /etc/system/kernel/.venv/lib/python3.9/site-packages (from stack-data->IPython->datarobotx[llm]) (0.2.2)\n",
       "Requirement already satisfied: asttokens>=2.1.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from stack-data->IPython->datarobotx[llm]) (2.2.1)\n",
       "Requirement already satisfied: executing>=1.2.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from stack-data->IPython->datarobotx[llm]) (1.2.0)\n",
       "Requirement already satisfied: MarkupSafe>=2.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jinja2->altair->datarobotx[llm]) (2.1.2)\n",
       "Requirement already satisfied: rpds-py>=0.7.1 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair->datarobotx[llm]) (0.10.0)\n",
       "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair->datarobotx[llm]) (2023.7.1)\n",
       "Requirement already satisfied: referencing>=0.28.4 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair->datarobotx[llm]) (0.30.2)\n",
       "Requirement already satisfied: certifi>=2017.4.17 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from requests>=2.28.1->datarobot-early-access->datarobotx[llm]) (2023.7.22)\n",
       "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain; extra == \"llm\"->datarobotx[llm]) (3.20.1)\n",
       "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain; extra == \"llm\"->datarobotx[llm]) (0.9.0)\n",
       "Requirement already satisfied: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /etc/system/kernel/.venv/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain; extra == \"llm\"->datarobotx[llm]) (2.0.2)\n",
       "Requirement already satisfied: wheel in /etc/system/kernel/.venv/lib/python3.9/site-packages (from nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch; extra == \"llm\"->datarobotx[llm]) (0.41.2)\n",
       "Requirement already satisfied: cmake in /etc/system/kernel/.venv/lib/python3.9/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch; extra == \"llm\"->datarobotx[llm]) (3.27.2)\n",
       "Requirement already satisfied: lit in /etc/system/kernel/.venv/lib/python3.9/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch; extra == \"llm\"->datarobotx[llm]) (16.0.6)\n",
       "Requirement already satisfied: mpmath>=0.19 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from sympy->torch; extra == \"llm\"->datarobotx[llm]) (1.3.0)\n",
       "Requirement already satisfied: packaging>=17.0 in /etc/system/kernel/.venv/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain; extra == \"llm\"->datarobotx[llm]) (23.1)\n",
       "Building wheels for collected packages: json2html\n",
       "  Building wheel for json2html (setup.py) ... \u001b[?25l"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-\b \b\\\b \b|\b \bdone\n",
       "\u001b[?25h  Created wheel for json2html: filename=json2html-1.3.0-py3-none-any.whl size=7591 sha256=3bff1b7c8dcde6f8bcc55292e25d6a7a33fe9366ced3ae91a7e446a212ef21b1\n",
       "  Stored in directory: /home/notebooks/.cache/pip/wheels/b9/56/a2/f610a5e8a635d74d27c9971d6099b2521d2155169ff2d99b89\n",
       "Successfully built json2html\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Installing collected packages: json2html, tiktoken\n",
       "Successfully installed json2html-1.3.0 tiktoken-0.4.0\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datarobotx[llm] json2html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d148937002b973c5382b",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Document Corpus\n",
    "Below is the corpus of both trusted and non-trusted medical research abstracts. These will simulate the real world documents that need to be processed and added to the Agent's knowledge base. This dataset is sourced from <a href='https://www.kaggle.com/datasets/anshulmehtakaggl/200000-abstracts-for-seq-sentence-classification'>Kaggle</a>. For this demo we will be using a subset of the papers to help readers run the notebook quickly. Please find the files.zip file <a href='https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/medical_agent/files.zip'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7765",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": null,
    "hide_code": null,
    "hide_results": null,
    "scrolled": null
   },
   "outputs": [],
   "source": [
    "# Decompress the documents\n",
    "#!tar -xf ./storage/dr_docs.tar -C ./storage/\n",
    "import shutil\n",
    "shutil.unpack_archive('/home/notebooks/storage/files.zip', '/home/notebooks/storage/', 'zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7766",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir('/home/notebooks/storage/files/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d27c713ebddce5cc2bcd",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Trusted Research Papers\n",
    "As our aim is to only include trusted papers into the knowledge base, we will define a function to check if the paper can be trusted or not. In this demo, we are building a DataRobot AutoML predictive model to predict if the research paper trust level is high or not. Using <a href='https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/'>DataRobot</a> and <a href='https://drx.datarobot.com/model/automl.html'>DataRobotX</a> APIs it is easy to build and deploy this model. Please find the dataset medical_papers_trust_scoring.csv <a href='https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/medical_agent/medical_papers_trust_scoring.csv'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edd1861eb4beb24a041dc7",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Waiting for autopilot to complete...\u001b[0m\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Creating project\u001b[0m\n",
       "\u001b[1m  - \u001b[0mUploading project dataset...\n",
       "\r",
       "    100%|██████████████████████████████████| 2.98M/2.98M [00:00<00:00, 45.7MB/s]\n",
       "\u001b[1m  - \u001b[0mAwaiting project initialization...\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mCreated project\n",
       "    [hungry-pare](https://app.datarobot.com/projects/64f0231eced0f1b02a290ca0/eda)\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Running autopilot\u001b[0m\n",
       "\u001b[1m  - \u001b[0mAwaiting autopilot initialization...\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mFitting models...\n",
       "\r",
       "    |                                  |0 completed [00:03, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:06, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:09, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:12, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:15, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:18, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:21, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:24, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:27, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:30, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:33, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:36, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:39, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:42, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:45, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:48, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:51, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |                                  |0 completed [00:54, Fitting=4, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████▌                         |1 completed [00:57, Fitting=3, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████▌                         |1 completed [01:00, Fitting=3, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████████████████▍             |3 completed [01:03, Fitting=2, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████████████████▌        |3 completed [01:06, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████████████████▌        |3 completed [01:09, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████████████████▌        |3 completed [01:12, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████████████████▌        |3 completed [01:15, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████████████████▌        |3 completed [01:18, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████████████████▌        |3 completed [01:21, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████████████████▌        |3 completed [01:24, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████████████████▌        |3 completed [01:27, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [01:30, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [01:33, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [01:36, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [01:39, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [01:42, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [01:45, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [01:48, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [01:51, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [01:54, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [01:57, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [02:00, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [02:03, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [02:06, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [02:09, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [02:12, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [02:15, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [02:18, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [02:21, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████▊                           |4 completed [02:24, Fitting=8, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████▏                          |4 completed [02:27, Fitting=7, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████▏                          |4 completed [02:30, Fitting=7, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████▌                          |4 completed [02:33, Fitting=6, Queued=8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████                          |4 completed [02:36, Fitting=6, Queued=7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████                          |4 completed [02:39, Fitting=8, Queued=5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████                          |4 completed [02:42, Fitting=8, Queued=5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████                          |4 completed [02:45, Fitting=8, Queued=5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████                          |4 completed [02:48, Fitting=8, Queued=5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████                          |4 completed [02:51, Fitting=8, Queued=5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████▌                         |4 completed [02:54, Fitting=7, Queued=5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████▌                         |4 completed [02:57, Fitting=8, Queued=4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████▍                       |4 completed [03:00, Fitting=5, Queued=4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████▍                       |4 completed [03:03, Fitting=5, Queued=4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████▍                       |4 completed [03:06, Fitting=5, Queued=4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████▍                       |4 completed [03:09, Fitting=8, Queued=1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████▍                       |4 completed [03:12, Fitting=8, Queued=1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████▍                       |4 completed [03:15, Fitting=8, Queued=1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████▎                      |4 completed [03:18, Fitting=7, Queued=1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████████▎                     |4 completed [03:21, Fitting=6, Queued=1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |████████████▎                     |4 completed [03:24, Fitting=7, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████▌                    |4 completed [03:28, Fitting=6, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████▌                    |4 completed [03:30, Fitting=6, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████▌                    |4 completed [03:33, Fitting=6, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |█████████████▌                    |4 completed [03:36, Fitting=6, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████                   |4 completed [03:39, Fitting=5, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████                   |4 completed [03:42, Fitting=5, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████                   |4 completed [03:45, Fitting=5, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████                   |4 completed [03:48, Fitting=5, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████▍              |4 completed [03:51, Fitting=3, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████▍              |4 completed [03:54, Fitting=3, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████▍              |4 completed [03:57, Fitting=3, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████▍              |4 completed [04:00, Fitting=3, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████▍              |4 completed [04:03, Fitting=3, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████▋           |4 completed [04:06, Fitting=2, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:09, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:12, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:15, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:19, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:21, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:24, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:27, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:30, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:33, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:36, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [04:40, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [04:43, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [04:46, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [04:49, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [04:52, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [04:55, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [04:58, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:01, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:04, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:07, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:10, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:13, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:16, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:19, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:22, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:25, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:28, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:31, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:34, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:37, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:40, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|4 completed [05:43, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [05:46, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [05:49, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [05:52, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [05:55, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [05:58, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [06:01, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [06:04, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [06:07, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [06:10, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [06:13, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [06:16, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [06:19, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [06:22, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |███████████████████████████▏      |4 completed [06:25, Fitting=1, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:28, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:31, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:34, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:37, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:40, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:43, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:46, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:49, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:52, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:55, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [06:58, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [07:01, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [07:04, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [07:07, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [07:10, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [07:14, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [07:16, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [07:19, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [07:22, Fitting=0, Queued=0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "    |██████████████████████████████████|5 completed [07:24, Fitting=0, Queued=0]\n",
       "     ,,,,,,,,,\n",
       "   #-#       #-#\n",
       "   # #   \u001b[33m*\u001b[0m   # #    CHAMPION\n",
       "    # #     # #     Elastic-Net Classifier with Naive Bayes Feature Weighting (L2)\n",
       "      #*# #*#\n",
       "        /,\\\n",
       "       `````\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Autopilot complete\u001b[0m\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Creating deployment\u001b[0m\n",
       "\u001b[1m  - \u001b[0mCalculating feature impact...\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datarobotx as drx\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize Client if running this notebook out of DataRobot platform\n",
    "#drx.Client()\n",
    "\n",
    "df = pd.read_csv('storage/medical_papers_trust_scoring.csv')\n",
    "df_train, df_test = train_test_split(df, test_size=0.4, random_state=42)\n",
    "model = drx.AutoMLModel()\n",
    "model.fit(df_train, target='trust')\n",
    "deployment = model.deploy(wait_for_autopilot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edeeab1eb4beb24a042711",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Waiting for deployment to be initialized...\u001b[0m\n",
       "\u001b[1m  - \u001b[0mInitializing model for prediction explanations...\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mAwaiting deployment creation...\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Making predictions\u001b[0m\n",
       "\u001b[1m  - \u001b[0mMaking predictions with deployment\n",
       "    [confident-agnesi](https://app.datarobot.com/deployments/64f02556e90d38197cd2a5a2/overview)\n",
       "\u001b[1m  - \u001b[0mUploading dataset to be scored...\n",
       "\r",
       "    100%|██████████████████████████████████| 2.00M/2.00M [00:00<00:00, 34.2MB/s]\n",
       "\u001b[1m  - \u001b[0mScoring...\n",
       "\u001b[1m  - \u001b[0mCreated deployment\n",
       "    [confident-agnesi](https://app.datarobot.com/deployments/64f02556e90d38197cd2a5a2/overview)\n",
       "    from model [Elastic-Net Classifier with Naive Bayes Feature Weighting\n",
       "    (L2)](https://app.datarobot.com/projects/64f0231eced0f1b02a290ca0/models/64f024d35dfcafa3b95be4ae/blueprint)\n",
       "    in project\n",
       "    [hungry-pare](https://app.datarobot.com/projects/64f0231eced0f1b02a290ca0/eda)\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Deployment complete\u001b[0m\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Predictions complete\u001b[0m\n",
       "<class 'datarobotx.common.utils.FutureDataFrame'>\n",
       "RangeIndex: 1000 entries, 0 to 999\n",
       "Data columns (total 1 columns):\n",
       " #   Column      Non-Null Count  Dtype \n",
       "---  ------      --------------  ----- \n",
       " 0   prediction  1000 non-null   object\n",
       "dtypes: object(1)\n",
       "memory usage: 7.9+ KB\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = deployment.predict(df_test)\n",
    "df_test['predictions'] = predictions.prediction.values\n",
    "predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7767",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Making predictions\u001b[0m\n",
       "\u001b[1m  - \u001b[0mMaking predictions with deployment\n",
       "    [confident-agnesi](https://app.datarobot.com/deployments/64f02556e90d38197cd2a5a2/overview)\n",
       "\u001b[1m  - \u001b[0mUploading dataset to be scored...\n",
       "\r",
       "    100%|███████████████████████████████████| 1.81k/1.81k [00:00<00:00, 103kB/s]\n",
       "\u001b[1m  - \u001b[0mScoring...\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Predictions complete\u001b[0m\n",
       "Trust level for paper # 24219891  low\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Making predictions\u001b[0m\n",
       "\u001b[1m  - \u001b[0mMaking predictions with deployment\n",
       "    [confident-agnesi](https://app.datarobot.com/deployments/64f02556e90d38197cd2a5a2/overview)\n",
       "\u001b[1m  - \u001b[0mUploading dataset to be scored...\n",
       "\r",
       "    100%|███████████████████████████████████| 1.89k/1.89k [00:00<00:00, 115kB/s]\n",
       "\u001b[1m  - \u001b[0mScoring...\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Predictions complete\u001b[0m\n",
       "Trust level for paper # 24229754  high\n",
       "CPU times: user 105 ms, sys: 13.5 ms, total: 118 ms\n",
       "Wall time: 4.01 s\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def get_paper_trust_level(file_path):\n",
    "    file_paper = open(file_path,\"r+\")\n",
    "    paper_content = file_paper.read()\n",
    "    file_paper.close()\n",
    "    pred = deployment.predict(pd.DataFrame({'abstract':[paper_content]}), wait_for_autopilot=True)\n",
    "    return pred['prediction'].iloc[0]\n",
    "\n",
    "print(\"Trust level for paper # 24219891 \", get_paper_trust_level('/home/notebooks/storage/files/24219891.txt'))\n",
    "print(\"Trust level for paper # 24229754 \", get_paper_trust_level('/home/notebooks/storage/files/24229754.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d7768",
   "metadata": {
    "chart_settings": null,
    "collapsed": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "# Load and Split Text\n",
    "\n",
    "If applying this recipe to a different use case, consider:\n",
    "\n",
    "- Using additional or alternative document loaders\n",
    "- Filtering out extraneous or noisy documents\n",
    "- Choosing an appropriate `chunk_size` and `overlap`. These are counted by number of characters, NOT tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7769",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Load Documents",
    "scrolled": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading /home/notebooks/storage/files/ directory\n",
       "[nltk_data] Downloading package punkt to /home/notebooks/nltk_data...\n",
       "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
       "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
       "[nltk_data]     /home/notebooks/nltk_data...\n",
       "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Splitting 2500 documents\n",
       "Created 3474 documents\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "SOURCE_DOCUMENTS_DIR = \"/home/notebooks/storage/files/\"\n",
    "SOURCE_DOCUMENTS_FILTER = \"*.txt\"\n",
    "\n",
    "loader = DirectoryLoader(f\"{SOURCE_DOCUMENTS_DIR}\", glob=SOURCE_DOCUMENTS_FILTER)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=1000,\n",
    ")\n",
    "\n",
    "print(f\"Loading {SOURCE_DOCUMENTS_DIR} directory\")\n",
    "data = loader.load()\n",
    "print(f\"Splitting {len(data)} documents\")\n",
    "docs = splitter.split_documents(data)\n",
    "print(f\"Created {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d336907ce3b7662d7d7b",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Filtration\n",
    "Filtering only trusted papers to be loaded to the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d776a",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\r",
       "100%|██████████| 3474/3474 [00:01<00:00, 2204.90it/s]\n",
       "254"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "approved_docs = []\n",
    "for i in tqdm(range(len(docs))):\n",
    "    if((docs[i].metadata['source'].split('/')[-1] in df_test[df_test.predictions=='high']['filename'].tolist())):\n",
    "        approved_docs.append(docs[i])\n",
    "len(approved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d776b",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"24432712 BACKGROUND\\tThe EXAcerbations of Chronic Pulmonary Disease Tool ( EXACT ) is a patient-reported outcome measure to standardize the symptomatic assessment of chronic obstructive pulmonary disease exacerbations , including reported and unreported events . BACKGROUND\\tThe instrument has been validated in a short-term study of patients with acute exacerbation and stable disease ; its performance in longer-term studies has not been assessed . OBJECTIVE\\tTo test the EXACT 's performance in three randomized controlled trials and describe the relationship between resource-defined medically treated exacerbations ( MTEs ) and symptom ( EXACT ) - defined events . METHODS\\tPrespecified secondary analyses of data from phase II randomized controlled trials testing new drugs for the management of chronic obstructive pulmonary disease : one 6-month trial ( United States ) ( n = 235 ) and two 3-month , multinational trials ( AZ 1 [ n = 749 ] , AZ 2 [ n = 597 ] ) . METHODS\\tIn each case , the experimental drugs were found to be ineffective , permitting assessment of the EXACT 's performance in three independent studies of moderate to severe high-risk patients on maintenance therapies . RESULTS\\tThe mean age of subjects was 62 to 64 years ; 48 to 76 % were male . RESULTS\\tMean FEV1 % predicted was 42 to 59 % .\", metadata={'source': '/home/notebooks/storage/files/24432712.txt'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approved_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d776c",
   "metadata": {
    "chart_settings": null,
    "collapsed": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "# Create Vector Database from Documents\n",
    "\n",
    "1. This notebook uses FAISS, an open source, in-memory vector store that can be serialized and loaded to disk.\n",
    "2. It uses the open source HuggingFace `all-MiniLM-L6-v2` [embeddings model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). Users are free to experiment with other embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d776d",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Create VectorDB",
    "scrolled": null
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e646712f3ad140e19657efed6e125980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb1bd50761c4a46a9193cfe7c95afb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a076e9a7ec4f7fb88f71a3f040851f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7ee4afcb9c4ca08bc7fde26829448b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf0492d92ef4de9b8eb85b789f63ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65577ba36bbb4da886269a0ec11a8f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30a2c29ba9045e1bf46b51565595087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5c0f1f29bf4bdd8e83704599da1421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021269f6b71d4b3fab1e90eda569b8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aad8fa89c81407980d0bcbc90fdd4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adaf29756c54841b347e0080ad89a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de67b67f4f344cfc87146da0deb4318e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f7a08d7e2748f69e0823dddb96862e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b56dc4525cf4d17ae2064389ea17a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "FAISS VectorDB has 254 documents\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "else:\n",
    "    EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "\n",
    "# Will download the model the first time it runs\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    cache_folder=\"storage/deploy/sentencetransformers\",\n",
    ")\n",
    "try:\n",
    "    # Load existing db from disk if previously built\n",
    "    db = FAISS.load_local(\"storage/deploy/faiss-db\", embedding_function)\n",
    "except:\n",
    "    texts = [doc.page_content for doc in approved_docs]\n",
    "    metadatas = [doc.metadata for doc in approved_docs]   \n",
    "    # Build and save the FAISS db to persistent notebook storage; this can take some time w/o GPUs\n",
    "    db = FAISS.from_texts(texts, embedding_function, metadatas=metadatas)  \n",
    "    db.save_local(\"storage/deploy/faiss-db\")\n",
    "\n",
    "print(f\"FAISS VectorDB has {db.index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d65b937002b973c539c9",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Sanity Tests on Vector Database\n",
    "Testing the Vector Database retrieval of relevant information for our <a href='https://arxiv.org/abs/2005.11401'>RAG</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d776e",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Test the vectorDB",
    "scrolled": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='24516645 OBJECTIVE\\tThe training response of an intensified period of high-intensity exercise is not clear . OBJECTIVE\\tTherefore , we compared the cardiovascular adaptations of completing 24 high-intensity aerobic interval training sessions carried out for either three or eight weeks , respectively . METHODS\\tTwenty-one healthy subjects ( 23.02.1 years , 10 females ) completed 24 high-intensity training sessions throughout a time-period of either eight weeks ( moderate frequency , MF ) or three weeks ( high frequency , HF ) followed by a detraining period of nine weeks without any training . METHODS\\tIn both groups , maximal oxygen uptake ( VO2max ) was evaluated before training , at the 9 ( th ) and 17 ( th ) session and four days after the final 24 ( th ) training session . METHODS\\tIn the detraining phase VO2max was evaluated after 12 days and thereafter every second week for eight weeks . METHODS\\tLeft ventricular echocardiography , carbon monoxide lung diffusion transfer factor , brachial artery flow mediated dilatation and vastus lateralis citrate maximal synthase activity was tested before and after training . RESULTS\\tThe cardiovascular adaptation after HF training was delayed compared to training with MF . RESULTS\\tFour days after ending training the HF group showed no improvement ( +3.0 % , p = 0.126 ) , whereas the MF group reached their highest VO2max with a 10.7 % improvement ( p < 0.001 : group difference p = 0.035 ) .\\n\\nRESULTS\\tThe HF group reached their highest VO2max ( 6.1 % increase , p = 0.026 ) twelve days into the detraining period , compared to a concomitant reduction to 7.9 % of VO2max ( p < 0.001 ) above baseline in the MF group ( group difference p = 0.609 ) . CONCLUSIONS\\tBoth HF and MF training of high-intensity aerobic exercise improves VO2max . CONCLUSIONS\\tThe cardiovascular adaptation following a HF programme of high-intensity exercise is however delayed compared to MF training . BACKGROUND\\tClinicalTrials.gov NCT00733941 .', metadata={'source': '/home/notebooks/storage/files/24516645.txt'}),\n",
       " Document(page_content='RESULTS\\tThere was no change in peak oxygen uptake from baseline to follow-up in either group ( intervention group 27.9 ( 4.7 ) to 28.8 ( 5.6 ) mLkg ( -1 ) min ( -1 ) , control group 32.0 ( 6.2 ) to 32.8 ( 5.8 ) mLkg ( -1 ) min ( -1 ) , with no between-group difference , p = 0.22 ) . RESULTS\\tQuality of life and blood biomarkers remained essentially unchanged , and both self-reported and measured physical activity levels were similar between groups after 12 months . CONCLUSIONS\\tA maintenance exercise program for 12 months did not improve adherence to exercise or peak oxygen uptake in CAD patients after discharge from cardiac rehabilitation compared to usual care . CONCLUSIONS\\tThis suggests that infrequent supervised high intensity interval training sessions are inadequate to improve peak oxygen uptake in this patient group . BACKGROUND\\tClinicalTrials.gov NCT01246570 .', metadata={'source': '/home/notebooks/storage/files/25247991.txt'}),\n",
       " Document(page_content='RESULTS\\tOver 1.5-4 .5 h , MPS remained increased above baseline after all treatments but was greatest after W25 ( 267 % ) and W6 + High-Leu ( 220 % ) treatments ( P = 0.002 ) . CONCLUSIONS\\tA low-protein ( 6.25 g ) mixed macronutrient beverage can be as effective as a high-protein dose ( 25 g ) at stimulating increased MPS rates when supplemented with a high ( 5.0 g total leucine ) amount of leucine . CONCLUSIONS\\tThese results have important implications for formulations of protein beverages designed to enhance muscle anabolism . CONCLUSIONS\\tThis trial was registered at clinicaltrials.gov as NCT 1530646 .', metadata={'source': '/home/notebooks/storage/files/24284442.txt'}),\n",
       " Document(page_content='25247991 BACKGROUND\\tExercise capacity is a strong predictor of survival in patients with coronary artery disease ( CAD ) . BACKGROUND\\tExercise capacity improves after cardiac rehabilitation exercise training , but previous studies have demonstrated a decline in peak oxygen uptake after ending a formal rehabilitation program . BACKGROUND\\tThere is a lack of knowledge on how long-term exercise adherence can be achieved in CAD patients . BACKGROUND\\tWe therefore assessed if a 12-month maintenance program following cardiac rehabilitation would lead to increased adherence to exercise and increased exercise capacity compared to usual care . METHODS\\tTwo-centre , open , parallel randomized controlled trial with 12 months follow-up comparing usual care to a maintenance program . METHODS\\tThe maintenance program consisted of one monthly supervised high intensity interval training session , a written exercise program and exercise diary , and a maximum exercise test every third month during follow-up . METHODS\\tForty-nine patients ( 15 women ) on optimal medical treatment were included following discharge from cardiac rehabilitation . METHODS\\tThe primary endpoint was change in peak oxygen uptake at follow-up ; secondary endpoints were physical activity level , quality of life and blood markers of cardiovascular risk .', metadata={'source': '/home/notebooks/storage/files/25247991.txt'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the database\n",
    "#db.similarity_search(\"Find papers around obesity\")\n",
    "db.similarity_search(\"Can antioxidants impact exercise performance in normobaric hypoxia\")\n",
    "#db.max_marginal_relevance_search(\"How do I replace a custom model on an existing custom environment?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d776f",
   "metadata": {
    "chart_settings": null,
    "collapsed": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "# Define Hooks for Deploying an Unstructured Custom Model\n",
    "Deployinng unstructured custom models in DataRobot requires two hooks load_model and score_unstructured, as this helps DataRobot understand the model structure, inputs, outputs and monitors. More information is available <a href='https://drx.datarobot.com/consume/deploy.html#example-3-thin-monitored-openai-wrapper-with-secret-handling'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7770",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Prepare custom hooks for deploying knowledge base",
    "scrolled": null
   },
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_BASE = os.environ['OPENAI_API_BASE']\n",
    "OPENAI_ORGANIZATION = os.environ['OPENAI_ORGANIZATION']\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "OPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\n",
    "OPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\n",
    "OPENAI_DEPLOYMENT_NAME = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n",
    "\n",
    "def load_model(input_dir):\n",
    "    \"\"\"Custom model hook for loading our knowledge base.\"\"\"\n",
    "    import os\n",
    "    from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "    from langchain.vectorstores.faiss import FAISS\n",
    "    os.environ[\"OPENAI_API_TYPE\"] = OPENAI_API_TYPE\n",
    "    os.environ[\"OPENAI_API_BASE\"] = OPENAI_API_BASE\n",
    "    embedding_function = SentenceTransformerEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        cache_folder=input_dir + '/' + 'storage/deploy/sentencetransformers',\n",
    "    )\n",
    "    db = FAISS.load_local(input_dir + \"/\" + \"storage/deploy/faiss-db\", embedding_function)\n",
    "    return OPENAI_DEPLOYMENT_NAME, db\n",
    "\n",
    "\n",
    "def score_unstructured(model, data, query, **kwargs) -> str:\n",
    "    \"\"\"Custom model hook for making completions with our knowledge base.\n",
    "\n",
    "    When requesting predictions from the deployment, pass a dictionary\n",
    "    with the following keys:\n",
    "    - 'question' the question to be passed to the retrieval chain\n",
    "    - 'openai_api_key' the openai token to be used\n",
    "    - 'chat_history' (optional) a list of two-element lists corresponding to\n",
    "      preceding dialogue between the Human and AI, respectively\n",
    "\n",
    "    datarobot-user-models (DRUM) handles loading the model and calling\n",
    "    this function with the appropriate parameters.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    rv : str\n",
    "        Json dictionary with keys:\n",
    "            - 'question' user's original question\n",
    "            - 'chat_history' chat history that was provided with the original question\n",
    "            - 'answer' the generated answer to the question\n",
    "            - 'references' list of references that were used to generate the answer\n",
    "            - 'error' - error message if exception in handling request\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.vectorstores.base import VectorStoreRetriever\n",
    "    from langchain.chat_models import AzureChatOpenAI\n",
    "    try:\n",
    "        deployment_name, db = model\n",
    "        data_dict = json.loads(data)\n",
    "        llm = AzureChatOpenAI(\n",
    "            deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "            openai_api_type=OPENAI_API_TYPE,\n",
    "            openai_api_base=OPENAI_API_BASE,\n",
    "            openai_api_version=OPENAI_API_VERSION,\n",
    "            openai_api_key=data_dict[\"openai_api_key\"],\n",
    "            openai_organization=OPENAI_ORGANIZATION,\n",
    "            model_name=OPENAI_DEPLOYMENT_NAME,\n",
    "            temperature=0,\n",
    "            verbose=True\n",
    "        )\n",
    "        retriever = VectorStoreRetriever(vectorstore=db,\n",
    "                                         # search_kwargs={\"filter\": {\"trust_level\": \"high\"}}\n",
    "                                        )\n",
    "        chain = ConversationalRetrievalChain.from_llm(llm, \n",
    "                                                      retriever=retriever, \n",
    "                                                      return_source_documents=True)\n",
    "        if 'chat_history' in data_dict:\n",
    "            chat_history = [(human, ai,) for human, ai in data_dict['chat_history']]\n",
    "        else:\n",
    "            chat_history = []\n",
    "        rv = chain(\n",
    "                inputs={\n",
    "                    'question': data_dict['question'],\n",
    "                    'chat_history': chat_history,\n",
    "                },\n",
    "             )\n",
    "        rv['references'] = [doc.metadata['source'] for doc in rv.pop('source_documents')]\n",
    "    except Exception as e:\n",
    "        rv = {'error': f\"{e.__class__.__name__}: {str(e)}\"}\n",
    "    return json.dumps(rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d6ff907ce3b7662d7eae",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Examples\n",
    "Here are some examples of the agent answering questions using the research papers as context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f02a567c4a97b62660b7dd",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from json2html import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_completion(question):\n",
    "    output = score_unstructured(\n",
    "    load_model(\".\"),\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"openai_api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "        }),None,)\n",
    "    output = json.loads(output)\n",
    "    output_cleaned = {'question':output['question'],\n",
    "                      'answer':output['answer'], \n",
    "                      'references':[ \n",
    "                          (open(file,'r')).read()[0:300].replace('\\t',' ').replace('\\n',' ')+'....'\n",
    "                          for file in output['references']]}\n",
    "    html_ = json2html.convert(json = output_cleaned)\n",
    "    return html_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7771",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>question</th><td>How to treat obesity? Please provide conclusions from papers where the methodology is robust.</td></tr><tr><th>answer</th><td>Based on the provided context, here are the conclusions from the papers that have robust methodologies:\n",
       "\n",
       "1. From the first paper (NCT00432809): Among obese patients with uncontrolled type 2 diabetes, 3 years of intensive medical therapy plus bariatric surgery resulted in glycemic control in significantly more patients than did medical therapy alone. Surgical groups showed greater reductions in weight and better quality of life compared to the medical therapy group.\n",
       "\n",
       "2. From the second paper (NCT00842426): Bariatric surgery (Roux-en-Y gastric bypass or sleeve gastrectomy) combined with intensive medical therapy resulted in a glycated hemoglobin level of 6.0% or less in a higher percentage of patients compared to intensive medical therapy alone. The use of glucose-lowering medications, including insulin, was lower in the surgical groups.\n",
       "\n",
       "Please note that these conclusions are specific to the context provided and may not encompass all possible conclusions related to the treatment of obesity. It is always recommended to consult with a healthcare professional for personalized advice and treatment options.</td></tr><tr><th>references</th><td><ul><li>24679060 BACKGROUND In short-term randomized trials ( duration , 1 to 2 years ) , bariatric surgery has been associated with improvement in type 2 diabetes mellitus . METHODS We assessed outcomes 3 years after the randomization of 150 obese patients with uncontrolled type 2 diabetes to receive eithe....</li><li>24369008 OBJECTIVE To examine whether baseline obesity severity modifies the effects of two different , primary care-based , technology-enhanced lifestyle interventions among overweight or obese adults with prediabetes and/or metabolic syndrome . METHODS We compared mean differences in changes from ....</li><li>24679060 BACKGROUND In short-term randomized trials ( duration , 1 to 2 years ) , bariatric surgery has been associated with improvement in type 2 diabetes mellitus . METHODS We assessed outcomes 3 years after the randomization of 150 obese patients with uncontrolled type 2 diabetes to receive eithe....</li><li>24754911 BACKGROUND The Canola Oil Multicenter Intervention Trial ( COMIT ) was a randomized controlled crossover study designed to evaluate the effects of five diets that provided different oils and/or oil blends on cardiovascular disease ( CVD ) risk factors in individuals with abdominal obesity .....</li></ul></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "question = \"How to treat obesity? Please provide conclusions from papers where the methodology is robust.\"\n",
    "display(HTML(get_completion(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7772",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Test Custom Model hooks locally before deploying",
    "scrolled": null
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>question</th><td>What are the effective treatments for rheumatoid arthritis? Please provide conclusions from papers where the methodology is robust.</td></tr><tr><th>answer</th><td>Based on the provided context, there are two papers that discuss effective treatments for rheumatoid arthritis:\n",
       "\n",
       "1. The first paper (24941177) compares the efficacy of tofacitinib, an oral Janus kinase inhibitor, with methotrexate monotherapy in patients with rheumatoid arthritis who had not previously received methotrexate or therapeutic doses of methotrexate. The study found that tofacitinib was effective in reducing joint damage and improving disease symptoms. However, it does not provide a direct comparison with other treatments.\n",
       "\n",
       "2. The second paper (not provided) discusses the use of nonsteroidal anti-inflammatory drugs (NSAIDs), specifically diclofenac, for the treatment of osteoarthritis. It mentions that NSAIDs, including diclofenac, are commonly used to treat osteoarthritis but are associated with dose-related adverse events. The study evaluates the efficacy and safety of low-dose submicron diclofenac in patients with osteoarthritis pain.\n",
       "\n",
       "Unfortunately, the provided context does not include robust conclusions from papers specifically discussing effective treatments for rheumatoid arthritis.</td></tr><tr><th>references</th><td><ul><li>24941177 BACKGROUND Methotrexate is the most frequently used first-line antirheumatic drug . BACKGROUND We report the findings of a phase 3 study of monotherapy with tofacitinib , an oral Janus kinase inhibitor , as compared with methotrexate monotherapy in patients with rheumatoid arthritis who had....</li><li>25050589 OBJECTIVE NSAIDs , such as diclofenac , are the most commonly used medications to treat osteoarthritis ( OA ) , but they are associated with dose-related adverse events ( AEs ) . OBJECTIVE Low-dose submicron diclofenac was developed using a new , proprietary dry milling process that creates....</li><li>25199526 BACKGROUND Knee osteoarthritis ( OA ) causes pain and long-term disability with annual healthcare costs exceeding $ 185 billion in the United States . BACKGROUND Few medical remedies effectively influence the course of the disease . BACKGROUND Finding effective treatments to maintain functi....</li><li>24885354 BACKGROUND Radiotherapy has a good effect in palliation of painful bone metastases , with a pain response rate of more than 60 % . BACKGROUND However , shortly after treatment , in approximately 40 % of patients a temporary pain flare occurs , which is defined as a two-point increase of the....</li></ul></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"What are the effective treatments for rheumatoid arthritis? Please provide \\\n",
    "conclusions from papers where the methodology is robust.\"\n",
    "display(HTML(get_completion(question)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d96c907ce3b7662d7f72",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "## Adversarial Example\n",
    "Here is an example where the knowledge base doesn't have the required information for the agent. This means that there is no trusted paper yet included in the knowledge base. With the combination of Temperature and the Knowledge Base, we can keep the Agent under checks and balances and avoid hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7773",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>question</th><td>Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?</td></tr><tr><th>answer</th><td>I don&#x27;t have any information on the specific effects of high sweetener intake on the pathogenesis of cardiometabolic disorders.</td></tr><tr><th>references</th><td><ul><li>25319187 BACKGROUND Whether the type of dietary fat could alter cardiometabolic responses to a hypercaloric diet is unknown . BACKGROUND In addition , subclinical cardiometabolic consequences of moderate weight gain require further study . RESULTS In a 7-week , double-blind , parallel-group , random....</li><li>24980134 BACKGROUND Managing cardiovascular risk factors is important for reducing vascular complications in type 2 diabetes , even in individuals who have achieved glycemic control . BACKGROUND Nut consumption is associated with reduced cardiovascular risk ; however , there is mixed evidence about ....</li><li>24284442 BACKGROUND Leucine is a key amino acid involved in the regulation of skeletal muscle protein synthesis . OBJECTIVE We assessed the effect of the supplementation of a lower-protein mixed macronutrient beverage with varying doses of leucine or a mixture of branched chain amino acids ( BCAAs )....</li><li>25833983 BACKGROUND Abdominal obesity and exaggerated postprandial lipemia are independent risk factors for cardiovascular disease ( CVD ) and mortality , and both are affected by dietary behavior . OBJECTIVE We investigated whether dietary supplementation with whey protein and medium-chain saturate....</li></ul></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?\"\n",
    "display(HTML(get_completion(question)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5da3f713ebddce5cc2e3f",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Adding new papers into the knowledge base\n",
    "Let's add a paper into the Knowledge base on the above topic to see what happens. Langchain provides hooks to add new documents to the Vector Database index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7774",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading /home/notebooks/storage/files/ directory\n",
       "Splitting 1 documents\n",
       "Created 1 documents\n",
       "\r",
       "100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
       " FAISS VectorDB has 255 documents\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE_DOCUMENTS_DIR = \"/home/notebooks/storage/files/\"\n",
    "SOURCE_DOCUMENTS_FILTER = \"24219891.txt\"\n",
    "\n",
    "loader = DirectoryLoader(f\"{SOURCE_DOCUMENTS_DIR}\", glob=SOURCE_DOCUMENTS_FILTER)\n",
    "print(f\"Loading {SOURCE_DOCUMENTS_DIR} directory\")\n",
    "data = loader.load()\n",
    "print(f\"Splitting {len(data)} documents\")\n",
    "docs = splitter.split_documents(data)\n",
    "print(f\"Created {len(docs)} documents\")\n",
    "for i in tqdm(range(len(docs))):\n",
    "    docs[i].metadata['trust_level'] = 'high'\n",
    "    \n",
    "texts = [doc.page_content for doc in docs]\n",
    "metadatas = [doc.metadata for doc in docs]\n",
    "db.add_texts(texts, metadatas)\n",
    "db.save_local(\"storage/deploy/faiss-db\")\n",
    "\n",
    "print(f\"\\n FAISS VectorDB has {db.index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5da9e937002b973c53b22",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "### Voila!\n",
    "The Agent now has the context to answer the question with the trusted paper that we just added to our knowledge base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7775",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>question</th><td>Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?</td></tr><tr><th>answer</th><td>Yes, high intake of added sweeteners, especially high-fructose intake, is considered to have a causal role in the pathogenesis of cardiometabolic disorders. It may not only cause weight gain but also low-grade inflammation, which is an independent risk factor for developing type 2 diabetes and cardiovascular disease.</td></tr><tr><th>references</th><td><ul><li>24219891 OBJECTIVE High intake of added sweeteners is considered to have a causal role in the pathogenesis of cardiometabolic disorders . OBJECTIVE Especially , high-fructose intake is regarded as potentially harmful to cardiometabolic health . OBJECTIVE It may cause not only weight gain but also lo....</li><li>25319187 BACKGROUND Whether the type of dietary fat could alter cardiometabolic responses to a hypercaloric diet is unknown . BACKGROUND In addition , subclinical cardiometabolic consequences of moderate weight gain require further study . RESULTS In a 7-week , double-blind , parallel-group , random....</li><li>24980134 BACKGROUND Managing cardiovascular risk factors is important for reducing vascular complications in type 2 diabetes , even in individuals who have achieved glycemic control . BACKGROUND Nut consumption is associated with reduced cardiovascular risk ; however , there is mixed evidence about ....</li><li>24284442 BACKGROUND Leucine is a key amino acid involved in the regulation of skeletal muscle protein synthesis . OBJECTIVE We assessed the effect of the supplementation of a lower-protein mixed macronutrient beverage with varying doses of leucine or a mixture of branched chain amino acids ( BCAAs )....</li></ul></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?\"\n",
    "display(HTML(get_completion(question)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d7776",
   "metadata": {
    "chart_settings": null,
    "collapsed": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "# Deploy the knowledge base to staging and query it\n",
    "\n",
    "This convenience method\n",
    "\n",
    "- Builds a new Custom Model Environment containing the contents of storage/deploy/\n",
    "- Assembles a new Custom Model with the provided hooks\n",
    "- Deploys an Unstructured Custom Model to your Deployments\n",
    "- Returns an object which can be used to make predictions\n",
    "\n",
    "Use `environment_id` to re-use an existing Custom Model Environment that you're happy with for shorter iteration cycles on the custom model hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7777",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Create Deployment",
    "scrolled": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Deploying custom model\u001b[0m\n",
       "\u001b[1m  - \u001b[0mUnable to auto-detect model type; any provided paths and files will be\n",
       "    exported - dependencies should be explicitly specified using\n",
       "    extra_requirements\n",
       "\u001b[1m  - \u001b[0mPreparing model and environment...\n",
       "\u001b[1m  - \u001b[0mConfigured environment [[Custom] Medical Research Papers\n",
       "    redux](https://app.datarobot.com/model-registry/custom-environments/64edfad0abee78c9e6b9dc45)\n",
       "    with requirements:\n",
       "      python 3.9.16\n",
       "      datarobot-drum==1.10.3\n",
       "      datarobot-mlops==8.2.7\n",
       "      cloudpickle>=2.0.0\n",
       "      langchain==0.0.244\n",
       "      faiss-cpu==1.7.4\n",
       "      sentence-transformers==2.2.2\n",
       "      openai==0.27.8\n",
       "\u001b[1m  - \u001b[0mAwaiting custom environment build...\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mConfiguring and uploading custom model...\n",
       "\r",
       "    100%|███████████████████████████████████| 92.4M/92.4M [00:00<00:00, 240MB/s]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mRegistered custom model [Medical Research Papers\n",
       "    redux](https://app.datarobot.com/model-registry/custom-models/64ee013fb4482185322c1375/info)\n",
       "    with target type: Unstructured\n",
       "\u001b[1m  - \u001b[0mCreating and deploying model package...\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mCreated deployment [Medical Research Papers\n",
       "    redux](https://app.datarobot.com/deployments/64ee0150da79fc4182e4e537/overview)\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Custom model deployment complete\u001b[0m\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datarobotx as drx\n",
    "\n",
    "deployment = drx.deploy(\n",
    "    \"storage/deploy/\",\n",
    "    name=\"Medical Research Papers redux\",\n",
    "    hooks={\n",
    "        \"score_unstructured\": score_unstructured,\n",
    "        \"load_model\": load_model\n",
    "    },\n",
    "    extra_requirements=[\"langchain\", \"faiss-cpu\", \"sentence-transformers\", \"openai\"],\n",
    "    # Re-use existing environment if you want to change the hook code,\n",
    "    # and not requirements\n",
    "    # environment_id=\"646e81c124b3abadc7c66da0\",\n",
    ")\n",
    "# enable storing prediction data, necessary for Data Export for monitoring purposes\n",
    "deployment.dr_deployment.update_predictions_data_collection_settings(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddcb16dc6e4d9381b58711",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Making predictions\u001b[0m\n",
       "\u001b[1m  - \u001b[0mMaking predictions with deployment [Medical Research Papers\n",
       "    redux](https://app.datarobot.com/deployments/64ee0150da79fc4182e4e537/overview)\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Predictions complete\u001b[0m\n",
       "{'question': 'Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?',\n",
       " 'chat_history': [],\n",
       " 'answer': 'Yes, high intake of added sweeteners, especially high-fructose intake, is considered to have a causal role in the pathogenesis of cardiometabolic disorders. It may not only cause weight gain but also low-grade inflammation, which is an independent risk factor for developing type 2 diabetes and cardiovascular disease.',\n",
       " 'references': ['/home/notebooks/storage/files/24219891.txt',\n",
       "  '/home/notebooks/storage/files/25319187.txt',\n",
       "  '/home/notebooks/storage/files/24980134.txt',\n",
       "  '/home/notebooks/storage/files/24284442.txt']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the deployment\n",
    "deployment.predict_unstructured(\n",
    "    {\n",
    "        \"question\": \"Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?\",\n",
    "        \"openai_api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef318fa9001ef9a08308a0",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "source": [
    "# Conclusion\n",
    "In this notebook, we have observed how to\n",
    "<br> - use predictive models to classify text files\n",
    "<br> - create a vector store out of research paper abstracts\n",
    "<br> - use Retrieval Augmented Generation with Generative AI model\n",
    "<br> - deploy said Generative AI model to the DataRobot platform\n",
    "<br> Inorder to create a Conversational Agent that can be used by healthcare professionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7779",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
