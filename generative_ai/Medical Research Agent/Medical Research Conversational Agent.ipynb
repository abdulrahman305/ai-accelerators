{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d5cc09937002b973c53689",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "# Conversational agent on medical research papers\n",
    "\n",
    "Medical professionals have to constantly stay informed of the latest research in the field, including domains beyond their specialization. Considering the rate at which research publications flood the internet, it becomes tough for medical professionals to ramp up the production of trusted and approved research papers. Qccess to trusted repositories helps them, but there are many sources like Nature, PubMed, Assorted Journals which also publish a lot of work. Having a knowledge system that curates trusted papers and then allows fast retrieval with a Question and Answer agent immensely simplifies the medical professionals' knowledge initiatives.\n",
    "\n",
    "Another key point to note is that an LLM can hallucinate and provide answers to a question. A knowledge base provides contextual data for the LLM to ground itself and prevents hallucination. Additionally, the knowledge base provides the LLM with information that it has not been trained on.\n",
    "\n",
    "This accelerator aims to provide instructions on how to build this type of system using DataRobot's generative AI solution framework. The accelerator shows how you can build a pipeline to create a knowledge base with only trusted research papers, and build a conversational agent that can answer questions from medical professionals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d7762",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "## Setup\n",
    "\n",
    "Before proceeding with running this notebook, review the following steps.\n",
    "\n",
    "1. Enable the following feature flags for your DataRobot account:\n",
    "    - Enable Notebooks Filesystem Management\n",
    "    - Enable Proxy models\n",
    "    - Enable Public Network Access for all Custom Models \n",
    "\t- Enable the Injection of Runtime Parameters for Custom Models\n",
    "    - Enable Monitoring Support for Generative Models\n",
    "    - Enable Custom Inference Models\n",
    "2. Enable the notebook filesystem for this notebook in the notebook sidebar.\n",
    "3. Add the notebook environment variables `OPENAI_API_KEY`, `OPENAI_ORGANIZATION`,\n",
    "   and `OPENAI_API_BASE`. Set the values with your Azure OpenAI credentials.\n",
    "4. Set the notebook session timeout to 180 minutes.\n",
    "5. Restart the notebook container using at least a \"Medium\" (16GB RAM) instance.\n",
    "6. Upload your documents archive to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7763",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": null,
    "hide_code": null,
    "hide_results": null
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "\n",
    "    assert \"OPENAI_API_KEY\" in os.environ\n",
    "    assert \"OPENAI_ORGANIZATION\" in os.environ\n",
    "    assert \"OPENAI_API_BASE\" in os.environ\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Please follow the setup steps before running the notebook.\"\n",
    "    ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5ceb2937002b973c5375e",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "### Install libraries\n",
    "The accelerator uses <a href='https://python.langchain.com/docs/get_started/introduction.html'>Langchain</a> for developing the agent, and <a href='https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/'>FAISS</a> and <a href='https://www.sbert.net/'>Sentence Transformers</a> for the <a href='https://arxiv.org/abs/2005.11401'>RAG</a> system. The LLM is an OpenAI model hosted on Azure. DataRobot provides the freedom for you to use your preferred components in the stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7764",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": true
   },
   "outputs": [],
   "source": [
    "!pip install \"langchain==0.1.1\" \\\n",
    "             \"faiss-cpu==1.7.4\" \\\n",
    "             \"sentence-transformers==2.2.2\" \\\n",
    "             \"unstructured==0.8.4\" \\\n",
    "             \"openai==0.27.8\" \\\n",
    "             \"datarobotx==0.1.14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edd147020c36f5a06ccf37",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": true,
    "name": "Installing DataRobotX"
   },
   "outputs": [],
   "source": [
    "!pip install datarobotx[llm] json2html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d148937002b973c5382b",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "### Document corpus\n",
    "\n",
    "The cells below contain the corpus of both trusted and non-trusted medical research abstracts. These will simulate the real-world documents that need to be processed and added to the agent's knowledge base. This dataset is sourced from <a href='https://www.kaggle.com/datasets/anshulmehtakaggl/200000-abstracts-for-seq-sentence-classification'>Kaggle</a>. This demo uses a subset of the papers to help you run the notebook quickly. You can find the files.zip file <a href='https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/medical_agent/files.zip'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7765",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": null,
    "hide_code": null,
    "hide_results": null
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/medical_agent/files.zip\",\n",
    "    allow_redirects=True,\n",
    ")\n",
    "open(\"/home/notebooks/storage/files.zip\", \"wb\").write(r.content)\n",
    "shutil.unpack_archive(\n",
    "    \"/home/notebooks/storage/files.zip\", \"/home/notebooks/storage/\", \"zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7766",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "len(os.listdir(\"/home/notebooks/storage/files/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d27c713ebddce5cc2bcd",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "### Trusted research papers\n",
    "\n",
    "As the aim of this accelerator is to only include trusted papers into the knowledge base, this workflow defines a function to check if the paper can be trusted or not. In this acclerator, you are building a DataRobot AutoML predictive model to predict if the research paper trust level is high or not. Using <a href='https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/'>DataRobot</a> and <a href='https://drx.datarobot.com/model/automl.html'>DataRobotX</a> APIs it is easy to build and deploy this model. You can find the dataset `medical_papers_trust_scoring.csv` <a href='https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/medical_agent/medical_papers_trust_scoring.csv'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edd1861eb4beb24a041dc7",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import datarobotx as drx\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize Client if running this notebook out of DataRobot platform\n",
    "# drx.Client()\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/medical_agent/medical_papers_trust_scoring.csv\"\n",
    ")\n",
    "df_train, df_test = train_test_split(df, test_size=0.4, random_state=42)\n",
    "model = drx.AutoMLModel()\n",
    "model.fit(df_train, target=\"trust\")\n",
    "deployment = model.deploy(wait_for_autopilot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edeeab1eb4beb24a042711",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Waiting for deployment to be initialized...\u001b[0m\n",
       "\u001b[1m  - \u001b[0mInitializing model for prediction explanations...\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mAwaiting deployment creation...\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Making predictions\u001b[0m\n",
       "\u001b[1m  - \u001b[0mMaking predictions with deployment\n",
       "    [dreamy-torvalds](https://app.datarobot.com/deployments/64f09dc8b4c00219dbd2a1a2/overview)\n",
       "\u001b[1m  - \u001b[0mUploading dataset to be scored...\n",
       "\u001b[1m  - \u001b[0mCreated deployment\n",
       "    [dreamy-torvalds](https://app.datarobot.com/deployments/64f09dc8b4c00219dbd2a1a2/overview)\n",
       "    from model [Elastic-Net Classifier with Naive Bayes Feature Weighting\n",
       "    (L2)](https://app.datarobot.com/projects/64f09b83d9e7c11a47b50649/models/64f09d3ea2ddf9b2a314de95/blueprint)\n",
       "    in project\n",
       "    [sad-boyd](https://app.datarobot.com/projects/64f09b83d9e7c11a47b50649/eda)\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Deployment complete\u001b[0m\n",
       "\r",
       "    100%|██████████████████████████████████| 2.00M/2.00M [00:00<00:00, 32.9MB/s]\n",
       "\u001b[1m  - \u001b[0mScoring...\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Predictions complete\u001b[0m\n",
       "<class 'datarobotx.common.utils.FutureDataFrame'>\n",
       "RangeIndex: 1000 entries, 0 to 999\n",
       "Data columns (total 1 columns):\n",
       " #   Column      Non-Null Count  Dtype \n",
       "---  ------      --------------  ----- \n",
       " 0   prediction  1000 non-null   object\n",
       "dtypes: object(1)\n",
       "memory usage: 7.9+ KB\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = deployment.predict(df_test)\n",
    "df_test[\"predictions\"] = predictions.prediction.values\n",
    "predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7767",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Making predictions\u001b[0m\n",
       "\u001b[1m  - \u001b[0mMaking predictions with deployment\n",
       "    [dreamy-torvalds](https://app.datarobot.com/deployments/64f09dc8b4c00219dbd2a1a2/overview)\n",
       "\u001b[1m  - \u001b[0mUploading dataset to be scored...\n",
       "\r",
       "    100%|███████████████████████████████████| 1.81k/1.81k [00:00<00:00, 108kB/s]\n",
       "\u001b[1m  - \u001b[0mScoring...\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Predictions complete\u001b[0m\n",
       "Trust level for paper # 24219891  low\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Making predictions\u001b[0m\n",
       "\u001b[1m  - \u001b[0mMaking predictions with deployment\n",
       "    [dreamy-torvalds](https://app.datarobot.com/deployments/64f09dc8b4c00219dbd2a1a2/overview)\n",
       "\u001b[1m  - \u001b[0mUploading dataset to be scored...\n",
       "\r",
       "    100%|███████████████████████████████████| 1.89k/1.89k [00:00<00:00, 101kB/s]\n",
       "\u001b[1m  - \u001b[0mScoring...\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Predictions complete\u001b[0m\n",
       "Trust level for paper # 24229754  high\n",
       "CPU times: user 76.3 ms, sys: 21 ms, total: 97.2 ms\n",
       "Wall time: 797 ms\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def get_paper_trust_level(file_path):\n",
    "    file_paper = open(file_path, \"r+\")\n",
    "    paper_content = file_paper.read()\n",
    "    file_paper.close()\n",
    "    pred = deployment.predict(\n",
    "        pd.DataFrame({\"abstract\": [paper_content]}), wait_for_autopilot=True\n",
    "    )\n",
    "    return pred[\"prediction\"].iloc[0]\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Trust level for paper # 24219891 \",\n",
    "    get_paper_trust_level(\"/home/notebooks/storage/files/24219891.txt\"),\n",
    ")\n",
    "print(\n",
    "    \"Trust level for paper # 24229754 \",\n",
    "    get_paper_trust_level(\"/home/notebooks/storage/files/24229754.txt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d7768",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "## Load and split text\n",
    "\n",
    "If you are applying this recipe to a different use case, consider the following:\n",
    "\n",
    "- Use additional or alternative document loaders.\n",
    "- Filter out extraneous or noisy documents.\n",
    "- Choose an appropriate `chunk_size` and `overlap`. These are counted by number of characters, NOT tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7769",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Load Documents"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading /home/notebooks/storage/files/ directory\n",
       "[nltk_data] Downloading package punkt to /home/notebooks/nltk_data...\n",
       "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
       "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
       "[nltk_data]     /home/notebooks/nltk_data...\n",
       "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Splitting 2500 documents\n",
       "Created 3474 documents\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "SOURCE_DOCUMENTS_DIR = \"/home/notebooks/storage/files/\"\n",
    "SOURCE_DOCUMENTS_FILTER = \"*.txt\"\n",
    "\n",
    "loader = DirectoryLoader(f\"{SOURCE_DOCUMENTS_DIR}\", glob=SOURCE_DOCUMENTS_FILTER)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=1000,\n",
    ")\n",
    "\n",
    "print(f\"Loading {SOURCE_DOCUMENTS_DIR} directory\")\n",
    "data = loader.load()\n",
    "print(f\"Splitting {len(data)} documents\")\n",
    "docs = splitter.split_documents(data)\n",
    "print(f\"Created {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d336907ce3b7662d7d7b",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "### Filtration\n",
    "\n",
    "This cell filters only trusted papers to be loaded to the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d776a",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\r",
       "100%|██████████| 3474/3474 [00:01<00:00, 2997.29it/s]\n",
       "254"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "approved_docs = []\n",
    "for i in tqdm(range(len(docs))):\n",
    "    if (\n",
    "        docs[i].metadata[\"source\"].split(\"/\")[-1]\n",
    "        in df_test[df_test.predictions == \"high\"][\"filename\"].tolist()\n",
    "    ):\n",
    "        approved_docs.append(docs[i])\n",
    "len(approved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d776b",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"24432712 BACKGROUND\\tThe EXAcerbations of Chronic Pulmonary Disease Tool ( EXACT ) is a patient-reported outcome measure to standardize the symptomatic assessment of chronic obstructive pulmonary disease exacerbations , including reported and unreported events . BACKGROUND\\tThe instrument has been validated in a short-term study of patients with acute exacerbation and stable disease ; its performance in longer-term studies has not been assessed . OBJECTIVE\\tTo test the EXACT 's performance in three randomized controlled trials and describe the relationship between resource-defined medically treated exacerbations ( MTEs ) and symptom ( EXACT ) - defined events . METHODS\\tPrespecified secondary analyses of data from phase II randomized controlled trials testing new drugs for the management of chronic obstructive pulmonary disease : one 6-month trial ( United States ) ( n = 235 ) and two 3-month , multinational trials ( AZ 1 [ n = 749 ] , AZ 2 [ n = 597 ] ) . METHODS\\tIn each case , the experimental drugs were found to be ineffective , permitting assessment of the EXACT 's performance in three independent studies of moderate to severe high-risk patients on maintenance therapies . RESULTS\\tThe mean age of subjects was 62 to 64 years ; 48 to 76 % were male . RESULTS\\tMean FEV1 % predicted was 42 to 59 % .\", metadata={'source': '/home/notebooks/storage/files/24432712.txt'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approved_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d776c",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "## Create a vector database from the documents\n",
    "\n",
    "1. This notebook uses FAISS, an open source, in-memory vector store that can be serialized and loaded to disk.\n",
    "2. The notebook uses the open source HuggingFace `all-MiniLM-L6-v2` [embeddings model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). Users are free to experiment with other embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d776d",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Create VectorDB"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "else:\n",
    "    EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "\n",
    "# Will download the model the first time it runs\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    cache_folder=\"storage/deploy/sentencetransformers\",\n",
    ")\n",
    "try:\n",
    "    # Load existing db from disk if previously built\n",
    "    db = FAISS.load_local(\"storage/deploy/faiss-db\", embedding_function)\n",
    "except:\n",
    "    texts = [doc.page_content for doc in approved_docs]\n",
    "    metadatas = [doc.metadata for doc in approved_docs]\n",
    "    # Build and save the FAISS db to persistent notebook storage; this can take some time w/o GPUs\n",
    "    db = FAISS.from_texts(texts, embedding_function, metadatas=metadatas)\n",
    "    db.save_local(\"storage/deploy/faiss-db\")\n",
    "\n",
    "print(f\"FAISS VectorDB has {db.index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d65b937002b973c539c9",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "### Perform sanity tests on the vector database\n",
    "\n",
    "Test the vector database retrieval of relevant information for your <a href='https://arxiv.org/abs/2005.11401'>RAG</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d776e",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Test the vectorDB"
   },
   "outputs": [],
   "source": [
    "# Test the database\n",
    "# db.similarity_search(\"Find papers around obesity\")\n",
    "db.similarity_search(\n",
    "    \"Can antioxidants impact exercise performance in normobaric hypoxia\"\n",
    ")\n",
    "# db.max_marginal_relevance_search(\"How do I replace a custom model on an existing custom environment?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d776f",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "## Define hooks for deploying an unstructured custom model\n",
    "\n",
    "Deploying unstructured custom models in DataRobot requires two hooks, `load_model` and `score_unstructured`. These hooks help DataRobot understand the model structure, inputs, outputs, and monitors. More information is available <a href='https://drx.datarobot.com/consume/deploy.html#example-3-thin-monitored-openai-wrapper-with-secret-handling'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7770",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Prepare custom hooks for deploying knowledge base"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\n",
    "OPENAI_ORGANIZATION = os.environ[\"OPENAI_ORGANIZATION\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "OPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\n",
    "OPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\n",
    "OPENAI_DEPLOYMENT_NAME = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n",
    "\n",
    "\n",
    "def load_model(input_dir):\n",
    "    \"\"\"Custom model hook for loading our knowledge base.\"\"\"\n",
    "    import os\n",
    "\n",
    "    from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "    from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "    os.environ[\"OPENAI_API_TYPE\"] = OPENAI_API_TYPE\n",
    "    os.environ[\"OPENAI_API_BASE\"] = OPENAI_API_BASE\n",
    "    embedding_function = SentenceTransformerEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        cache_folder=input_dir + \"/\" + \"storage/deploy/sentencetransformers\",\n",
    "    )\n",
    "    db = FAISS.load_local(\n",
    "        input_dir + \"/\" + \"storage/deploy/faiss-db\", embedding_function\n",
    "    )\n",
    "    return OPENAI_DEPLOYMENT_NAME, db\n",
    "\n",
    "\n",
    "def score_unstructured(model, data, query, **kwargs) -> str:\n",
    "    \"\"\"Custom model hook for making completions with our knowledge base.\n",
    "\n",
    "    When requesting predictions from the deployment, pass a dictionary\n",
    "    with the following keys:\n",
    "    - 'question' the question to be passed to the retrieval chain\n",
    "    - 'openai_api_key' the openai token to be used\n",
    "    - 'chat_history' (optional) a list of two-element lists corresponding to\n",
    "      preceding dialogue between the Human and AI, respectively\n",
    "\n",
    "    datarobot-user-models (DRUM) handles loading the model and calling\n",
    "    this function with the appropriate parameters.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    rv : str\n",
    "        Json dictionary with keys:\n",
    "            - 'question' user's original question\n",
    "            - 'chat_history' chat history that was provided with the original question\n",
    "            - 'answer' the generated answer to the question\n",
    "            - 'references' list of references that were used to generate the answer\n",
    "            - 'error' - error message if exception in handling request\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import AzureChatOpenAI\n",
    "    from langchain.vectorstores.base import VectorStoreRetriever\n",
    "\n",
    "    try:\n",
    "        deployment_name, db = model\n",
    "        data_dict = json.loads(data)\n",
    "        llm = AzureChatOpenAI(\n",
    "            deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "            openai_api_type=OPENAI_API_TYPE,\n",
    "            openai_api_base=OPENAI_API_BASE,\n",
    "            openai_api_version=OPENAI_API_VERSION,\n",
    "            openai_api_key=data_dict[\"openai_api_key\"],\n",
    "            openai_organization=OPENAI_ORGANIZATION,\n",
    "            model_name=OPENAI_DEPLOYMENT_NAME,\n",
    "            temperature=0,\n",
    "            verbose=True,\n",
    "        )\n",
    "        retriever = VectorStoreRetriever(\n",
    "            vectorstore=db,\n",
    "            # search_kwargs={\"filter\": {\"trust_level\": \"high\"}}\n",
    "        )\n",
    "        chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm, retriever=retriever, return_source_documents=True\n",
    "        )\n",
    "        if \"chat_history\" in data_dict:\n",
    "            chat_history = [\n",
    "                (\n",
    "                    human,\n",
    "                    ai,\n",
    "                )\n",
    "                for human, ai in data_dict[\"chat_history\"]\n",
    "            ]\n",
    "        else:\n",
    "            chat_history = []\n",
    "        rv = chain(\n",
    "            inputs={\n",
    "                \"question\": data_dict[\"question\"],\n",
    "                \"chat_history\": chat_history,\n",
    "            },\n",
    "        )\n",
    "        rv[\"references\"] = [\n",
    "            doc.metadata[\"source\"] for doc in rv.pop(\"source_documents\")\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        rv = {\"error\": f\"{e.__class__.__name__}: {str(e)}\"}\n",
    "    return json.dumps(rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d6ff907ce3b7662d7eae",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "## Examples\n",
    "Here are some examples of the agent answering questions using the research papers as context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f02a567c4a97b62660b7dd",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "\n",
    "from json2html import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def get_completion(question):\n",
    "    output = score_unstructured(\n",
    "        load_model(\".\"),\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"openai_api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "            }\n",
    "        ),\n",
    "        None,\n",
    "    )\n",
    "    output = json.loads(output)\n",
    "    output_cleaned = {\n",
    "        \"question\": output[\"question\"],\n",
    "        \"answer\": output[\"answer\"],\n",
    "        \"references\": [\n",
    "            (open(file, \"r\")).read()[0:300].replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "            + \"....\"\n",
    "            for file in output[\"references\"]\n",
    "        ],\n",
    "    }\n",
    "    html_ = json2html.convert(json=output_cleaned)\n",
    "    return html_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7771",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>question</th><td>How to treat obesity? Please provide conclusions from papers where the methodology is robust.</td></tr><tr><th>answer</th><td>Based on the provided context, here are the conclusions from the papers that have robust methodologies:\n",
       "\n",
       "1. In a study comparing different interventions for overweight or obese adults with prediabetes and/or metabolic syndrome, it was found that baseline obesity severity may influence the effectiveness of lifestyle interventions. Participants with a baseline BMI of 35 or higher had greater reductions in BMI, body weight, and waist circumference in a coach-led group intervention compared to usual care and self-directed individual intervention. On the other hand, the self-directed intervention was more effective than usual care only among participants with baseline BMIs between 25 and 35. [Study: 24369008]\n",
       "\n",
       "2. A randomized controlled trial involving obese patients with uncontrolled type 2 diabetes compared intensive medical therapy alone to intensive medical therapy plus bariatric surgery (gastric bypass or sleeve gastrectomy). After 3 years, the surgical groups had significantly better glycemic control, with a glycated hemoglobin level of 6.0% or less achieved by 38% of the gastric-bypass group and 24% of the sleeve-gastrectomy group, compared to only 5% in the medical-therapy group. The surgical groups also had greater reductions in weight and use of glucose-lowering medications. [Study: 24679060]\n",
       "\n",
       "Please note that these conclusions are specific to the provided papers and may not encompass all possible treatments for obesity. It is always recommended to consult with a healthcare professional for personalized advice and treatment options.</td></tr><tr><th>references</th><td><ul><li>24679060 BACKGROUND In short-term randomized trials ( duration , 1 to 2 years ) , bariatric surgery has been associated with improvement in type 2 diabetes mellitus . METHODS We assessed outcomes 3 years after the randomization of 150 obese patients with uncontrolled type 2 diabetes to receive eithe....</li><li>24369008 OBJECTIVE To examine whether baseline obesity severity modifies the effects of two different , primary care-based , technology-enhanced lifestyle interventions among overweight or obese adults with prediabetes and/or metabolic syndrome . METHODS We compared mean differences in changes from ....</li><li>24679060 BACKGROUND In short-term randomized trials ( duration , 1 to 2 years ) , bariatric surgery has been associated with improvement in type 2 diabetes mellitus . METHODS We assessed outcomes 3 years after the randomization of 150 obese patients with uncontrolled type 2 diabetes to receive eithe....</li><li>24754911 BACKGROUND The Canola Oil Multicenter Intervention Trial ( COMIT ) was a randomized controlled crossover study designed to evaluate the effects of five diets that provided different oils and/or oil blends on cardiovascular disease ( CVD ) risk factors in individuals with abdominal obesity .....</li></ul></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "question = \"How to treat obesity? Please provide conclusions from papers where the methodology is robust.\"\n",
    "display(HTML(get_completion(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7772",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Test Custom Model hooks locally before deploying"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>question</th><td>What are the effective treatments for rheumatoid arthritis? Please provide conclusions from papers where the methodology is robust.</td></tr><tr><th>answer</th><td>Based on the provided context, there are two papers that discuss effective treatments for rheumatoid arthritis:\n",
       "\n",
       "1. The first paper (24941177) compares the efficacy of tofacitinib, an oral Janus kinase inhibitor, with methotrexate monotherapy in patients with rheumatoid arthritis who had not previously received methotrexate or therapeutic doses of methotrexate. The study found that tofacitinib was effective in reducing joint damage and improving disease symptoms. However, it does not provide a direct comparison with other treatments.\n",
       "\n",
       "2. The second paper (not provided) discusses the use of nonsteroidal anti-inflammatory drugs (NSAIDs), specifically diclofenac, for the treatment of osteoarthritis. It mentions that NSAIDs, including diclofenac, are commonly used to treat osteoarthritis but are associated with dose-related adverse events. The study evaluates the efficacy and safety of low-dose submicron diclofenac in patients with osteoarthritis pain.\n",
       "\n",
       "Unfortunately, the provided context does not include any papers specifically discussing effective treatments for rheumatoid arthritis with robust methodology.</td></tr><tr><th>references</th><td><ul><li>24941177 BACKGROUND Methotrexate is the most frequently used first-line antirheumatic drug . BACKGROUND We report the findings of a phase 3 study of monotherapy with tofacitinib , an oral Janus kinase inhibitor , as compared with methotrexate monotherapy in patients with rheumatoid arthritis who had....</li><li>25050589 OBJECTIVE NSAIDs , such as diclofenac , are the most commonly used medications to treat osteoarthritis ( OA ) , but they are associated with dose-related adverse events ( AEs ) . OBJECTIVE Low-dose submicron diclofenac was developed using a new , proprietary dry milling process that creates....</li><li>25199526 BACKGROUND Knee osteoarthritis ( OA ) causes pain and long-term disability with annual healthcare costs exceeding $ 185 billion in the United States . BACKGROUND Few medical remedies effectively influence the course of the disease . BACKGROUND Finding effective treatments to maintain functi....</li><li>24885354 BACKGROUND Radiotherapy has a good effect in palliation of painful bone metastases , with a pain response rate of more than 60 % . BACKGROUND However , shortly after treatment , in approximately 40 % of patients a temporary pain flare occurs , which is defined as a two-point increase of the....</li></ul></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the effective treatments for rheumatoid arthritis? Please provide \\\n",
    "conclusions from papers where the methodology is robust.\"\n",
    "display(HTML(get_completion(question)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d96c907ce3b7662d7f72",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "### Adversarial example\n",
    "Here is an example where the knowledge base doesn't have the required information for the agent. This means that there is no trusted paper yet included in the knowledge base. With the combination of Temperature and the Knowledge Base, you can keep the agent under checks and balances and avoid hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7773",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>question</th><td>Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?</td></tr><tr><th>answer</th><td>I don&#x27;t have any information on the specific effects of high sweetener intake on the pathogenesis of cardiometabolic disorders.</td></tr><tr><th>references</th><td><ul><li>25319187 BACKGROUND Whether the type of dietary fat could alter cardiometabolic responses to a hypercaloric diet is unknown . BACKGROUND In addition , subclinical cardiometabolic consequences of moderate weight gain require further study . RESULTS In a 7-week , double-blind , parallel-group , random....</li><li>24980134 BACKGROUND Managing cardiovascular risk factors is important for reducing vascular complications in type 2 diabetes , even in individuals who have achieved glycemic control . BACKGROUND Nut consumption is associated with reduced cardiovascular risk ; however , there is mixed evidence about ....</li><li>24284442 BACKGROUND Leucine is a key amino acid involved in the regulation of skeletal muscle protein synthesis . OBJECTIVE We assessed the effect of the supplementation of a lower-protein mixed macronutrient beverage with varying doses of leucine or a mixture of branched chain amino acids ( BCAAs )....</li><li>25833983 BACKGROUND Abdominal obesity and exaggerated postprandial lipemia are independent risk factors for cardiovascular disease ( CVD ) and mortality , and both are affected by dietary behavior . OBJECTIVE We investigated whether dietary supplementation with whey protein and medium-chain saturate....</li></ul></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?\"\n",
    "display(HTML(get_completion(question)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5da3f713ebddce5cc2e3f",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "### Add new papers into the knowledge base\n",
    "\n",
    "You can add a paper to the knowledge base on the above topic to see what happens. Langchain provides hooks to add new documents to the vector database index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7774",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Loading /home/notebooks/storage/files/ directory\n",
       "Splitting 1 documents\n",
       "Created 1 documents\n",
       "\r",
       "100%|██████████| 1/1 [00:00<00:00, 17119.61it/s]\n",
       " FAISS VectorDB has 255 documents\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE_DOCUMENTS_DIR = \"/home/notebooks/storage/files/\"\n",
    "SOURCE_DOCUMENTS_FILTER = \"24219891.txt\"\n",
    "\n",
    "loader = DirectoryLoader(f\"{SOURCE_DOCUMENTS_DIR}\", glob=SOURCE_DOCUMENTS_FILTER)\n",
    "print(f\"Loading {SOURCE_DOCUMENTS_DIR} directory\")\n",
    "data = loader.load()\n",
    "print(f\"Splitting {len(data)} documents\")\n",
    "docs = splitter.split_documents(data)\n",
    "print(f\"Created {len(docs)} documents\")\n",
    "for i in tqdm(range(len(docs))):\n",
    "    docs[i].metadata[\"trust_level\"] = \"high\"\n",
    "\n",
    "texts = [doc.page_content for doc in docs]\n",
    "metadatas = [doc.metadata for doc in docs]\n",
    "db.add_texts(texts, metadatas)\n",
    "db.save_local(\"storage/deploy/faiss-db\")\n",
    "\n",
    "print(f\"\\n FAISS VectorDB has {db.index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5da9e937002b973c53b22",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "The agent now has the context to answer the question with the trusted paper that you just added to the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7775",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>question</th><td>Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?</td></tr><tr><th>answer</th><td>Yes, high intake of added sweeteners, especially high-fructose intake, is considered to have a causal role in the pathogenesis of cardiometabolic disorders. It may not only cause weight gain but also low-grade inflammation, which is an independent risk factor for developing type 2 diabetes and cardiovascular disease.</td></tr><tr><th>references</th><td><ul><li>24219891 OBJECTIVE High intake of added sweeteners is considered to have a causal role in the pathogenesis of cardiometabolic disorders . OBJECTIVE Especially , high-fructose intake is regarded as potentially harmful to cardiometabolic health . OBJECTIVE It may cause not only weight gain but also lo....</li><li>25319187 BACKGROUND Whether the type of dietary fat could alter cardiometabolic responses to a hypercaloric diet is unknown . BACKGROUND In addition , subclinical cardiometabolic consequences of moderate weight gain require further study . RESULTS In a 7-week , double-blind , parallel-group , random....</li><li>24980134 BACKGROUND Managing cardiovascular risk factors is important for reducing vascular complications in type 2 diabetes , even in individuals who have achieved glycemic control . BACKGROUND Nut consumption is associated with reduced cardiovascular risk ; however , there is mixed evidence about ....</li><li>24284442 BACKGROUND Leucine is a key amino acid involved in the regulation of skeletal muscle protein synthesis . OBJECTIVE We assessed the effect of the supplementation of a lower-protein mixed macronutrient beverage with varying doses of leucine or a mixture of branched chain amino acids ( BCAAs )....</li></ul></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?\"\n",
    "display(HTML(get_completion(question)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfeb907ce3b7662d7776",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "## Deploy the knowledge base\n",
    "\n",
    "The convenience method outlined in the cell below does the following:\n",
    "\n",
    "- Builds a new custom model environment containing the contents of storage.\n",
    "- Assembles a new custom model with the provided hooks.\n",
    "- Deploys an unstructured custom model.\n",
    "- Returns an object which can be used to make predictions.\n",
    "\n",
    "Use `environment_id` to re-use an existing custom model environment that you are happy with for shorter iteration cycles on the custom model hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfeb907ce3b7662d7777",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false,
    "name": "Create Deployment"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Deploying custom model\u001b[0m\n",
       "\u001b[1m  - \u001b[0mUnable to auto-detect model type; any provided paths and files will be\n",
       "    exported - dependencies should be explicitly specified using\n",
       "    extra_requirements\n",
       "\u001b[1m  - \u001b[0mPreparing model and environment...\n",
       "\u001b[1m  - \u001b[0mConfigured environment [[Custom] Medical Research Papers\n",
       "    redux](https://app.datarobot.com/model-registry/custom-environments/64edfad0abee78c9e6b9dc45)\n",
       "    with requirements:\n",
       "      python 3.9.16\n",
       "      datarobot-drum==1.10.3\n",
       "      datarobot-mlops==8.2.7\n",
       "      cloudpickle>=2.0.0\n",
       "      langchain==0.0.244\n",
       "      faiss-cpu==1.7.4\n",
       "      sentence-transformers==2.2.2\n",
       "      openai==0.27.8\n",
       "\u001b[1m  - \u001b[0mAwaiting custom environment build...\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mConfiguring and uploading custom model...\n",
       "\r",
       "    100%|███████████████████████████████████| 92.4M/92.4M [00:00<00:00, 240MB/s]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mRegistered custom model [Medical Research Papers\n",
       "    redux](https://app.datarobot.com/model-registry/custom-models/64ee013fb4482185322c1375/info)\n",
       "    with target type: Unstructured\n",
       "\u001b[1m  - \u001b[0mCreating and deploying model package...\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m  - \u001b[0mCreated deployment [Medical Research Papers\n",
       "    redux](https://app.datarobot.com/deployments/64ee0150da79fc4182e4e537/overview)\n",
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Custom model deployment complete\u001b[0m\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datarobotx as drx\n",
    "\n",
    "deployment = drx.deploy(\n",
    "    \"storage/deploy/\",\n",
    "    name=\"Medical Research Papers redux\",\n",
    "    hooks={\"score_unstructured\": score_unstructured, \"load_model\": load_model},\n",
    "    extra_requirements=[\"langchain\", \"faiss-cpu\", \"sentence-transformers\", \"openai\"],\n",
    "    # Re-use existing environment if you want to change the hook code,\n",
    "    # and not requirements\n",
    "    # environment_id=\"646e81c124b3abadc7c66da0\",\n",
    ")\n",
    "# Enable storing prediction data, necessary for Data Export for monitoring purposes\n",
    "deployment.dr_deployment.update_predictions_data_collection_settings(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddcb16dc6e4d9381b58711",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "python"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Making predictions\u001b[0m\n",
       "\u001b[1m  - \u001b[0mMaking predictions with deployment [Medical Research Papers\n",
       "    redux](https://app.datarobot.com/deployments/64ee0150da79fc4182e4e537/overview)\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[34m#\u001b[0m\u001b[1m Predictions complete\u001b[0m\n",
       "{'question': 'Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?',\n",
       " 'chat_history': [],\n",
       " 'answer': 'Yes, high intake of added sweeteners, especially high-fructose intake, is considered to have a causal role in the pathogenesis of cardiometabolic disorders. It may not only cause weight gain but also low-grade inflammation, which is an independent risk factor for developing type 2 diabetes and cardiovascular disease.',\n",
       " 'references': ['/home/notebooks/storage/files/24219891.txt',\n",
       "  '/home/notebooks/storage/files/25319187.txt',\n",
       "  '/home/notebooks/storage/files/24980134.txt',\n",
       "  '/home/notebooks/storage/files/24284442.txt']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the deployment\n",
    "deployment.predict_unstructured(\n",
    "    {\n",
    "        \"question\": \"Can high sweetener intake worsen pathogenesis of cardiometabolic disorders?\",\n",
    "        \"openai_api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef318fa9001ef9a08308a0",
   "metadata": {
    "chart_settings": null,
    "custom_metric_settings": null,
    "dataframe_view_options": null,
    "datarobot": {
     "language": "markdown"
    },
    "disable_run": false,
    "hide_code": false,
    "hide_results": false
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this accelerator, you have observed how to:\n",
    "<br> - Use predictive models to classify text files.\n",
    "<br> - Create a vector store out of research paper abstracts.\n",
    "<br> - Use Retrieval Augmented Generation with a generative AI model.\n",
    "<br> - Deploy a generative AI model to the DataRobot platform.\n",
    "<br> - Create a conversational agent that can be used by healthcare professionals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
